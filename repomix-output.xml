This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
__tests__/
  MarkdownRenderer.test.tsx
  models.test.ts
  providers.test.ts
  ThemeSwitcher.test.tsx
.github/
  workflows/
    ai-code-review.yml
    README.md
app/
  api/
    billing/
      route.ts
    chat/
      route.ts
    codereview/
      route.ts
  chat/
    page.tsx
  settings/
    page.tsx
  favicon.ico
  globals.css
  layout.tsx
  page.tsx
components/
  AISettingsPanel.tsx
  MarkdownRenderer.tsx
  ThemeSwitcher.tsx
  VercelGatewayFallbackModels.tsx
hooks/
  useAIChat.ts
lib/
  ai-handler.ts
  billing.ts
  models.json
  models.ts
  providers.ts
public/
  file.svg
  globe.svg
  next.svg
  vercel.svg
  window.svg
.gitignore
.prettierignore
.prettierrc.json
eslint.config.mjs
jest.config.ts
jest.setup.ts
next.config.ts
package.json
postcss.config.mjs
README.md
tailwind.config.ts
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/ai-code-review.yml">
name: AI Code Review

on:
  pull_request:
    types: [opened, synchronize]

jobs:
  review:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: AI Code Review
        env:
          API_ENDPOINT: ${{ secrets.AI_REVIEW_API_ENDPOINT }}
          AI_API_KEY: ${{ secrets.AI_API_KEY }}
          AI_PROVIDER: ${{ secrets.AI_PROVIDER }}
          AI_MODEL: ${{ secrets.AI_MODEL }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Get PR information
          PR_URL="${{ github.event.pull_request.html_url }}"
          PR_NUMBER="${{ github.event.pull_request.number }}"
          REPO_OWNER="${{ github.repository_owner }}"
          REPO_NAME="${{ github.event.repository.name }}"
          
          # Set defaults if not provided
          AI_PROVIDER="${AI_PROVIDER:-openai}"
          
          echo "Reviewing PR: $PR_URL"
          echo "Using provider: $AI_PROVIDER"
          
          # Prepare request payload
          if [ -n "$AI_MODEL" ]; then
            REQUEST_PAYLOAD=$(cat <<EOF
          {
            "messages": [
              {
                "role": "user",
                "content": "Please review this pull request: $PR_URL"
              }
            ],
            "provider": "$AI_PROVIDER",
            "apiKey": "$AI_API_KEY",
            "model": "$AI_MODEL",
            "stream": false
          }
          EOF
            )
          else
            REQUEST_PAYLOAD=$(cat <<EOF
          {
            "messages": [
              {
                "role": "user",
                "content": "Please review this pull request: $PR_URL"
              }
            ],
            "provider": "$AI_PROVIDER",
            "apiKey": "$AI_API_KEY",
            "stream": false
          }
          EOF
            )
          fi
          
          # Call the AI code review API (non-streaming)
          echo "Calling API endpoint: $API_ENDPOINT"
          REVIEW_RESPONSE=$(curl -s -X POST "$API_ENDPOINT" \
            -H "Content-Type: application/json" \
            -d "$REQUEST_PAYLOAD")
          
          # Extract the review text from the response
          # The API returns { text: "...", usage: {...} } for non-streaming
          REVIEW_TEXT=$(echo "$REVIEW_RESPONSE" | jq -r '.text // empty')
          
          if [ -z "$REVIEW_TEXT" ] || [ "$REVIEW_TEXT" = "null" ]; then
            echo "Error: No review content received"
            echo "Response: $REVIEW_RESPONSE"
            exit 1
          fi
          
          # Get usage info if available
          USAGE_INFO=""
          if echo "$REVIEW_RESPONSE" | jq -e '.usage' > /dev/null 2>&1; then
            PROMPT_TOKENS=$(echo "$REVIEW_RESPONSE" | jq -r '.usage.promptTokens // .usage.promptTokens // 0')
            COMPLETION_TOKENS=$(echo "$REVIEW_RESPONSE" | jq -r '.usage.completionTokens // .usage.completionTokens // 0')
            TOTAL_TOKENS=$(echo "$REVIEW_RESPONSE" | jq -r '.usage.totalTokens // .usage.totalTokens // 0')
            if [ "$TOTAL_TOKENS" != "0" ]; then
              USAGE_INFO="\n\n---\n**Usage:** $PROMPT_TOKENS prompt + $COMPLETION_TOKENS completion = $TOTAL_TOKENS tokens"
            fi
          fi
          
          # Post review as PR comment
          COMMENT_BODY=$(cat <<EOF
          ## ü§ñ AI Code Review
          
          $REVIEW_TEXT$USAGE_INFO
          EOF
          )
          
          curl -X POST \
            -H "Authorization: token $GITHUB_TOKEN" \
            -H "Accept: application/vnd.github.v3+json" \
            "https://api.github.com/repos/$REPO_OWNER/$REPO_NAME/issues/$PR_NUMBER/comments" \
            -d "{\"body\": $(echo "$COMMENT_BODY" | jq -Rs .)}"
          
          echo "Review posted successfully!"
</file>

<file path=".github/workflows/README.md">
# GitHub Actions Workflows

## AI Code Review Workflow

The `ai-code-review.yml` workflow automatically reviews pull requests using your AI code review API.

### Setup

1. **Deploy your AI Code Review app** to a publicly accessible URL (e.g., Vercel, Railway, etc.)

2. **Configure GitHub Secrets** in your repository settings (Settings ‚Üí Secrets and variables ‚Üí Actions):
   - `AI_REVIEW_API_ENDPOINT`: The full URL to your deployed API endpoint
     - Example: `https://ai-agent-86bjedhz7-xiongemis-projects.vercel.app/api/codereview`
   - `AI_API_KEY`: Your AI provider API key (OpenAI, Anthropic, etc.)
   - `AI_PROVIDER`: (Optional) AI provider to use (default: `openai`). Options: `openai`, `gemini`, `anthropic`, `deepseek`, `qwen`, `vercel-ai-gateway`
   - `AI_MODEL`: (Optional) Specific model to use. If not provided, uses the default model for the selected provider

   **Example Configuration:**
   ```
   AI_REVIEW_API_ENDPOINT: https://ai-agent-86bjedhz7-xiongemis-projects.vercel.app/api/codereview
   AI_API_KEY: sk-... (your OpenAI API key)
   AI_PROVIDER: openai
   ```

### How it Works

1. Triggers automatically on:
   - Pull request opened
   - Pull request updated (new commits pushed)

2. The workflow:
   - Checks out the PR code
   - Calls your AI code review API with the PR URL
   - Posts the review as a comment on the PR

### Example Configuration

```yaml
# In your workflow file, you can customize:
env:
  AI_PROVIDER: 'openai'  # or 'anthropic', 'gemini', etc.
  AI_MODEL: 'gpt-4'      # optional, uses default if not specified
```

### Customization

You can customize the workflow by:
- Changing the trigger events
- Modifying the system prompt
- Adding additional review criteria
- Customizing the comment format

### Troubleshooting

- **API endpoint not found**: Make sure `AI_REVIEW_API_ENDPOINT` secret is set correctly
- **No review posted**: Check the workflow logs for API errors
- **Authentication errors**: Verify your `AI_API_KEY` secret is correct
</file>

<file path="public/file.svg">
<svg fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="M14.5 13.5V5.41a1 1 0 0 0-.3-.7L9.8.29A1 1 0 0 0 9.08 0H1.5v13.5A2.5 2.5 0 0 0 4 16h8a2.5 2.5 0 0 0 2.5-2.5m-1.5 0v-7H8v-5H3v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1M9.5 5V2.12L12.38 5zM5.13 5h-.62v1.25h2.12V5zm-.62 3h7.12v1.25H4.5zm.62 3h-.62v1.25h7.12V11z" clip-rule="evenodd" fill="#666" fill-rule="evenodd"/></svg>
</file>

<file path="public/globe.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><g clip-path="url(#a)"><path fill-rule="evenodd" clip-rule="evenodd" d="M10.27 14.1a6.5 6.5 0 0 0 3.67-3.45q-1.24.21-2.7.34-.31 1.83-.97 3.1M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16m.48-1.52a7 7 0 0 1-.96 0H7.5a4 4 0 0 1-.84-1.32q-.38-.89-.63-2.08a40 40 0 0 0 3.92 0q-.25 1.2-.63 2.08a4 4 0 0 1-.84 1.31zm2.94-4.76q1.66-.15 2.95-.43a7 7 0 0 0 0-2.58q-1.3-.27-2.95-.43a18 18 0 0 1 0 3.44m-1.27-3.54a17 17 0 0 1 0 3.64 39 39 0 0 1-4.3 0 17 17 0 0 1 0-3.64 39 39 0 0 1 4.3 0m1.1-1.17q1.45.13 2.69.34a6.5 6.5 0 0 0-3.67-3.44q.65 1.26.98 3.1M8.48 1.5l.01.02q.41.37.84 1.31.38.89.63 2.08a40 40 0 0 0-3.92 0q.25-1.2.63-2.08a4 4 0 0 1 .85-1.32 7 7 0 0 1 .96 0m-2.75.4a6.5 6.5 0 0 0-3.67 3.44 29 29 0 0 1 2.7-.34q.31-1.83.97-3.1M4.58 6.28q-1.66.16-2.95.43a7 7 0 0 0 0 2.58q1.3.27 2.95.43a18 18 0 0 1 0-3.44m.17 4.71q-1.45-.12-2.69-.34a6.5 6.5 0 0 0 3.67 3.44q-.65-1.27-.98-3.1" fill="#666"/></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h16v16H0z"/></clipPath></defs></svg>
</file>

<file path="public/next.svg">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 394 80"><path fill="#000" d="M262 0h68.5v12.7h-27.2v66.6h-13.6V12.7H262V0ZM149 0v12.7H94v20.4h44.3v12.6H94v21h55v12.6H80.5V0h68.7zm34.3 0h-17.8l63.8 79.4h17.9l-32-39.7 32-39.6h-17.9l-23 28.6-23-28.6zm18.3 56.7-9-11-27.1 33.7h17.8l18.3-22.7z"/><path fill="#000" d="M81 79.3 17 0H0v79.3h13.6V17l50.2 62.3H81Zm252.6-.4c-1 0-1.8-.4-2.5-1s-1.1-1.6-1.1-2.6.3-1.8 1-2.5 1.6-1 2.6-1 1.8.3 2.5 1a3.4 3.4 0 0 1 .6 4.3 3.7 3.7 0 0 1-3 1.8zm23.2-33.5h6v23.3c0 2.1-.4 4-1.3 5.5a9.1 9.1 0 0 1-3.8 3.5c-1.6.8-3.5 1.3-5.7 1.3-2 0-3.7-.4-5.3-1s-2.8-1.8-3.7-3.2c-.9-1.3-1.4-3-1.4-5h6c.1.8.3 1.6.7 2.2s1 1.2 1.6 1.5c.7.4 1.5.5 2.4.5 1 0 1.8-.2 2.4-.6a4 4 0 0 0 1.6-1.8c.3-.8.5-1.8.5-3V45.5zm30.9 9.1a4.4 4.4 0 0 0-2-3.3 7.5 7.5 0 0 0-4.3-1.1c-1.3 0-2.4.2-3.3.5-.9.4-1.6 1-2 1.6a3.5 3.5 0 0 0-.3 4c.3.5.7.9 1.3 1.2l1.8 1 2 .5 3.2.8c1.3.3 2.5.7 3.7 1.2a13 13 0 0 1 3.2 1.8 8.1 8.1 0 0 1 3 6.5c0 2-.5 3.7-1.5 5.1a10 10 0 0 1-4.4 3.5c-1.8.8-4.1 1.2-6.8 1.2-2.6 0-4.9-.4-6.8-1.2-2-.8-3.4-2-4.5-3.5a10 10 0 0 1-1.7-5.6h6a5 5 0 0 0 3.5 4.6c1 .4 2.2.6 3.4.6 1.3 0 2.5-.2 3.5-.6 1-.4 1.8-1 2.4-1.7a4 4 0 0 0 .8-2.4c0-.9-.2-1.6-.7-2.2a11 11 0 0 0-2.1-1.4l-3.2-1-3.8-1c-2.8-.7-5-1.7-6.6-3.2a7.2 7.2 0 0 1-2.4-5.7 8 8 0 0 1 1.7-5 10 10 0 0 1 4.3-3.5c2-.8 4-1.2 6.4-1.2 2.3 0 4.4.4 6.2 1.2 1.8.8 3.2 2 4.3 3.4 1 1.4 1.5 3 1.5 5h-5.8z"/></svg>
</file>

<file path="public/vercel.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1155 1000"><path d="m577.3 0 577.4 1000H0z" fill="#fff"/></svg>
</file>

<file path="public/window.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.5 2.5h13v10a1 1 0 0 1-1 1h-11a1 1 0 0 1-1-1zM0 1h16v11.5a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 0 12.5zm3.75 4.5a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5M7 4.75a.75.75 0 1 1-1.5 0 .75.75 0 0 1 1.5 0m1.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" fill="#666"/></svg>
</file>

<file path=".gitignore">
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts
</file>

<file path=".prettierignore">
node_modules/
pnpm-lock.yaml
.next/
.env
.env.local
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts",
    ".next/dev/types/**/*.ts",
    "**/*.mts"
  ],
  "exclude": ["node_modules"]
}
</file>

<file path="__tests__/MarkdownRenderer.test.tsx">
import '@testing-library/jest-dom';
import { render, screen } from '@testing-library/react';

// Mock marked to avoid ESM issues in Jest
jest.mock('marked', () => ({
  marked: {
    parse: (text: string) => {
      // Simple mock markdown parser for testing
      return text
        .replace(/^# (.+)$/gm, '<h1>$1</h1>')
        .replace(/^## (.+)$/gm, '<h2>$1</h2>')
        .replace(/^### (.+)$/gm, '<h3>$1</h3>')
        .replace(/^- (.+)$/gm, '<ul><li>$1</li></ul>')
        .replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>')
        .replace(/\*(.+?)\*/g, '<em>$1</em>')
        .replace(/\[(.+?)\]\((.+?)\)/g, '<a href="$2">$1</a>')
        .replace(/```javascript\n([\s\S]+?)```/g, '<pre><code>$1</code></pre>');
    },
    setOptions: jest.fn(),
  },
}));

import { MarkdownRenderer } from '@/components/MarkdownRenderer';

describe('MarkdownRenderer', () => {
  it('should render markdown headings', () => {
    const markdown = '# Heading 1\n## Heading 2\n### Heading 3';
    render(<MarkdownRenderer content={markdown} />);

    const h1 = screen.getByText('Heading 1');
    expect(h1.tagName).toBe('H1');

    const h2 = screen.getByText('Heading 2');
    expect(h2.tagName).toBe('H2');

    const h3 = screen.getByText('Heading 3');
    expect(h3.tagName).toBe('H3');
  });

  it('should render markdown lists', () => {
    const markdown = '- Item 1\n- Item 2\n- Item 3';
    render(<MarkdownRenderer content={markdown} />);

    const list = screen.getByText('Item 1').closest('ul');
    expect(list).toBeInTheDocument();
    expect(screen.getByText('Item 2')).toBeInTheDocument();
    expect(screen.getByText('Item 3')).toBeInTheDocument();
  });

  it('should render markdown code blocks', () => {
    const markdown = '```javascript\nconst x = 1;\n```';
    render(<MarkdownRenderer content={markdown} />);

    const codeBlock = screen.getByText('const x = 1;');
    expect(codeBlock.closest('pre')).toBeInTheDocument();
  });

  it('should render markdown links', () => {
    const markdown = '[Link Text](https://example.com)';
    render(<MarkdownRenderer content={markdown} />);

    const link = screen.getByText('Link Text');
    expect(link.tagName).toBe('A');
    expect(link).toHaveAttribute('href', 'https://example.com');
  });

  it('should render markdown bold and italic text', () => {
    const markdown = '**bold** and *italic*';
    render(<MarkdownRenderer content={markdown} />);

    const bold = screen.getByText('bold');
    expect(bold.tagName).toBe('STRONG');

    const italic = screen.getByText('italic');
    expect(italic.tagName).toBe('EM');
  });

  it('should handle empty content', () => {
    const { container } = render(<MarkdownRenderer content="" />);
    const markdownContent = container.querySelector('.markdown-content');
    expect(markdownContent).toBeInTheDocument();
  });

  it('should apply custom className', () => {
    render(<MarkdownRenderer content="Test" className="custom-class" />);
    const container = screen.getByText('Test').closest('.markdown-content');
    expect(container).toHaveClass('custom-class');
  });
});
</file>

<file path="__tests__/models.test.ts">
import {
  getModelsForProvider,
  getDefaultModel,
  getModelInfo,
} from '@/lib/models';

describe('Models', () => {
  describe('getModelsForProvider', () => {
    it('should return models for a valid provider', () => {
      const models = getModelsForProvider('openai');
      expect(models).toBeDefined();
      expect(Array.isArray(models)).toBe(true);
    });

    it('should return empty array for invalid provider', () => {
      const models = getModelsForProvider('invalid-provider');
      expect(models).toEqual([]);
    });
  });

  describe('getDefaultModel', () => {
    it('should return default model for a valid provider', () => {
      const model = getDefaultModel('openai');
      expect(model).toBeDefined();
      expect(typeof model).toBe('string');
    });

    it('should return empty string for invalid provider', () => {
      const model = getDefaultModel('invalid-provider');
      expect(model).toBe('');
    });
  });

  describe('getModelInfo', () => {
    it('should return model info for valid provider and model', () => {
      const defaultModel = getDefaultModel('openai');
      if (defaultModel) {
        const modelInfo = getModelInfo('openai', defaultModel);
        expect(modelInfo).toBeDefined();
        if (modelInfo) {
          expect(modelInfo).toHaveProperty('id');
          expect(modelInfo).toHaveProperty('name');
          expect(modelInfo).toHaveProperty('description');
        }
      }
    });

    it('should return undefined for invalid model', () => {
      const modelInfo = getModelInfo('openai', 'invalid-model-id');
      expect(modelInfo).toBeUndefined();
    });
  });
});
</file>

<file path="__tests__/providers.test.ts">
import {
  providerConfigs,
  getEnvApiKey,
  type ProviderId,
} from '@/lib/providers';

describe('Providers', () => {
  describe('providerConfigs', () => {
    it('should have all expected providers', () => {
      const expectedProviders: ProviderId[] = [
        'openai',
        'gemini',
        'anthropic',
        'deepseek',
        'qwen',
        'vercel-ai-gateway',
      ];

      expectedProviders.forEach((provider) => {
        expect(providerConfigs[provider]).toBeDefined();
        expect(providerConfigs[provider]).toHaveProperty('createProvider');
        expect(providerConfigs[provider]).toHaveProperty('defaultModel');
      });
    });

    it('should create provider with API key', () => {
      const testApiKey = 'test-api-key-123';
      const provider = providerConfigs.openai.createProvider(testApiKey);
      expect(provider).toBeDefined();
    });
  });

  describe('getEnvApiKey', () => {
    it('should return undefined when env var is not set', () => {
      // Clear the env var for testing
      const originalEnv = process.env.OPENAI_API_KEY;
      delete process.env.OPENAI_API_KEY;

      const apiKey = getEnvApiKey('openai');
      expect(apiKey).toBeUndefined();

      // Restore original env
      if (originalEnv) {
        process.env.OPENAI_API_KEY = originalEnv;
      }
    });

    it('should map provider IDs to correct env var names', () => {
      const testCases: Array<{ provider: ProviderId; expectedEnv: string }> = [
        { provider: 'openai', expectedEnv: 'OPENAI_API_KEY' },
        { provider: 'gemini', expectedEnv: 'GOOGLE_GENERATIVE_AI_API_KEY' },
        { provider: 'anthropic', expectedEnv: 'ANTHROPIC_API_KEY' },
        { provider: 'deepseek', expectedEnv: 'DEEPSEEK_API_KEY' },
        { provider: 'qwen', expectedEnv: 'QWEN_API_KEY' },
        {
          provider: 'vercel-ai-gateway',
          expectedEnv: 'VERCEL_AI_GATEWAY_API_KEY',
        },
      ];

      testCases.forEach(({ provider, expectedEnv }) => {
        // This test verifies the mapping exists, not that the env var is set
        const envKey = expectedEnv;
        expect(envKey).toBeDefined();
      });
    });
  });
});
</file>

<file path="__tests__/ThemeSwitcher.test.tsx">
import '@testing-library/jest-dom';
import { render, screen } from '@testing-library/react';
import ThemeSwitcher from '@/components/ThemeSwitcher';

// Mock next-themes since it uses client-side hooks
jest.mock('next-themes', () => ({
  useTheme: () => ({
    theme: 'light',
    setTheme: jest.fn(),
    themes: ['light', 'dark', 'system'],
  }),
  ThemeProvider: ({ children }: { children: React.ReactNode }) => children,
}));

describe('ThemeSwitcher', () => {
  it('should render the theme switcher', () => {
    render(<ThemeSwitcher />);
    // The component should render without errors
    expect(document.body).toBeInTheDocument();
  });
});
</file>

<file path="components/MarkdownRenderer.tsx">
'use client';

import { marked } from 'marked';
import { useEffect, useMemo } from 'react';

interface MarkdownRendererProps {
  content: string;
  className?: string;
}

export function MarkdownRenderer({
  content,
  className = '',
}: MarkdownRendererProps) {
  // Configure marked options
  useEffect(() => {
    marked.setOptions({
      breaks: true, // Convert line breaks to <br>
      gfm: true, // GitHub Flavored Markdown
    });
  }, []);

  // Convert markdown to HTML
  const htmlContent = useMemo(() => {
    try {
      return marked.parse(content) as string;
    } catch (error) {
      console.error('Error parsing markdown:', error);
      return content; // Fallback to raw content
    }
  }, [content]);

  return (
    <div
      className={`markdown-content ${className}`}
      dangerouslySetInnerHTML={{ __html: htmlContent }}
    />
  );
}
</file>

<file path="components/VercelGatewayFallbackModels.tsx">
'use client';

import { useState, useEffect } from 'react';
import { X, ChevronDown } from 'lucide-react';
import { getModelsForProvider, type ModelInfo } from '@/lib/models';

interface VercelGatewayFallbackModelsProps {
  fallbackModels: string[];
  onFallbackModelsChange: (models: string[]) => void;
}

const STORAGE_KEY = 'vercel-gateway-fallback-models';

export function VercelGatewayFallbackModels({
  fallbackModels,
  onFallbackModelsChange,
}: VercelGatewayFallbackModelsProps) {
  const [isOpen, setIsOpen] = useState(false);
  const [availableModels] = useState<ModelInfo[]>(() =>
    getModelsForProvider('vercel-ai-gateway'),
  );

  const addModel = (modelId: string) => {
    if (!fallbackModels.includes(modelId)) {
      onFallbackModelsChange([...fallbackModels, modelId]);
    }
  };

  const removeModel = (modelId: string) => {
    onFallbackModelsChange(fallbackModels.filter((id) => id !== modelId));
  };

  const availableToAdd = availableModels.filter(
    (model) => !fallbackModels.includes(model.id),
  );

  return (
    <div className="flex flex-col gap-2">
      <button
        type="button"
        onClick={() => setIsOpen(!isOpen)}
        className="flex items-center justify-between w-full max-w-md px-3 py-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 hover:bg-zinc-50 dark:hover:bg-zinc-700 transition-colors"
      >
        <span className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
          Fallback Models ({fallbackModels.length})
        </span>
        <ChevronDown
          size={16}
          className={`text-zinc-500 transition-transform ${
            isOpen ? 'rotate-180' : ''
          }`}
        />
      </button>

      {isOpen && (
        <div className="flex flex-col gap-3 p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-zinc-50 dark:bg-zinc-900 max-w-md">
          {/* Current Fallback Models */}
          {fallbackModels.length > 0 ? (
            <div className="flex flex-col gap-2">
              <label className="text-xs font-medium text-zinc-700 dark:text-zinc-300">
                Current Fallback Models (in order):
              </label>
              <div className="flex flex-col gap-1.5">
                {fallbackModels.map((modelId, index) => {
                  const model = availableModels.find((m) => m.id === modelId);
                  return (
                    <div
                      key={modelId}
                      className="flex items-center justify-between p-2 rounded border border-zinc-200 dark:border-zinc-700 bg-white dark:bg-zinc-800"
                    >
                      <div className="flex items-center gap-2">
                        <span className="text-xs text-zinc-500 dark:text-zinc-400">
                          {index + 1}.
                        </span>
                        <span className="text-sm text-zinc-900 dark:text-zinc-100">
                          {model?.name || modelId}
                        </span>
                      </div>
                      <button
                        type="button"
                        onClick={() => removeModel(modelId)}
                        className="p-1 rounded hover:bg-zinc-100 dark:hover:bg-zinc-700 text-zinc-500 hover:text-red-600 dark:hover:text-red-400 transition-colors"
                        title="Remove"
                      >
                        <X size={14} />
                      </button>
                    </div>
                  );
                })}
              </div>
            </div>
          ) : (
            <p className="text-sm text-zinc-500 dark:text-zinc-400">
              No fallback models configured. Add models below.
            </p>
          )}

          {/* Add Model Dropdown */}
          {availableToAdd.length > 0 && (
            <div className="flex flex-col gap-2">
              <label className="text-xs font-medium text-zinc-700 dark:text-zinc-300">
                Add Fallback Model:
              </label>
              <div className="relative">
                <select
                  onChange={(e) => {
                    if (e.target.value) {
                      addModel(e.target.value);
                      e.target.value = ''; // Reset selection
                    }
                  }}
                  className="w-full appearance-none px-3 py-2 pr-10 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 text-sm focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent cursor-pointer"
                  defaultValue=""
                >
                  <option value="">Select a model to add...</option>
                  {availableToAdd.map((model) => (
                    <option key={model.id} value={model.id}>
                      {model.name}{' '}
                      {model.description && `- ${model.description}`}
                    </option>
                  ))}
                </select>
                <ChevronDown
                  size={16}
                  className="absolute right-3 top-1/2 -translate-y-1/2 text-zinc-500 pointer-events-none"
                />
              </div>
            </div>
          )}

          {availableToAdd.length === 0 && fallbackModels.length > 0 && (
            <p className="text-xs text-zinc-500 dark:text-zinc-400">
              All available models have been added.
            </p>
          )}

          {/* Info */}
          <p className="text-xs text-zinc-500 dark:text-zinc-400 mt-1">
            Fallback models are used in order if the primary model is
            unavailable.
          </p>
        </div>
      )}
    </div>
  );
}

// Hook to load fallback models from localStorage
export function useVercelGatewayFallbackModels(): [
  string[],
  (models: string[]) => void,
] {
  const [fallbackModels, setFallbackModels] = useState<string[]>([]);

  useEffect(() => {
    const saved = localStorage.getItem(STORAGE_KEY);
    if (saved) {
      try {
        const parsed = JSON.parse(saved);
        if (Array.isArray(parsed)) {
          setFallbackModels(parsed);
        }
      } catch (e) {
        console.error('Failed to parse saved fallback models', e);
      }
    }
  }, []);

  const updateFallbackModels = (models: string[]) => {
    setFallbackModels(models);
    if (models.length > 0) {
      localStorage.setItem(STORAGE_KEY, JSON.stringify(models));
    } else {
      localStorage.removeItem(STORAGE_KEY);
    }
  };

  return [fallbackModels, updateFallbackModels];
}
</file>

<file path="lib/ai-handler.ts">
import {
  streamText,
  generateText,
  convertToModelMessages,
  UIMessage,
  stepCountIs,
  type LanguageModel,
  type ToolSet,
  type StopCondition,
} from 'ai';
import { NextResponse } from 'next/server';
import { providerConfigs, ProviderId, getEnvApiKey } from '@/lib/providers';
import { deductCredits, getCredits } from '@/lib/billing';

export interface AIHandlerOptions {
  messages: UIMessage[] | Array<{ role: string; content: string }>;
  provider?: ProviderId;
  apiKey?: string;
  model?: string;
  stream?: boolean;
  systemPrompt?: string;
  tools?: ToolSet;
  stopWhen?: StopCondition<any> | Array<StopCondition<any>>;
  enableUsageMetadata?: boolean; // Whether to include usage in streaming response metadata
  logPrefix?: string; // Prefix for log messages
  enableStepLogging?: boolean; // Whether to log tool calls and results
}

export async function handleAIRequest(options: AIHandlerOptions) {
  const {
    messages: rawMessages,
    provider: providerId = 'openai',
    apiKey,
    model: requestedModel,
    stream = true,
    systemPrompt,
    tools,
    stopWhen,
    enableUsageMetadata = false,
    logPrefix = 'AI',
    enableStepLogging = false,
  } = options;

  // Check credits
  if (getCredits() <= 0) {
    return NextResponse.json(
      { error: 'Insufficient credits' },
      { status: 402 },
    );
  }

  // Validate messages
  if (!rawMessages || !Array.isArray(rawMessages)) {
    return NextResponse.json(
      { error: 'Messages are required and must be an array' },
      { status: 400 },
    );
  }

  // Convert messages to ModelMessage[] format
  // Streaming sends UIMessage[] format, non-streaming sends {role, content} format
  const messages = stream
    ? convertToModelMessages(rawMessages as UIMessage[])
    : (rawMessages as Array<{ role: string; content: string }>).map((m) => ({
        role: m.role as 'user' | 'assistant' | 'system',
        content: m.content,
      }));

  // Validate provider
  if (!providerConfigs[providerId]) {
    return NextResponse.json({ error: 'Invalid provider' }, { status: 400 });
  }

  const config = providerConfigs[providerId];

  // Log for debugging
  console.log(
    `[${logPrefix}] Provider: ${providerId}, Model: ${requestedModel || config.defaultModel}, Stream: ${stream}, Has API Key: ${!!apiKey}`,
  );

  // Get API key from request or environment
  const resolvedApiKey = apiKey || getEnvApiKey(providerId);

  if (!resolvedApiKey) {
    return NextResponse.json(
      {
        error: `No API key provided for ${providerId}. Please add it in Settings.`,
      },
      { status: 400 },
    );
  }

  // Log for debugging
  console.log(
    `[${logPrefix}] Requested model: ${requestedModel || config.defaultModel}, API key source: ${apiKey ? 'request body' : 'environment'}`,
  );

  const provider = config.createProvider(resolvedApiKey);
  const modelName = requestedModel || config.defaultModel;
  // Type assertion: providers return LanguageModelV1 | LanguageModelV2, but streamText expects LanguageModel
  const model = provider(modelName) as any as LanguageModel;

  if (stream) {
    // Streaming mode
    const result = streamText({
      model,
      system: systemPrompt,
      ...(tools && { tools }),
      messages,
      ...(stopWhen && { stopWhen }),
      onFinish: ({ usage, finishReason }) => {
        console.log(
          `[${logPrefix}] Stream finished. Reason: ${finishReason}, Tokens: ${usage.inputTokens}/${usage.outputTokens}`,
        );
        deductCredits(
          modelName,
          usage.inputTokens ?? 0,
          usage.outputTokens ?? 0,
        );
      },
      ...(enableStepLogging && {
        onStepFinish: ({ text, toolCalls, toolResults, finishReason }) => {
          if (toolCalls && toolCalls.length > 0) {
            console.log(
              `[${logPrefix}] Tool calls made: ${toolCalls.map((tc) => tc.toolName).join(', ')}`,
            );
          }
          if (toolResults && toolResults.length > 0) {
            console.log(
              `[${logPrefix}] Tool results received: ${toolResults.length} results, Step finish reason: ${finishReason}`,
            );
          }
        },
      }),
    });

    return result.toUIMessageStreamResponse({
      ...(enableUsageMetadata && {
        messageMetadata: ({ part }) => {
          // Include usage information when available
          if (part.type === 'finish') {
            return {
              usage: {
                promptTokens: part.totalUsage?.inputTokens ?? 0,
                completionTokens: part.totalUsage?.outputTokens ?? 0,
                totalTokens:
                  part.totalUsage?.totalTokens ??
                  (part.totalUsage?.inputTokens ?? 0) +
                    (part.totalUsage?.outputTokens ?? 0),
              },
            };
          }
          return undefined;
        },
      }),
      onFinish: ({ messages, responseMessage }) => {
        console.log(
          `[${logPrefix}] Stream completed. Response message ID: ${responseMessage.id}`,
        );
      },
    });
  } else {
    // Non-streaming mode - better for usage tracking
    const result = await generateText({
      model,
      system: systemPrompt,
      ...(tools && { tools }),
      messages,
    });

    // Accurate usage tracking - AI SDK v5 uses inputTokens/outputTokens
    const promptTokens = result.usage.inputTokens ?? 0;
    const completionTokens = result.usage.outputTokens ?? 0;
    const billingResult = deductCredits(
      modelName,
      promptTokens,
      completionTokens,
    );

    return NextResponse.json({
      text: result.text,
      usage: {
        promptTokens,
        completionTokens,
        totalTokens: promptTokens + completionTokens,
      },
      billing: billingResult,
    });
  }
}
</file>

<file path="lib/models.ts">
import modelsConfig from './models.json';

export interface ModelInfo {
  id: string;
  name: string;
  description: string;
}

export interface ProviderModels {
  defaultModel: string;
  models: ModelInfo[];
}

export type ModelsConfig = Record<string, ProviderModels>;

export const models = modelsConfig as ModelsConfig;

export function getModelsForProvider(providerId: string): ModelInfo[] {
  return models[providerId]?.models || [];
}

export function getDefaultModel(providerId: string): string {
  return models[providerId]?.defaultModel || '';
}

export function getModelInfo(
  providerId: string,
  modelId: string,
): ModelInfo | undefined {
  return models[providerId]?.models.find((m) => m.id === modelId);
}
</file>

<file path=".prettierrc.json">
{
  "singleQuote": true
}
</file>

<file path="eslint.config.mjs">
import { defineConfig, globalIgnores } from 'eslint/config';
import nextVitals from 'eslint-config-next/core-web-vitals';
import nextTs from 'eslint-config-next/typescript';

const eslintConfig = defineConfig([
  ...nextVitals,
  ...nextTs,
  // Override default ignores of eslint-config-next.
  globalIgnores([
    // Default ignores of eslint-config-next:
    '.next/**',
    'out/**',
    'build/**',
    'next-env.d.ts',
  ]),
]);

export default eslintConfig;
</file>

<file path="jest.setup.ts">
import '@testing-library/jest-dom';

// Polyfill for TransformStream which is not available in jsdom environment
if (typeof globalThis.TransformStream === 'undefined') {
  // @ts-ignore - TransformStream polyfill
  globalThis.TransformStream = class TransformStream {
    constructor() {
      // Minimal polyfill - actual implementation not needed for tests
    }
  };
}
</file>

<file path="next.config.ts">
import type { NextConfig } from 'next';

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;
</file>

<file path="postcss.config.mjs">
const config = {
  plugins: {
    '@tailwindcss/postcss': {},
  },
};

export default config;
</file>

<file path="lib/providers.ts">
import { createOpenAI } from '@ai-sdk/openai';
import { createGoogleGenerativeAI } from '@ai-sdk/google';
import { createAnthropic } from '@ai-sdk/anthropic';
import { createDeepSeek } from '@ai-sdk/deepseek';
import { createQwen } from 'qwen-ai-provider';
import { getDefaultModel } from '@/lib/models';

// Provider configurations - exported for reuse
export const providerConfigs = {
  openai: {
    createProvider: (apiKey: string) => createOpenAI({ apiKey }),
    defaultModel: getDefaultModel('openai'),
  },
  gemini: {
    createProvider: (apiKey: string) => createGoogleGenerativeAI({ apiKey }),
    defaultModel: getDefaultModel('gemini'),
  },
  anthropic: {
    createProvider: (apiKey: string) => createAnthropic({ apiKey }),
    defaultModel: getDefaultModel('anthropic'),
  },
  deepseek: {
    createProvider: (apiKey: string) =>
      createDeepSeek({
        apiKey,
        baseURL: 'https://api.deepseek.com',
      }),
    defaultModel: getDefaultModel('deepseek'),
  },
  qwen: {
    createProvider: (apiKey: string) => createQwen({ apiKey }),
    defaultModel: getDefaultModel('qwen'),
  },
  'vercel-ai-gateway': {
    createProvider: (apiKey: string) =>
      createOpenAI({
        apiKey,
        baseURL: 'https://ai-gateway.vercel.sh/v1',
      }),
    defaultModel: getDefaultModel('vercel-ai-gateway'),
  },
};

export type ProviderId = keyof typeof providerConfigs;

export function getEnvApiKey(providerId: ProviderId): string | undefined {
  const envKeys: Record<ProviderId, string> = {
    openai: 'OPENAI_API_KEY',
    gemini: 'GOOGLE_GENERATIVE_AI_API_KEY',
    anthropic: 'ANTHROPIC_API_KEY',
    deepseek: 'DEEPSEEK_API_KEY',
    qwen: 'QWEN_API_KEY',
    'vercel-ai-gateway': 'VERCEL_AI_GATEWAY_API_KEY',
  };
  return process.env[envKeys[providerId]];
}
</file>

<file path="jest.config.ts">
import type { Config } from 'jest';
import nextJest from 'next/jest.js';

const createJestConfig = nextJest({
  // Provide the path to your Next.js app to load next.config.js and .env files in your test environment
  dir: './',
});

// Add any custom config to be passed to Jest
const config: Config = {
  coverageProvider: 'v8',
  testEnvironment: 'jsdom',
  // Add more setup options before each test is run
  setupFilesAfterEnv: ['<rootDir>/jest.setup.ts'],
  // Handle module path aliases (matching tsconfig.json paths)
  moduleNameMapper: {
    '^@/(.*)$': '<rootDir>/$1',
  },
  // Mock web streams API for tests
  testEnvironmentOptions: {
    customExportConditions: [''],
  },
  // Transform ES modules from node_modules (marked uses ESM)
  transformIgnorePatterns: ['node_modules/(?!(marked)/)'],
  // Extensions to treat as ESM
  extensionsToTreatAsEsm: ['.ts', '.tsx'],
};

// createJestConfig is exported this way to ensure that next/jest can load the Next.js config which is async
export default createJestConfig(config);
</file>

<file path="README.md">
# AI Code Reviewer

A Next.js application that uses AI to review code files and GitHub pull requests. Supports multiple AI providers with streaming responses, markdown rendering, and a credit-based billing system.

## Features

- ü§ñ **Multi-Provider AI Support**: Choose from OpenAI, Google Gemini, Anthropic Claude, DeepSeek, Qwen, and Vercel AI Gateway
- üìù **Code Review**: Review local files or GitHub pull requests
- üí¨ **Chat Interface**: General-purpose AI chat with optional file reading tools
- üìä **Usage Tracking**: Credit-based billing system with cost tracking
- üåì **Dark Mode**: Built-in theme switcher
- üì± **Responsive Design**: Works on desktop and mobile devices
- ‚ö° **Streaming Responses**: Real-time streaming for better UX
- üé® **Markdown Rendering**: Beautiful markdown rendering for AI responses
- üß™ **Testing**: Jest test suite included

## Tech Stack

- **Framework**: Next.js 16 (App Router)
- **Language**: TypeScript
- **Styling**: Tailwind CSS 4
- **AI SDK**: Vercel AI SDK v5
- **Markdown**: Marked
- **Testing**: Jest + React Testing Library
- **Package Manager**: pnpm

## Getting Started

### Prerequisites

- Node.js 20+
- pnpm (recommended) or npm/yarn

### Installation

1. Clone the repository:

```bash
git clone <repository-url>
cd my-ai-app
```

2. Install dependencies:

```bash
pnpm install
```

3. Set up environment variables (optional - you can also add API keys in Settings):

```bash
cp .env.example .env
```

Add your API keys:

```env
OPENAI_API_KEY=your_key_here
GOOGLE_GENERATIVE_AI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here
DEEPSEEK_API_KEY=your_key_here
QWEN_API_KEY=your_key_here
VERCEL_AI_GATEWAY_API_KEY=your_key_here
```

4. Run the development server:

```bash
pnpm dev
```

5. Open [http://localhost:3000](http://localhost:3000) in your browser.

## Usage

### Code Review

1. Navigate to the home page (`/`)
2. Select your preferred AI provider and model
3. Choose input mode:
   - **File Path**: Enter a local file path to review
   - **PR Link**: Enter a GitHub PR URL (e.g., `https://github.com/owner/repo/pull/123`)
4. Optionally customize the system prompt
5. Click "Review Code" to start the review

### Chat

1. Navigate to `/chat`
2. Select your AI provider and model
3. Optionally enable file reading tools
4. Start chatting with the AI

### Settings

1. Navigate to `/settings`
2. Add or update API keys for different providers
3. Keys are stored in localStorage

## Project Structure

```
my-ai-app/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ billing/        # Billing API endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/          # Chat API endpoint
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ codereview/    # Code review API endpoint
‚îÇ   ‚îú‚îÄ‚îÄ chat/              # Chat page
‚îÇ   ‚îú‚îÄ‚îÄ settings/          # Settings page
‚îÇ   ‚îî‚îÄ‚îÄ page.tsx           # Code review page (home)
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ AISettingsPanel.tsx    # AI provider/model selector
‚îÇ   ‚îú‚îÄ‚îÄ MarkdownRenderer.tsx   # Markdown to HTML renderer
‚îÇ   ‚îî‚îÄ‚îÄ ThemeSwitcher.tsx      # Dark mode toggle
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îî‚îÄ‚îÄ useAIChat.ts           # Custom hook for AI chat
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ ai-handler.ts         # Shared AI request handler
‚îÇ   ‚îú‚îÄ‚îÄ billing.ts            # Credit/billing logic
‚îÇ   ‚îú‚îÄ‚îÄ models.ts              # Model configuration
‚îÇ   ‚îî‚îÄ‚îÄ providers.ts           # AI provider configurations
‚îî‚îÄ‚îÄ __tests__/                 # Jest test files
```

## API Routes

### `/api/codereview`

Code review endpoint with file reading and PR reading tools.

**Request Body:**

```json
{
  "messages": [...],
  "provider": "openai",
  "model": "gpt-4",
  "apiKey": "optional-api-key",
  "stream": true,
  "systemPrompt": "Custom system prompt"
}
```

### `/api/chat`

General chat endpoint with optional file reading tools.

**Request Body:**

```json
{
  "messages": [...],
  "provider": "openai",
  "model": "gpt-4",
  "apiKey": "optional-api-key",
  "stream": true,
  "systemPrompt": "Custom system prompt",
  "enableTools": false
}
```

### `/api/billing`

Get current credit balance and usage history.

## Available Scripts

- `pnpm dev` - Start development server
- `pnpm build` - Build for production
- `pnpm start` - Start production server
- `pnpm lint` - Run ESLint
- `pnpm format` - Format code with Prettier
- `pnpm test` - Run Jest tests
- `pnpm test:watch` - Run tests in watch mode
- `pnpm test:coverage` - Run tests with coverage report

## Testing

The project includes Jest tests for:

- Model utilities (`lib/models.ts`)
- Provider configurations (`lib/providers.ts`)
- Markdown renderer component
- Theme switcher component

Run tests:

```bash
pnpm test
```

## Supported AI Providers

See [`lib/models.json`](./lib/models.json) for the complete list of supported models for each provider.

| Provider          | Notes                    |
| ----------------- | ------------------------ |
| OpenAI            | Requires API key         |
| Google Gemini     | Requires API key         |
| Anthropic         | Requires API key         |
| DeepSeek          | Requires API key         |
| Qwen              | Requires API key         |
| Vercel AI Gateway | Requires gateway API key |

## Features in Detail

### Code Review Tools

- **readFile**: Reads local file contents
- **readPullRequest**: Fetches and parses GitHub PR files (public repos only)

### Billing System

- Credit-based system
- Tracks usage per model
- Cost calculation based on input/output tokens
- Usage history tracking

### Markdown Rendering

- Full markdown support (headings, lists, code blocks, tables, etc.)
- Syntax highlighting ready
- Dark mode compatible
- Compact spacing for readability

## Environment Variables

All API keys can be set via environment variables or through the Settings page:

- `OPENAI_API_KEY`
- `GOOGLE_GENERATIVE_AI_API_KEY`
- `ANTHROPIC_API_KEY`
- `DEEPSEEK_API_KEY`
- `QWEN_API_KEY`
- `VERCEL_AI_GATEWAY_API_KEY`

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

[Add your license here]

## Acknowledgments

- Built with [Next.js](https://nextjs.org)
- AI powered by [Vercel AI SDK](https://sdk.vercel.ai)
- Styled with [Tailwind CSS](https://tailwindcss.com)
</file>

<file path="tailwind.config.ts">
import type { Config } from 'tailwindcss';

const config: Config = {
  darkMode: 'class',
  content: [
    './pages/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    './app/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {},
  },
  plugins: [],
};
export default config;
</file>

<file path="app/api/billing/route.ts">
import { getCredits } from '@/lib/billing';
import { NextResponse } from 'next/server';

export function GET() {
  const credits = getCredits();
  return NextResponse.json({ credits });
}
</file>

<file path="components/ThemeSwitcher.tsx">
'use client';

import { useTheme } from 'next-themes';
import { useState, useEffect } from 'react';
import { Sun, Moon } from 'lucide-react';

const ThemeSwitcher = () => {
  const [mounted, setMounted] = useState(false);
  const { theme, setTheme } = useTheme();

  useEffect(() => {
    setMounted(true);
  }, []);

  if (!mounted) {
    return null;
  }

  return (
    <button
      onClick={() => setTheme(theme === 'dark' ? 'light' : 'dark')}
      className="flex items-center gap-2 px-3 py-2 text-sm font-medium text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white"
    >
      {theme === 'dark' ? <Sun size={16} /> : <Moon size={16} />}
      {theme === 'dark' ? 'Light' : 'Dark'} Mode
    </button>
  );
};

export default ThemeSwitcher;
</file>

<file path="lib/billing.ts">
// In a real-world application, this data should be stored in a database.
// This is a simulation for demonstration purposes.
// Using in-memory storage - will reset on server restart.

export const modelCosts: Record<string, { input: number; output: number }> = {
  'gpt-4o': {
    input: 0.005 / 1000, // Cost per 1000 tokens
    output: 0.015 / 1000, // Cost per 1000 tokens
  },
  'gemini-1.5-pro': {
    input: 0.00125 / 1000,
    output: 0.005 / 1000,
  },
  'claude-sonnet-4-20250514': {
    input: 0.003 / 1000,
    output: 0.015 / 1000,
  },
  'deepseek-chat': {
    input: 0.00014 / 1000,
    output: 0.00028 / 1000,
  },
  'qwen-plus': {
    input: 0.0008 / 1000,
    output: 0.002 / 1000,
  },
};

export let userCredits = 1.0; // Initial credits in USD

// Usage history for tracking (in-memory)
export const usageHistory: Array<{
  timestamp: Date;
  model: string;
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
  cost: number;
}> = [];

export function deductCredits(
  model: string,
  promptTokens: number,
  completionTokens: number,
): { cost: number; remaining: number } {
  const costs = modelCosts[model] || modelCosts['gpt-4o'];
  const cost = promptTokens * costs.input + completionTokens * costs.output;
  userCredits -= cost;

  // Track usage
  usageHistory.push({
    timestamp: new Date(),
    model,
    promptTokens,
    completionTokens,
    totalTokens: promptTokens + completionTokens,
    cost,
  });

  return { cost, remaining: userCredits };
}

export function getCredits() {
  return userCredits;
}

export function getUsageHistory() {
  return usageHistory;
}
</file>

<file path="lib/models.json">
{
  "openai": {
    "defaultModel": "gpt-5-mini",
    "models": [
      {
        "id": "gpt-5.2",
        "name": "GPT-5.2",
        "description": "The best model for coding and agentic tasks across industries"
      },
      {
        "id": "gpt-5.2-pro",
        "name": "GPT-5.2 Pro",
        "description": "The smartest and most precise model"
      },
      {
        "id": "gpt-5-mini",
        "name": "GPT-5 Mini",
        "description": "A faster, cheaper version of GPT-5 for well-defined tasks"
      },
      {
        "id": "gpt-4.1",
        "name": "GPT-4.1",
        "description": "Enhanced GPT-4 model"
      },
      {
        "id": "gpt-4.1-mini",
        "name": "GPT-4.1 Mini",
        "description": "Faster GPT-4.1 variant"
      },
      {
        "id": "gpt-4.1-nano",
        "name": "GPT-4.1 Nano",
        "description": "Lightweight GPT-4.1 variant"
      }
    ]
  },
  "gemini": {
    "defaultModel": "gemini-1.5-pro",
    "models": [
      {
        "id": "gemini-1.5-pro",
        "name": "Gemini 1.5 Pro",
        "description": "Most capable model"
      },
      {
        "id": "gemini-1.5-flash",
        "name": "Gemini 1.5 Flash",
        "description": "Faster and cheaper"
      },
      {
        "id": "gemini-pro",
        "name": "Gemini Pro",
        "description": "Previous generation"
      }
    ]
  },
  "anthropic": {
    "defaultModel": "claude-sonnet-4-20250514",
    "models": [
      {
        "id": "claude-sonnet-4-20250514",
        "name": "Claude Sonnet 4",
        "description": "Latest Claude model"
      },
      {
        "id": "claude-3-5-sonnet-20241022",
        "name": "Claude 3.5 Sonnet",
        "description": "Previous generation"
      },
      {
        "id": "claude-3-opus-20240229",
        "name": "Claude 3 Opus",
        "description": "Most capable"
      },
      {
        "id": "claude-3-sonnet-20240229",
        "name": "Claude 3 Sonnet",
        "description": "Balanced performance"
      },
      {
        "id": "claude-3-haiku-20240307",
        "name": "Claude 3 Haiku",
        "description": "Fast and affordable"
      }
    ]
  },
  "deepseek": {
    "defaultModel": "deepseek-coder",
    "models": [
      {
        "id": "deepseek-coder",
        "name": "DeepSeek Coder",
        "description": "Optimized for code"
      },
      {
        "id": "deepseek-chat",
        "name": "DeepSeek Chat",
        "description": "Standard model"
      }
    ]
  },
  "qwen": {
    "defaultModel": "qwen-plus",
    "models": [
      {
        "id": "qwen-plus",
        "name": "Qwen Plus",
        "description": "Enhanced model"
      },
      {
        "id": "qwen-turbo",
        "name": "Qwen Turbo",
        "description": "Faster model"
      },
      { "id": "qwen-max", "name": "Qwen Max", "description": "Most capable" }
    ]
  },
  "vercel-ai-gateway": {
    "defaultModel": "openai/gpt-5-mini",
    "models": [
      {
        "id": "openai/gpt-5.2",
        "name": "OpenAI GPT-5.2",
        "description": "The best model for coding and agentic tasks across industries"
      },
      {
        "id": "openai/gpt-5.2-pro",
        "name": "OpenAI GPT-5.2 Pro",
        "description": "The smartest and most precise model"
      },
      {
        "id": "openai/gpt-5-mini",
        "name": "OpenAI GPT-5 Mini",
        "description": "A faster, cheaper version of GPT-5 for well-defined tasks"
      },
      {
        "id": "openai/gpt-4.1",
        "name": "OpenAI GPT-4.1",
        "description": "Enhanced GPT-4 model"
      },
      {
        "id": "openai/gpt-4.1-mini",
        "name": "OpenAI GPT-4.1 Mini",
        "description": "Faster GPT-4.1 variant"
      },
      {
        "id": "openai/gpt-4.1-nano",
        "name": "OpenAI GPT-4.1 Nano",
        "description": "Lightweight GPT-4.1 variant"
      },
      {
        "id": "google/gemini-1.5-pro",
        "name": "Google Gemini 1.5 Pro",
        "description": "Most capable model"
      },
      {
        "id": "google/gemini-1.5-flash",
        "name": "Google Gemini 1.5 Flash",
        "description": "Faster and cheaper"
      },
      {
        "id": "google/gemini-pro",
        "name": "Google Gemini Pro",
        "description": "Previous generation"
      },
      {
        "id": "anthropic/claude-sonnet-4-20250514",
        "name": "Anthropic Claude Sonnet 4",
        "description": "Latest Claude model"
      },
      {
        "id": "anthropic/claude-3-5-sonnet-20241022",
        "name": "Anthropic Claude 3.5 Sonnet",
        "description": "Previous generation"
      },
      {
        "id": "anthropic/claude-3-opus-20240229",
        "name": "Anthropic Claude 3 Opus",
        "description": "Most capable"
      },
      {
        "id": "anthropic/claude-3-sonnet-20240229",
        "name": "Anthropic Claude 3 Sonnet",
        "description": "Balanced performance"
      },
      {
        "id": "anthropic/claude-3-haiku-20240307",
        "name": "Anthropic Claude 3 Haiku",
        "description": "Fast and affordable"
      },
      {
        "id": "deepseek/deepseek-coder",
        "name": "DeepSeek Coder",
        "description": "Optimized for code"
      },
      {
        "id": "deepseek/deepseek-chat",
        "name": "DeepSeek Chat",
        "description": "Standard model"
      },
      {
        "id": "qwen/qwen-plus",
        "name": "Qwen Plus",
        "description": "Enhanced model"
      },
      {
        "id": "qwen/qwen-turbo",
        "name": "Qwen Turbo",
        "description": "Faster model"
      },
      {
        "id": "qwen/qwen-max",
        "name": "Qwen Max",
        "description": "Most capable"
      }
    ]
  }
}
</file>

<file path="app/settings/page.tsx">
'use client';

import { useState, useEffect } from 'react';
import Link from 'next/link';
import { Eye, EyeOff, Save, ArrowLeft, Sparkles } from 'lucide-react';

interface ApiKeyConfig {
  id: string;
  name: string;
  placeholder: string;
  key: string;
}

const defaultProviders: ApiKeyConfig[] = [
  { id: 'openai', name: 'OpenAI', placeholder: 'sk-...', key: '' },
  { id: 'gemini', name: 'Google Gemini', placeholder: 'AIza...', key: '' },
  { id: 'deepseek', name: 'DeepSeek', placeholder: 'sk-...', key: '' },
  { id: 'anthropic', name: 'Anthropic', placeholder: 'sk-ant-...', key: '' },
  { id: 'qwen', name: 'Qwen (Alibaba)', placeholder: 'sk-...', key: '' },
  {
    id: 'vercel-ai-gateway',
    name: 'Vercel AI Gateway',
    placeholder: 'vag_...',
    key: '',
  },
];

export default function SettingsPage() {
  const [providers, setProviders] = useState<ApiKeyConfig[]>(defaultProviders);
  const [visibleKeys, setVisibleKeys] = useState<Record<string, boolean>>({});
  const [saved, setSaved] = useState(false);

  useEffect(() => {
    // Load saved keys from localStorage
    const savedKeys = localStorage.getItem('ai-api-keys');
    if (savedKeys) {
      try {
        const parsed = JSON.parse(savedKeys);
        setProviders((prev) =>
          prev.map((p) => ({
            ...p,
            key: parsed[p.id] || '',
          })),
        );
      } catch (e) {
        console.error('Failed to parse saved keys', e);
      }
    }
  }, []);

  const toggleVisibility = (id: string) => {
    setVisibleKeys((prev) => ({ ...prev, [id]: !prev[id] }));
  };

  const updateKey = (id: string, value: string) => {
    setProviders((prev) =>
      prev.map((p) => (p.id === id ? { ...p, key: value } : p)),
    );
    setSaved(false);
  };

  const handleSave = () => {
    const keysToSave = providers.reduce(
      (acc, p) => ({ ...acc, [p.id]: p.key }),
      {},
    );
    localStorage.setItem('ai-api-keys', JSON.stringify(keysToSave));
    setSaved(true);
    setTimeout(() => setSaved(false), 2000);
  };

  return (
    <div className="w-full max-w-3xl mx-auto py-16 px-4 sm:px-6 lg:px-8">
      <div className="flex flex-col gap-8">
        <div className="flex flex-col gap-2">
          <Link
            href="/"
            className="flex items-center gap-2 text-sm text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white w-fit"
          >
            <ArrowLeft size={16} />
            Back to Home
          </Link>
          <h1 className="text-3xl font-semibold leading-10 tracking-tight text-black dark:text-zinc-50">
            Settings
          </h1>
          <p className="text-lg leading-8 text-zinc-600 dark:text-zinc-400">
            Configure your AI provider API keys below.
          </p>
        </div>

        <div className="flex flex-col gap-6">
          <div className="flex items-center gap-2 text-zinc-800 dark:text-zinc-200">
            <Sparkles size={20} />
            <h2 className="text-xl font-medium">AI Provider API Keys</h2>
          </div>

          <div className="flex flex-col gap-4">
            {providers.map((provider) => (
              <div
                key={provider.id}
                className="flex flex-col gap-2 p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-white dark:bg-zinc-900"
              >
                <label
                  htmlFor={provider.id}
                  className="text-sm font-medium text-zinc-700 dark:text-zinc-300"
                >
                  {provider.name}
                </label>
                <div className="relative flex items-center">
                  <input
                    id={provider.id}
                    type={visibleKeys[provider.id] ? 'text' : 'password'}
                    value={provider.key}
                    onChange={(e) => updateKey(provider.id, e.target.value)}
                    placeholder={provider.placeholder}
                    className="w-full px-3 py-2 pr-10 text-sm border border-zinc-300 dark:border-zinc-700 rounded-md bg-zinc-50 dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 placeholder-zinc-400 dark:placeholder-zinc-500 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
                  />
                  <button
                    type="button"
                    onClick={() => toggleVisibility(provider.id)}
                    className="absolute right-2 p-1 text-zinc-500 hover:text-zinc-700 dark:hover:text-zinc-300"
                  >
                    {visibleKeys[provider.id] ? (
                      <EyeOff size={18} />
                    ) : (
                      <Eye size={18} />
                    )}
                  </button>
                </div>
              </div>
            ))}
          </div>

          <button
            onClick={handleSave}
            className="flex items-center justify-center gap-2 w-full sm:w-fit px-6 py-3 text-sm font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-lg transition-colors"
          >
            <Save size={16} />
            {saved ? 'Saved!' : 'Save API Keys'}
          </button>

          <p className="text-xs text-zinc-500 dark:text-zinc-500">
            API keys are stored locally in your browser. They are never sent to
            our servers.
          </p>
        </div>
      </div>
    </div>
  );
}
</file>

<file path="components/AISettingsPanel.tsx">
'use client';

import { useState, useEffect, useMemo } from 'react';
import { ChevronDown, Zap, Clock } from 'lucide-react';
import {
  getModelsForProvider,
  getDefaultModel,
  type ModelInfo,
} from '@/lib/models';
import {
  VercelGatewayFallbackModels,
  useVercelGatewayFallbackModels,
} from './VercelGatewayFallbackModels';

export const providers = [
  { id: 'openai', name: 'OpenAI' },
  { id: 'gemini', name: 'Google Gemini' },
  { id: 'anthropic', name: 'Anthropic (Claude)' },
  { id: 'deepseek', name: 'DeepSeek' },
  { id: 'qwen', name: 'Qwen (Alibaba)' },
  { id: 'vercel-ai-gateway', name: 'Vercel AI Gateway' },
];

export interface UsageInfo {
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
}

export interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  usage?: UsageInfo;
}

export interface BillingData {
  totalCost: number;
  usageHistory: Array<{
    timestamp: string;
    model: string;
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
    cost: number;
  }>;
}

interface AISettingsPanelProps {
  selectedProvider: string;
  onProviderChange: (provider: string) => void;
  selectedModel?: string;
  onModelChange?: (model: string) => void;
  useStreaming: boolean;
  onStreamingChange: (streaming: boolean) => void;
  showCredits?: boolean;
  fallbackModels?: string[];
  onFallbackModelsChange?: (models: string[]) => void;
}

export function AISettingsPanel({
  selectedProvider,
  onProviderChange,
  selectedModel,
  onModelChange,
  useStreaming,
  onStreamingChange,
  showCredits = true,
  fallbackModels,
  onFallbackModelsChange,
}: AISettingsPanelProps) {
  const [billingData, setBillingData] = useState<BillingData | null>(null);
  const [apiKeys, setApiKeys] = useState<Record<string, string>>({});
  const [internalFallbackModels, setInternalFallbackModels] =
    useVercelGatewayFallbackModels();

  // Use prop fallback models if provided, otherwise use internal state
  const currentFallbackModels =
    fallbackModels !== undefined ? fallbackModels : internalFallbackModels;
  const handleFallbackModelsChange =
    onFallbackModelsChange || setInternalFallbackModels;

  // Get available models for the selected provider
  const availableModels = useMemo(
    () => getModelsForProvider(selectedProvider),
    [selectedProvider],
  );

  // Determine the current selected model (use prop or default)
  const currentModel = selectedModel || getDefaultModel(selectedProvider) || '';

  // Handle provider change - reset model to default
  const handleProviderChange = (provider: string) => {
    onProviderChange(provider);
    if (onModelChange) {
      onModelChange(getDefaultModel(provider));
    }
  };

  const fetchBilling = async () => {
    try {
      const response = await fetch('/api/billing');
      const data = await response.json();
      setBillingData(data);
    } catch (e) {
      console.error('Failed to fetch billing data', e);
    }
  };

  useEffect(() => {
    fetchBilling();

    // Load API keys from localStorage
    const savedKeys = localStorage.getItem('ai-api-keys');
    if (savedKeys) {
      try {
        setApiKeys(JSON.parse(savedKeys));
      } catch (e) {
        console.error('Failed to parse saved keys', e);
      }
    }
  }, []);

  const currentProvider = providers.find((p) => p.id === selectedProvider);

  return (
    <div className="flex flex-col gap-4">
      {/* Credits/Cost Display */}
      {showCredits && billingData && (
        <p className="text-lg leading-8 text-zinc-600 dark:text-zinc-400">
          Total Cost:{' '}
          <span className="font-semibold text-black dark:text-white">
            ${billingData?.totalCost?.toFixed(4) ?? 0}
          </span>
        </p>
      )}

      {/* Provider Selector */}
      <div className="flex flex-col gap-2">
        <label className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
          AI Provider
        </label>
        <div className="relative w-full max-w-md">
          <select
            value={selectedProvider}
            onChange={(e) => handleProviderChange(e.target.value)}
            className="w-full appearance-none px-3 py-2 pr-10 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent cursor-pointer"
          >
            {providers.map((provider) => (
              <option key={provider.id} value={provider.id}>
                {provider.name}
              </option>
            ))}
          </select>
          <ChevronDown
            size={16}
            className="absolute right-3 top-1/2 -translate-y-1/2 text-zinc-500 pointer-events-none"
          />
        </div>
        {!apiKeys[selectedProvider] && (
          <p className="text-xs text-amber-600 dark:text-amber-400">
            No API key set for {currentProvider?.name}. Add it in Settings.
          </p>
        )}
      </div>

      {/* Model Selector */}
      {availableModels.length > 0 && onModelChange && (
        <div className="flex flex-col gap-2">
          <label className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
            Model
          </label>
          <div className="relative w-full max-w-md">
            <select
              value={currentModel}
              onChange={(e) => onModelChange(e.target.value)}
              className="w-full appearance-none px-3 py-2 pr-10 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent cursor-pointer"
            >
              {availableModels.map((model) => (
                <option key={model.id} value={model.id}>
                  {model.name} {model.description && `- ${model.description}`}
                </option>
              ))}
            </select>
            <ChevronDown
              size={16}
              className="absolute right-3 top-1/2 -translate-y-1/2 text-zinc-500 pointer-events-none"
            />
          </div>
        </div>
      )}

      {/* Vercel AI Gateway Fallback Models */}
      {selectedProvider === 'vercel-ai-gateway' && (
        <VercelGatewayFallbackModels
          fallbackModels={currentFallbackModels}
          onFallbackModelsChange={handleFallbackModelsChange}
        />
      )}

      {/* Streaming Toggle */}
      <div className="flex items-center gap-3 max-w-md">
        <button
          type="button"
          onClick={() => onStreamingChange(true)}
          className={`flex items-center gap-2 px-3 py-2 rounded-md text-sm font-medium transition-colors ${
            useStreaming
              ? 'bg-blue-600 text-white'
              : 'bg-zinc-100 dark:bg-zinc-800 text-zinc-700 dark:text-zinc-300 hover:bg-zinc-200 dark:hover:bg-zinc-700'
          }`}
        >
          <Zap size={14} />
          Streaming
        </button>
        <button
          type="button"
          onClick={() => onStreamingChange(false)}
          className={`flex items-center gap-2 px-3 py-2 rounded-md text-sm font-medium transition-colors ${
            !useStreaming
              ? 'bg-blue-600 text-white'
              : 'bg-zinc-100 dark:bg-zinc-800 text-zinc-700 dark:text-zinc-300 hover:bg-zinc-200 dark:hover:bg-zinc-700'
          }`}
        >
          <Clock size={14} />
          Non-Streaming
        </button>
        <span className="text-xs text-zinc-500 dark:text-zinc-500">
          {useStreaming ? 'Real-time output' : 'Better usage tracking'}
        </span>
      </div>
    </div>
  );
}

// Hook to load API keys from localStorage
export function useApiKeys() {
  const [apiKeys, setApiKeys] = useState<Record<string, string>>({});

  useEffect(() => {
    const savedKeys = localStorage.getItem('ai-api-keys');
    if (savedKeys) {
      try {
        setApiKeys(JSON.parse(savedKeys));
      } catch (e) {
        console.error('Failed to parse saved keys', e);
      }
    }
  }, []);

  return apiKeys;
}

// Hook to fetch billing data
export function useBilling() {
  const [billingData, setBillingData] = useState<BillingData | null>(null);

  const fetchBilling = async () => {
    try {
      const response = await fetch('/api/billing');
      const data = await response.json();
      setBillingData(data);
    } catch (e) {
      console.error('Failed to fetch billing data', e);
    }
  };

  useEffect(() => {
    fetchBilling();
  }, []);

  return { billingData, refetch: fetchBilling };
}
</file>

<file path="hooks/useAIChat.ts">
'use client';

import { useChat } from '@ai-sdk/react';
import { DefaultChatTransport } from 'ai';
import { useState, useMemo, useCallback, useRef, useEffect } from 'react';
import { useApiKeys, useBilling, Message } from '@/components/AISettingsPanel';
import { getDefaultModel } from '@/lib/models';

interface UseAIChatOptions {
  /** API endpoint to call */
  endpoint: string;
  /** Extra body parameters to include in requests (can be a function for dynamic values) */
  extraBody?: () => Record<string, unknown>;
}

export function useAIChat({ endpoint, extraBody }: UseAIChatOptions) {
  const [selectedProvider, setSelectedProvider] = useState('openai');
  const [selectedModel, setSelectedModel] = useState<string>(() =>
    getDefaultModel('openai'),
  );

  // Reset model when provider changes
  const handleProviderChange = useCallback((provider: string) => {
    const defaultModel = getDefaultModel(provider);
    setSelectedProvider(provider);
    setSelectedModel(defaultModel);
  }, []);
  const [inputValue, setInputValue] = useState('');
  const [useStreaming, setUseStreaming] = useState(true);
  const [nonStreamingMessages, setNonStreamingMessages] = useState<Message[]>(
    [],
  );
  const [isNonStreamingLoading, setIsNonStreamingLoading] = useState(false);
  const [nonStreamingError, setNonStreamingError] = useState<Error | null>(
    null,
  );
  const [streamingErrorState, setStreamingErrorState] = useState<Error | null>(
    null,
  );

  const apiKeys = useApiKeys();
  const { billingData, refetch: refetchBilling } = useBilling();

  // Use refs to store latest values so the transport callback always reads current values
  const providerRef = useRef(selectedProvider);
  const modelRef = useRef(selectedModel);
  const apiKeysRef = useRef(apiKeys);
  const extraBodyRef = useRef(extraBody);

  // Update refs when values change
  useEffect(() => {
    providerRef.current = selectedProvider;
    modelRef.current = selectedModel;
    apiKeysRef.current = apiKeys;
    extraBodyRef.current = extraBody;
  }, [selectedProvider, selectedModel, apiKeys, extraBody]);

  // Use DefaultChatTransport with prepareSendMessagesRequest to ensure correct format
  // DefaultChatTransport works with toUIMessageStreamResponse() which properly handles errors
  const transport = useMemo(() => {
    console.log(
      `[Transport Created] Provider: ${selectedProvider}, Model: ${selectedModel}`,
    );

    return new DefaultChatTransport({
      api: endpoint,
      // Use prepareSendMessagesRequest to explicitly include messages in the request body
      prepareSendMessagesRequest: ({ messages, body: requestBody }) => {
        // Read current values from refs to ensure we always use the latest
        const currentProvider = providerRef.current;
        const currentModel = modelRef.current;
        const currentApiKeys = apiKeysRef.current;
        const providerApiKey = currentApiKeys[currentProvider];

        // Debug logging for all providers
        console.log(
          `[Frontend Streaming] Provider: ${currentProvider}, Model: ${currentModel}, API Key: ${providerApiKey ? providerApiKey.substring(0, 10) + '...' : 'NOT SET'}`,
        );

        return {
          body: {
            messages,
            provider: currentProvider,
            model: currentModel,
            apiKey: providerApiKey, // Only send if it exists
            stream: true,
            ...(extraBodyRef.current?.() ?? {}),
            ...requestBody,
          },
        };
      },
    });
  }, [endpoint, selectedProvider, selectedModel, apiKeys, extraBody]);

  const {
    messages: streamingMessages,
    status,
    sendMessage,
    error: useChatError,
    clearError: clearUseChatError,
  } = useChat({
    transport,
    onError: (error) => {
      console.error('Streaming error:', error);
      setStreamingErrorState(error);
    },
    onFinish: () => {
      refetchBilling();
    },
  });

  // Combine useChat error with our state error, and check status for error state
  const streamingError =
    useChatError ||
    streamingErrorState ||
    (status === 'error'
      ? new Error('An error occurred during streaming')
      : null);

  const isStreamingLoading = status === 'streaming' || status === 'submitted';
  const isLoading = useStreaming ? isStreamingLoading : isNonStreamingLoading;

  // Non-streaming submit handler
  const handleNonStreamingSubmit = useCallback(async () => {
    if (!inputValue.trim() || isLoading) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      role: 'user',
      content: inputValue,
    };

    setNonStreamingMessages((prev) => [...prev, userMessage]);
    setInputValue('');
    setIsNonStreamingLoading(true);
    setNonStreamingError(null);

    try {
      const requestBody = {
        messages: [...nonStreamingMessages, userMessage].map((m) => ({
          role: m.role,
          content: m.content,
        })),
        provider: selectedProvider,
        model: selectedModel,
        apiKey: apiKeys[selectedProvider],
        stream: false,
        ...(extraBody?.() ?? {}),
      };

      // Debug logging for non-streaming requests
      console.log(
        `[Frontend Non-Streaming] Provider: ${selectedProvider}, Model: ${selectedModel}, API Key: ${apiKeys[selectedProvider] ? apiKeys[selectedProvider].substring(0, 10) + '...' : 'NOT SET'}`,
      );

      const response = await fetch(endpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(requestBody),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || `HTTP error! status: ${response.status}`);
      }

      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: 'assistant',
        content: data.text,
        usage: data.usage,
      };

      setNonStreamingMessages((prev) => [...prev, assistantMessage]);
      refetchBilling();
    } catch (error) {
      console.error('Error:', error);
      setNonStreamingError(
        error instanceof Error ? error : new Error(String(error)),
      );
    } finally {
      setIsNonStreamingLoading(false);
    }
  }, [
    inputValue,
    isLoading,
    nonStreamingMessages,
    selectedProvider,
    selectedModel,
    apiKeys,
    endpoint,
    extraBody,
    refetchBilling,
  ]);

  // Unified submit handler
  const handleSubmit = useCallback(
    async (e: React.FormEvent) => {
      e.preventDefault();
      if (!inputValue.trim() || isLoading) return;

      if (useStreaming) {
        // Clear previous errors before sending new message
        setStreamingErrorState(null);
        // Use sendMessage with text format (AI SDK 5 style)
        await sendMessage({ text: inputValue });
        setInputValue('');
      } else {
        await handleNonStreamingSubmit();
      }
    },
    [
      inputValue,
      isLoading,
      useStreaming,
      sendMessage,
      handleNonStreamingSubmit,
    ],
  );

  // Use appropriate messages based on mode
  const messages = useStreaming ? streamingMessages : nonStreamingMessages;

  // Use appropriate error based on mode
  const error = useStreaming ? streamingError : nonStreamingError;

  // Clear error function
  const clearError = useCallback(() => {
    if (useStreaming) {
      clearUseChatError();
      setStreamingErrorState(null);
    } else {
      setNonStreamingError(null);
    }
  }, [useStreaming, clearUseChatError]);

  return {
    // State
    selectedProvider,
    setSelectedProvider: handleProviderChange,
    selectedModel,
    setSelectedModel,
    inputValue,
    setInputValue,
    useStreaming,
    setUseStreaming,
    isLoading,
    messages,
    billingData,
    error,

    // Handlers
    handleSubmit,
    clearError,
  };
}
</file>

<file path="app/globals.css">
@import 'tailwindcss';

/* Enable class-based dark mode for next-themes */
@custom-variant dark (&:where(.dark, .dark *));

:root {
  --background: #ffffff;
  --foreground: #171717;
}

.dark {
  --background: #0a0a0a;
  --foreground: #ededed;
}

body {
  background: var(--background);
  color: var(--foreground);
  font-family: Arial, Helvetica, sans-serif;
}

/* Markdown content styles */
.markdown-content * {
  line-height: 1;
}

.markdown-content h1 {
  @apply text-2xl font-bold text-zinc-900 dark:text-zinc-100;
  border-bottom: 1px solid rgb(228 228 231 / 0.5);
}

.markdown-content h2 {
  @apply text-xl font-semibold text-zinc-900 dark:text-zinc-100;
  border-bottom: 1px solid rgb(228 228 231 / 0.3);
}

.markdown-content h3 {
  @apply text-lg font-semibold text-zinc-900 dark:text-zinc-100;
}

.markdown-content h4 {
  @apply text-base font-semibold text-zinc-900 dark:text-zinc-100;
}

.markdown-content h5,
.markdown-content h6 {
  @apply text-sm font-semibold text-zinc-900 dark:text-zinc-100;
}

.markdown-content ul,
.markdown-content ol {
  @apply ml-6;
}

.markdown-content ul {
  @apply list-disc;
}

.markdown-content ol {
  @apply list-decimal;
}

.markdown-content blockquote {
  @apply border-l-4 border-zinc-300 dark:border-zinc-700 pl-4 italic text-zinc-600 dark:text-zinc-400;
}

.markdown-content code {
  @apply bg-zinc-100 dark:bg-zinc-800 px-1.5 py-0.5 rounded text-sm font-mono text-zinc-900 dark:text-zinc-100;
}

.markdown-content pre {
  @apply bg-zinc-100 dark:bg-zinc-800 p-3 rounded-lg overflow-x-auto;
  margin: 0;
}

.markdown-content pre code {
  @apply bg-transparent p-0;
}

.markdown-content a {
  @apply text-blue-600 dark:text-blue-400 hover:underline;
}

.markdown-content strong {
  @apply font-semibold text-zinc-900 dark:text-zinc-100;
}

.markdown-content em {
  @apply italic;
}

.markdown-content hr {
  @apply border-zinc-300 dark:border-zinc-700;
}

.markdown-content table {
  @apply w-full border-collapse;
}

.markdown-content th,
.markdown-content td {
  @apply border border-zinc-300 dark:border-zinc-700 px-4 py-2;
}

.markdown-content th {
  @apply bg-zinc-100 dark:bg-zinc-800 font-semibold text-zinc-900 dark:text-zinc-100;
}

.markdown-content img {
  @apply max-w-full h-auto rounded-lg;
}
</file>

<file path="app/chat/page.tsx">
'use client';

import { useState, useCallback } from 'react';
import { MessageCircle, Send, Sparkles, AlertCircle, X } from 'lucide-react';
import { AISettingsPanel, providers } from '@/components/AISettingsPanel';
import { useAIChat } from '@/hooks/useAIChat';
import { MarkdownRenderer } from '@/components/MarkdownRenderer';
import { useVercelGatewayFallbackModels } from '@/components/VercelGatewayFallbackModels';

export default function ChatPage() {
  const [systemPrompt, setSystemPrompt] = useState(
    'You are a helpful AI assistant.',
  );
  const [showSettings, setShowSettings] = useState(false);
  const [enableTools, setEnableTools] = useState(false);
  const [fallbackModels, setFallbackModels] = useVercelGatewayFallbackModels();

  // Use useCallback so the function reference is stable
  const getExtraBody = useCallback(
    () => ({
      systemPrompt,
      enableTools,
      ...(fallbackModels.length > 0 && { fallbackModels }),
    }),
    [systemPrompt, enableTools, fallbackModels],
  );

  const {
    selectedProvider,
    setSelectedProvider,
    selectedModel,
    setSelectedModel,
    inputValue,
    setInputValue,
    useStreaming,
    setUseStreaming,
    isLoading,
    messages,
    handleSubmit,
    error,
    clearError,
  } = useAIChat({
    endpoint: '/api/chat',
    extraBody: getExtraBody,
  });

  const currentProvider = providers.find((p) => p.id === selectedProvider);

  return (
    <div className="w-full max-w-3xl mx-auto py-16 px-4 sm:px-6 lg:px-8">
      <div className="flex flex-col items-center gap-6 text-center sm:items-start sm:text-left">
        <h1 className="flex items-center gap-3 text-3xl font-semibold leading-10 tracking-tight text-black dark:text-zinc-50">
          <MessageCircle size={32} />
          AI Chat
        </h1>
        <p className="max-w-md text-lg leading-8 text-zinc-600 dark:text-zinc-400">
          Chat with AI using a custom system prompt.
        </p>
      </div>

      <form onSubmit={handleSubmit} className="w-full mt-8">
        <div className="flex flex-col gap-4">
          {/* Shared AI Settings Panel */}
          <AISettingsPanel
            selectedProvider={selectedProvider}
            onProviderChange={setSelectedProvider}
            selectedModel={selectedModel}
            onModelChange={setSelectedModel}
            useStreaming={useStreaming}
            onStreamingChange={setUseStreaming}
            fallbackModels={fallbackModels}
            onFallbackModelsChange={setFallbackModels}
          />

          {/* Settings Toggle */}
          <button
            type="button"
            onClick={() => setShowSettings(!showSettings)}
            className="flex items-center gap-2 text-sm text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white w-fit"
          >
            <Sparkles size={14} />
            {showSettings ? 'Hide' : 'Show'} Chat Settings
          </button>

          {/* Collapsible Settings */}
          {showSettings && (
            <div className="flex flex-col gap-4 p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-zinc-50 dark:bg-zinc-900">
              {/* System Prompt */}
              <div className="flex flex-col gap-2">
                <label className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
                  System Prompt
                </label>
                <textarea
                  value={systemPrompt}
                  onChange={(e) => setSystemPrompt(e.target.value)}
                  placeholder="Enter a custom system prompt..."
                  rows={3}
                  className="w-full p-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-white placeholder-zinc-400 resize-none"
                />
              </div>

              {/* Enable Tools Toggle */}
              <label className="flex items-center gap-2 cursor-pointer">
                <input
                  type="checkbox"
                  checked={enableTools}
                  onChange={(e) => setEnableTools(e.target.checked)}
                  className="w-4 h-4 rounded border-zinc-300 dark:border-zinc-600"
                />
                <span className="text-sm text-zinc-700 dark:text-zinc-300">
                  Enable file reading tools
                </span>
              </label>
            </div>
          )}

          {/* Message Input */}
          <div className="flex gap-2 max-w-md">
            <input
              className="flex-1 p-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-white placeholder-zinc-400"
              value={inputValue}
              placeholder="Type your message..."
              onChange={(e) => setInputValue(e.target.value)}
            />
            <button
              type="submit"
              disabled={isLoading || !inputValue.trim()}
              className="flex h-10 items-center justify-center gap-2 rounded-md bg-blue-600 px-4 text-white transition-colors hover:bg-blue-700 disabled:opacity-50 disabled:cursor-not-allowed"
            >
              <Send size={16} />
              {isLoading ? '...' : 'Send'}
            </button>
          </div>
        </div>
      </form>

      {/* Error Banner */}
      {error && (
        <div className="mt-4 p-4 rounded-lg border border-red-300 dark:border-red-800 bg-red-50 dark:bg-red-950 flex items-start gap-3">
          <AlertCircle
            size={20}
            className="text-red-600 dark:text-red-400 flex-shrink-0 mt-0.5"
          />
          <div className="flex-1">
            <p className="font-medium text-red-800 dark:text-red-200">Error</p>
            <p className="text-sm text-red-700 dark:text-red-300 mt-1">
              {error.message}
            </p>
          </div>
          <button
            onClick={clearError}
            className="text-red-600 dark:text-red-400 hover:text-red-800 dark:hover:text-red-200"
          >
            <X size={18} />
          </button>
        </div>
      )}

      <div className="flex flex-col-reverse w-full mt-8 gap-4">
        {messages.map((m) => (
          <div
            key={m.id}
            className={`whitespace-pre-wrap p-4 rounded-lg border ${
              m.role === 'user'
                ? 'border-blue-200 dark:border-blue-800 bg-blue-50 dark:bg-blue-950'
                : 'border-zinc-200 dark:border-zinc-800 bg-white dark:bg-zinc-900'
            }`}
          >
            <div className="flex items-center justify-between mb-2">
              <strong className="text-zinc-900 dark:text-zinc-100">
                {m.role === 'user' ? 'You' : currentProvider?.name || 'AI'}
              </strong>
              {/* Show usage for non-streaming messages */}
              {'usage' in m && m.usage && (
                <span className="text-xs text-zinc-500 dark:text-zinc-500">
                  {m.usage.promptTokens} prompt + {m.usage.completionTokens}{' '}
                  completion = {m.usage.totalTokens} tokens
                </span>
              )}
            </div>
            <div className="text-zinc-700 dark:text-zinc-300">
              {/* Handle both streaming (parts) and non-streaming (content) formats */}
              {'parts' in m && m.parts ? (
                m.parts.map((part, i) =>
                  part.type === 'text' ? (
                    <MarkdownRenderer key={i} content={part.text} />
                  ) : null,
                )
              ) : 'content' in m && m.content ? (
                <MarkdownRenderer content={m.content} />
              ) : null}
            </div>
          </div>
        ))}
      </div>
    </div>
  );
}
</file>

<file path="package.json">
{
  "name": "my-ai-app",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "eslint",
    "format": "prettier --write .",
    "test": "jest",
    "test:watch": "jest --watch",
    "test:coverage": "jest --coverage"
  },
  "dependencies": {
    "@ai-sdk/anthropic": "^2.0.56",
    "@ai-sdk/deepseek": "^1.0.32",
    "@ai-sdk/google": "^2.0.47",
    "@ai-sdk/openai": "^2.0.87",
    "@ai-sdk/react": "^2.0.115",
    "@types/marked": "^6.0.0",
    "ai": "^5.0.113",
    "lucide-react": "^0.561.0",
    "marked": "^17.0.1",
    "next": "16.0.10",
    "next-themes": "^0.4.6",
    "qwen-ai-provider": "^0.1.1",
    "react": "19.2.1",
    "react-dom": "19.2.1",
    "zod": "^4.2.1"
  },
  "devDependencies": {
    "@tailwindcss/postcss": "^4",
    "@testing-library/dom": "^10.4.1",
    "@testing-library/jest-dom": "^6.9.1",
    "@testing-library/react": "^16.3.1",
    "@types/jest": "^30.0.0",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "16.0.10",
    "jest": "^30.2.0",
    "jest-environment-jsdom": "^30.2.0",
    "prettier": "^3.7.4",
    "tailwindcss": "^4",
    "ts-node": "^10.9.2",
    "typescript": "^5"
  }
}
</file>

<file path="app/api/chat/route.ts">
import { UIMessage } from 'ai';
import { stepCountIs } from 'ai';
import { NextResponse } from 'next/server';
import { handleAIRequest } from '@/lib/ai-handler';
import { codeTools } from '@/app/api/codereview/route';
import { ProviderId } from '@/lib/providers';

export async function POST(req: Request) {
  try {
    const {
      messages: rawMessages,
      provider: providerId = 'openai',
      apiKey,
      model: requestedModel,
      stream = true,
      systemPrompt,
      enableTools = false, // Whether to enable file reading tools
    }: {
      messages: UIMessage[];
      provider?: ProviderId;
      apiKey?: string;
      model?: string;
      stream?: boolean;
      systemPrompt?: string;
      enableTools?: boolean;
    } = await req.json();

    return handleAIRequest({
      messages: rawMessages,
      provider: providerId,
      apiKey,
      model: requestedModel,
      stream,
      systemPrompt: systemPrompt || 'You are a helpful AI assistant.',
      tools: enableTools ? codeTools : undefined,
      // Allow up to 10 steps for chat (less than codereview since it's simpler)
      stopWhen: enableTools ? stepCountIs(10) : undefined,
      enableUsageMetadata: false,
      logPrefix: 'Chat',
    });
  } catch (error) {
    console.error('Chat error:', error);
    const message =
      error instanceof Error ? error.message : 'An unexpected error occurred';
    return NextResponse.json({ error: message }, { status: 500 });
  }
}
</file>

<file path="app/layout.tsx">
import type { Metadata } from 'next';
import { Geist, Geist_Mono } from 'next/font/google';
import { ThemeProvider } from 'next-themes';
import ThemeSwitcher from '../components/ThemeSwitcher';
import Link from 'next/link';
import { Code, MessageCircle, Settings } from 'lucide-react';
import './globals.css';

const geistSans = Geist({
  variable: '--font-geist-sans',
  subsets: ['latin'],
});

const geistMono = Geist_Mono({
  variable: '--font-geist-mono',
  subsets: ['latin'],
});

export const metadata: Metadata = {
  title: 'Create Next App',
  description: 'Generated by create next app',
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" suppressHydrationWarning>
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased flex flex-col min-h-screen bg-zinc-50 dark:bg-black`}
      >
        <ThemeProvider attribute="class">
          <header className="w-full border-b border-zinc-200 dark:border-zinc-800">
            <div className="flex items-center justify-between h-16 px-4 mx-auto max-w-7xl sm:px-6 lg:px-8">
              <h1 className="text-2xl font-bold text-black dark:text-white">
                <Link href="/">AI Agent</Link>
              </h1>
              <div className="flex items-center gap-4">
                <Link
                  href="/"
                  className="flex items-center gap-2 text-sm font-medium text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white"
                >
                  <Code size={16} />
                  Code Review
                </Link>
                <Link
                  href="/chat"
                  className="flex items-center gap-2 text-sm font-medium text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white"
                >
                  <MessageCircle size={16} />
                  Chat
                </Link>
                <ThemeSwitcher />
                <Link
                  href="/settings"
                  className="flex items-center gap-2 text-sm font-medium text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white"
                >
                  <Settings size={16} />
                  Settings
                </Link>
              </div>
            </div>
          </header>
          <main className="flex flex-col flex-1">{children}</main>
        </ThemeProvider>
      </body>
    </html>
  );
}
</file>

<file path="app/page.tsx">
'use client';

import { useState, useCallback } from 'react';
import { Code, GitPullRequest, AlertCircle, X, Sparkles } from 'lucide-react';
import { AISettingsPanel, providers } from '@/components/AISettingsPanel';
import { useAIChat } from '@/hooks/useAIChat';
import { MarkdownRenderer } from '@/components/MarkdownRenderer';
import { useVercelGatewayFallbackModels } from '@/components/VercelGatewayFallbackModels';

type InputMode = 'file' | 'pr';

export default function Home() {
  const [inputMode, setInputMode] = useState<InputMode>('file');
  const [systemPrompt, setSystemPrompt] = useState(
    'You are a code reviewer. You will be given a file path and you will review the code in that file.',
  );
  const [showSettings, setShowSettings] = useState(false);
  const [fallbackModels, setFallbackModels] = useVercelGatewayFallbackModels();

  // Use useCallback so the function reference is stable
  const getExtraBody = useCallback(
    () => ({
      systemPrompt,
      ...(fallbackModels.length > 0 && { fallbackModels }),
    }),
    [systemPrompt, fallbackModels],
  );

  const {
    selectedProvider,
    setSelectedProvider,
    selectedModel,
    setSelectedModel,
    inputValue,
    setInputValue,
    useStreaming,
    setUseStreaming,
    isLoading,
    messages,
    handleSubmit,
    error,
    clearError,
  } = useAIChat({
    endpoint: '/api/codereview',
    extraBody: getExtraBody,
  });

  const currentProvider = providers.find((p) => p.id === selectedProvider);

  return (
    <div className="w-full max-w-3xl mx-auto py-16 px-4 sm:px-6 lg:px-8">
      <div className="flex flex-col items-center gap-6 text-center sm:items-start sm:text-left">
        <h1 className="max-w-xs text-3xl font-semibold leading-10 tracking-tight text-black dark:text-zinc-50">
          AI Code Reviewer
        </h1>
        <p className="max-w-md text-lg leading-8 text-zinc-600 dark:text-zinc-400">
          Enter the path to a file to have it reviewed by an AI agent.
        </p>
      </div>

      <form onSubmit={handleSubmit} className="w-full mt-8">
        <div className="flex flex-col gap-4">
          {/* Shared AI Settings Panel */}
          <AISettingsPanel
            selectedProvider={selectedProvider}
            onProviderChange={setSelectedProvider}
            selectedModel={selectedModel}
            onModelChange={setSelectedModel}
            useStreaming={useStreaming}
            onStreamingChange={setUseStreaming}
            fallbackModels={fallbackModels}
            onFallbackModelsChange={setFallbackModels}
          />

          {/* Settings Toggle */}
          <button
            type="button"
            onClick={() => setShowSettings(!showSettings)}
            className="flex items-center gap-2 text-sm text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white w-fit"
          >
            <Sparkles size={14} />
            {showSettings ? 'Hide' : 'Show'} Review Settings
          </button>

          {/* Collapsible Settings */}
          {showSettings && (
            <div className="flex flex-col gap-4 p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-zinc-50 dark:bg-zinc-900">
              {/* System Prompt */}
              <div className="flex flex-col gap-2">
                <label className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
                  System Prompt
                </label>
                <textarea
                  value={systemPrompt}
                  onChange={(e) => setSystemPrompt(e.target.value)}
                  placeholder="Enter a custom system prompt for code review..."
                  rows={3}
                  className="w-full p-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-white placeholder-zinc-400 resize-none"
                />
              </div>
            </div>
          )}

          {/* Input Mode Toggle */}
          <div className="flex gap-2">
            <button
              type="button"
              onClick={() => setInputMode('file')}
              className={`flex items-center gap-2 px-3 py-1.5 rounded-md text-sm font-medium transition-colors ${
                inputMode === 'file'
                  ? 'bg-blue-600 text-white'
                  : 'bg-zinc-100 dark:bg-zinc-800 text-zinc-600 dark:text-zinc-400 hover:bg-zinc-200 dark:hover:bg-zinc-700'
              }`}
            >
              <Code size={14} />
              File Path
            </button>
            <button
              type="button"
              onClick={() => setInputMode('pr')}
              className={`flex items-center gap-2 px-3 py-1.5 rounded-md text-sm font-medium transition-colors ${
                inputMode === 'pr'
                  ? 'bg-blue-600 text-white'
                  : 'bg-zinc-100 dark:bg-zinc-800 text-zinc-600 dark:text-zinc-400 hover:bg-zinc-200 dark:hover:bg-zinc-700'
              }`}
            >
              <GitPullRequest size={14} />
              PR Link
            </button>
          </div>

          <input
            className="w-full max-w-md p-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-white placeholder-zinc-400"
            value={inputValue}
            placeholder={
              inputMode === 'file'
                ? 'Enter file path...'
                : 'Enter GitHub PR URL (e.g., https://github.com/owner/repo/pull/123)'
            }
            onChange={(e) => setInputValue(e.target.value)}
          />
          <button
            type="submit"
            disabled={isLoading || !inputValue.trim()}
            className="flex h-12 w-fit items-center justify-center gap-2 rounded-md bg-blue-600 px-6 text-white transition-colors hover:bg-blue-700 disabled:opacity-50 disabled:cursor-not-allowed"
          >
            {inputMode === 'file' ? (
              <Code size={16} />
            ) : (
              <GitPullRequest size={16} />
            )}
            {isLoading ? 'Reviewing...' : 'Review Code'}
          </button>
        </div>
      </form>

      {/* Error Banner */}
      {error && (
        <div className="mt-4 p-4 rounded-lg border border-red-300 dark:border-red-800 bg-red-50 dark:bg-red-950 flex items-start gap-3">
          <AlertCircle
            size={20}
            className="text-red-600 dark:text-red-400 flex-shrink-0 mt-0.5"
          />
          <div className="flex-1">
            <p className="font-medium text-red-800 dark:text-red-200">Error</p>
            <p className="text-sm text-red-700 dark:text-red-300 mt-1">
              {error.message}
            </p>
          </div>
          <button
            onClick={clearError}
            className="text-red-600 dark:text-red-400 hover:text-red-800 dark:hover:text-red-200"
          >
            <X size={18} />
          </button>
        </div>
      )}

      <div className="flex flex-col-reverse w-full mt-8 gap-4">
        {messages.map((m) => (
          <div
            key={m.id}
            className="whitespace-pre-wrap p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-white dark:bg-zinc-900"
          >
            <div className="flex items-center justify-between mb-2">
              <strong className="text-zinc-900 dark:text-zinc-100">
                {m.role === 'user' ? 'You' : currentProvider?.name || 'AI'}
              </strong>
              {/* Show usage for non-streaming messages */}
              {'usage' in m && m.usage && (
                <span className="text-xs text-zinc-500 dark:text-zinc-500">
                  {m.usage.promptTokens} prompt + {m.usage.completionTokens}{' '}
                  completion = {m.usage.totalTokens} tokens
                </span>
              )}
            </div>
            <div className="text-zinc-700 dark:text-zinc-300">
              {/* Handle both streaming (parts) and non-streaming (content) formats */}
              {'parts' in m && m.parts ? (
                m.parts.map((part, i) =>
                  part.type === 'text' ? (
                    <MarkdownRenderer key={i} content={part.text} />
                  ) : null,
                )
              ) : 'content' in m && m.content ? (
                <MarkdownRenderer content={m.content} />
              ) : null}
            </div>
          </div>
        ))}
      </div>
    </div>
  );
}
</file>

<file path="app/api/codereview/route.ts">
import { UIMessage, stepCountIs } from 'ai';
import { tool } from 'ai';
import { z } from 'zod';
import { readFile } from 'fs/promises';
import { NextResponse } from 'next/server';
import { handleAIRequest } from '@/lib/ai-handler';
import { ProviderId } from '@/lib/providers';

// Shared tools
export const codeTools = {
  readFile: tool({
    description: 'Read the content of a file.',
    inputSchema: z.object({
      path: z.string().describe('The path to the file to read.'),
    }),
    execute: async ({ path }) => {
      try {
        const content = await readFile(path, 'utf-8');
        return content;
      } catch (error) {
        return `Error reading file: ${error instanceof Error ? error.message : 'Unknown error'}`;
      }
    },
  }),
  readPullRequest: tool({
    description:
      'Read files from a public GitHub pull request. Provide the PR URL to fetch all changed files and their contents.',
    inputSchema: z.object({
      prUrl: z
        .string()
        .describe(
          'The GitHub PR URL (e.g., https://github.com/owner/repo/pull/123).',
        ),
    }),
    execute: async ({ prUrl }) => {
      try {
        // Parse PR URL: https://github.com/owner/repo/pull/123
        const prUrlMatch = prUrl.match(
          /github\.com\/([^\/]+)\/([^\/]+)\/pull\/(\d+)/i,
        );
        if (!prUrlMatch) {
          return 'Error: Invalid GitHub PR URL format. Expected: https://github.com/owner/repo/pull/123';
        }

        const [, owner, repo, prNumber] = prUrlMatch;

        // Fetch PR files using GitHub REST API (no auth needed for public repos)
        const filesResponse = await fetch(
          `https://api.github.com/repos/${owner}/${repo}/pulls/${prNumber}/files`,
          {
            headers: {
              Accept: 'application/vnd.github.v3+json',
            },
          },
        );

        if (!filesResponse.ok) {
          if (filesResponse.status === 404) {
            return `Error: PR not found. Make sure the PR is public and the URL is correct.`;
          }
          return `Error fetching PR files: ${filesResponse.status} ${filesResponse.statusText}`;
        }

        const files = await filesResponse.json();

        if (!Array.isArray(files) || files.length === 0) {
          return 'No files found in this pull request.';
        }

        // Format the response with file information
        const fileContents = files.map((file: any) => {
          const content = file.patch || file.contents || '';
          return {
            filename: file.filename,
            status: file.status, // added, modified, removed, renamed
            additions: file.additions,
            deletions: file.deletions,
            changes: file.changes,
            patch: content.substring(0, 50000), // Limit patch size
            raw_url: file.contents_url || file.blob_url,
          };
        });

        return JSON.stringify(
          {
            pr_url: prUrl,
            owner,
            repo,
            pr_number: prNumber,
            total_files: files.length,
            files: fileContents,
          },
          null,
          2,
        );
      } catch (error) {
        return `Error reading pull request: ${error instanceof Error ? error.message : 'Unknown error'}`;
      }
    },
  }),
};

export async function POST(req: Request) {
  try {
    const {
      messages: rawMessages,
      provider: providerId = 'openai',
      apiKey,
      model: requestedModel,
      stream = true, // Default to streaming
      systemPrompt, // Custom system prompt
    }: {
      messages: UIMessage[] | Array<{ role: string; content: string }>;
      provider?: ProviderId;
      apiKey?: string;
      model?: string;
      stream?: boolean;
      systemPrompt?: string;
    } = await req.json();

    // Use handleAIRequest from the shared library, with codereview-specific options
    return handleAIRequest({
      messages: rawMessages,
      provider: providerId,
      apiKey,
      model: requestedModel,
      stream,
      systemPrompt:
        systemPrompt ||
        `You are a code reviewer.
You will be given a file path and you will review the code in that file.`,
      tools: codeTools, // Always enable tools for code review
      // Allow up to 20 steps to handle complex reviews with multiple tool calls
      // This covers: reading PR, reading multiple files, and generating the review
      stopWhen: stepCountIs(20),
      enableUsageMetadata: true, // Include usage in streaming response for UI display
      enableStepLogging: true, // Log tool calls for debugging
      logPrefix: 'CodeReview',
    });
  } catch (error) {
    console.error('Code review error:', error);
    const message =
      error instanceof Error ? error.message : 'An unexpected error occurred';
    return NextResponse.json({ error: message }, { status: 500 });
  }
}
</file>

</files>
