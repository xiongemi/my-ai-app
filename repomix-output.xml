This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
__tests__/
  MarkdownRenderer.test.tsx
  models.test.ts
  providers.test.ts
  ThemeSwitcher.test.tsx
.github/
  workflows/
    ai-code-review.yml
    README.md
app/
  api/
    billing/
      route.ts
    chat/
      route.ts
    codereview/
      route.ts
  chat/
    page.tsx
  settings/
    page.tsx
  favicon.ico
  globals.css
  layout.tsx
  page.tsx
components/
  AISettingsPanel.tsx
  Billing.tsx
  MarkdownRenderer.tsx
  ThemeSwitcher.tsx
  VercelGatewayFallbackModels.tsx
hooks/
  useAIChat.ts
lib/
  ai-handler.ts
  billing.ts
  models.json
  models.ts
  providers.ts
public/
  file.svg
  globe.svg
  next.svg
  vercel.svg
  window.svg
.gitignore
.prettierignore
.prettierrc.json
eslint.config.mjs
jest.config.ts
jest.setup.ts
next.config.ts
package.json
postcss.config.mjs
README.md
tailwind.config.ts
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="components/Billing.tsx">
'use client';

import React, { useState, useEffect } from 'react';

export interface BillingData {
  totalCost: number;
  totalTokens?: number;
  usageHistory: Array<{
    timestamp: string;
    model: string;
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
    cost: number;
  }>;
}

// Create a context for billing data so it can be shared across components
const BillingContext = React.createContext<{
  billingData: BillingData | null;
  refetch: () => Promise<void>;
} | null>(null);

// Hook to fetch billing data - uses context if available, otherwise creates its own state
export function useBilling() {
  const context = React.useContext(BillingContext);

  // If context exists, use it
  if (context) {
    return context;
  }

  // Otherwise, create local state (for backward compatibility)
  const [billingData, setBillingData] = useState<BillingData | null>(null);

  const fetchBilling = async () => {
    try {
      const response = await fetch('/api/billing');
      const data = await response.json();
      setBillingData(data);
    } catch (e) {
      console.error('Failed to fetch billing data', e);
    }
  };

  useEffect(() => {
    fetchBilling();
  }, []);

  return { billingData, refetch: fetchBilling };
}

// Provider component to share billing state
export function BillingProvider({ children }: { children: React.ReactNode }) {
  const [billingData, setBillingData] = useState<BillingData | null>(null);

  const fetchBilling = async () => {
    try {
      const response = await fetch('/api/billing');
      const data = await response.json();
      setBillingData(data);
    } catch (e) {
      console.error('Failed to fetch billing data', e);
    }
  };

  useEffect(() => {
    fetchBilling();
    // Billing data is refetched automatically after AI requests complete
    // via refetch() calls in useAIChat and other components
  }, []);

  return (
    <BillingContext.Provider value={{ billingData, refetch: fetchBilling }}>
      {children}
    </BillingContext.Provider>
  );
}
</file>

<file path="public/file.svg">
<svg fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="M14.5 13.5V5.41a1 1 0 0 0-.3-.7L9.8.29A1 1 0 0 0 9.08 0H1.5v13.5A2.5 2.5 0 0 0 4 16h8a2.5 2.5 0 0 0 2.5-2.5m-1.5 0v-7H8v-5H3v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1M9.5 5V2.12L12.38 5zM5.13 5h-.62v1.25h2.12V5zm-.62 3h7.12v1.25H4.5zm.62 3h-.62v1.25h7.12V11z" clip-rule="evenodd" fill="#666" fill-rule="evenodd"/></svg>
</file>

<file path="public/globe.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><g clip-path="url(#a)"><path fill-rule="evenodd" clip-rule="evenodd" d="M10.27 14.1a6.5 6.5 0 0 0 3.67-3.45q-1.24.21-2.7.34-.31 1.83-.97 3.1M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16m.48-1.52a7 7 0 0 1-.96 0H7.5a4 4 0 0 1-.84-1.32q-.38-.89-.63-2.08a40 40 0 0 0 3.92 0q-.25 1.2-.63 2.08a4 4 0 0 1-.84 1.31zm2.94-4.76q1.66-.15 2.95-.43a7 7 0 0 0 0-2.58q-1.3-.27-2.95-.43a18 18 0 0 1 0 3.44m-1.27-3.54a17 17 0 0 1 0 3.64 39 39 0 0 1-4.3 0 17 17 0 0 1 0-3.64 39 39 0 0 1 4.3 0m1.1-1.17q1.45.13 2.69.34a6.5 6.5 0 0 0-3.67-3.44q.65 1.26.98 3.1M8.48 1.5l.01.02q.41.37.84 1.31.38.89.63 2.08a40 40 0 0 0-3.92 0q.25-1.2.63-2.08a4 4 0 0 1 .85-1.32 7 7 0 0 1 .96 0m-2.75.4a6.5 6.5 0 0 0-3.67 3.44 29 29 0 0 1 2.7-.34q.31-1.83.97-3.1M4.58 6.28q-1.66.16-2.95.43a7 7 0 0 0 0 2.58q1.3.27 2.95.43a18 18 0 0 1 0-3.44m.17 4.71q-1.45-.12-2.69-.34a6.5 6.5 0 0 0 3.67 3.44q-.65-1.27-.98-3.1" fill="#666"/></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h16v16H0z"/></clipPath></defs></svg>
</file>

<file path="public/next.svg">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 394 80"><path fill="#000" d="M262 0h68.5v12.7h-27.2v66.6h-13.6V12.7H262V0ZM149 0v12.7H94v20.4h44.3v12.6H94v21h55v12.6H80.5V0h68.7zm34.3 0h-17.8l63.8 79.4h17.9l-32-39.7 32-39.6h-17.9l-23 28.6-23-28.6zm18.3 56.7-9-11-27.1 33.7h17.8l18.3-22.7z"/><path fill="#000" d="M81 79.3 17 0H0v79.3h13.6V17l50.2 62.3H81Zm252.6-.4c-1 0-1.8-.4-2.5-1s-1.1-1.6-1.1-2.6.3-1.8 1-2.5 1.6-1 2.6-1 1.8.3 2.5 1a3.4 3.4 0 0 1 .6 4.3 3.7 3.7 0 0 1-3 1.8zm23.2-33.5h6v23.3c0 2.1-.4 4-1.3 5.5a9.1 9.1 0 0 1-3.8 3.5c-1.6.8-3.5 1.3-5.7 1.3-2 0-3.7-.4-5.3-1s-2.8-1.8-3.7-3.2c-.9-1.3-1.4-3-1.4-5h6c.1.8.3 1.6.7 2.2s1 1.2 1.6 1.5c.7.4 1.5.5 2.4.5 1 0 1.8-.2 2.4-.6a4 4 0 0 0 1.6-1.8c.3-.8.5-1.8.5-3V45.5zm30.9 9.1a4.4 4.4 0 0 0-2-3.3 7.5 7.5 0 0 0-4.3-1.1c-1.3 0-2.4.2-3.3.5-.9.4-1.6 1-2 1.6a3.5 3.5 0 0 0-.3 4c.3.5.7.9 1.3 1.2l1.8 1 2 .5 3.2.8c1.3.3 2.5.7 3.7 1.2a13 13 0 0 1 3.2 1.8 8.1 8.1 0 0 1 3 6.5c0 2-.5 3.7-1.5 5.1a10 10 0 0 1-4.4 3.5c-1.8.8-4.1 1.2-6.8 1.2-2.6 0-4.9-.4-6.8-1.2-2-.8-3.4-2-4.5-3.5a10 10 0 0 1-1.7-5.6h6a5 5 0 0 0 3.5 4.6c1 .4 2.2.6 3.4.6 1.3 0 2.5-.2 3.5-.6 1-.4 1.8-1 2.4-1.7a4 4 0 0 0 .8-2.4c0-.9-.2-1.6-.7-2.2a11 11 0 0 0-2.1-1.4l-3.2-1-3.8-1c-2.8-.7-5-1.7-6.6-3.2a7.2 7.2 0 0 1-2.4-5.7 8 8 0 0 1 1.7-5 10 10 0 0 1 4.3-3.5c2-.8 4-1.2 6.4-1.2 2.3 0 4.4.4 6.2 1.2 1.8.8 3.2 2 4.3 3.4 1 1.4 1.5 3 1.5 5h-5.8z"/></svg>
</file>

<file path="public/vercel.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1155 1000"><path d="m577.3 0 577.4 1000H0z" fill="#fff"/></svg>
</file>

<file path="public/window.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.5 2.5h13v10a1 1 0 0 1-1 1h-11a1 1 0 0 1-1-1zM0 1h16v11.5a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 0 12.5zm3.75 4.5a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5M7 4.75a.75.75 0 1 1-1.5 0 .75.75 0 0 1 1.5 0m1.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" fill="#666"/></svg>
</file>

<file path=".gitignore">
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts
</file>

<file path=".prettierignore">
node_modules/
pnpm-lock.yaml
.next/
.env
.env.local
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts",
    ".next/dev/types/**/*.ts",
    "**/*.mts"
  ],
  "exclude": ["node_modules"]
}
</file>

<file path="__tests__/MarkdownRenderer.test.tsx">
import '@testing-library/jest-dom';
import { render, screen } from '@testing-library/react';

// Mock marked to avoid ESM issues in Jest
jest.mock('marked', () => ({
  marked: {
    parse: (text: string) => {
      // Simple mock markdown parser for testing
      return text
        .replace(/^# (.+)$/gm, '<h1>$1</h1>')
        .replace(/^## (.+)$/gm, '<h2>$1</h2>')
        .replace(/^### (.+)$/gm, '<h3>$1</h3>')
        .replace(/^- (.+)$/gm, '<ul><li>$1</li></ul>')
        .replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>')
        .replace(/\*(.+?)\*/g, '<em>$1</em>')
        .replace(/\[(.+?)\]\((.+?)\)/g, '<a href="$2">$1</a>')
        .replace(/```javascript\n([\s\S]+?)```/g, '<pre><code>$1</code></pre>');
    },
    setOptions: jest.fn(),
  },
}));

import { MarkdownRenderer } from '@/components/MarkdownRenderer';

describe('MarkdownRenderer', () => {
  it('should render markdown headings', () => {
    const markdown = '# Heading 1\n## Heading 2\n### Heading 3';
    render(<MarkdownRenderer content={markdown} />);

    const h1 = screen.getByText('Heading 1');
    expect(h1.tagName).toBe('H1');

    const h2 = screen.getByText('Heading 2');
    expect(h2.tagName).toBe('H2');

    const h3 = screen.getByText('Heading 3');
    expect(h3.tagName).toBe('H3');
  });

  it('should render markdown lists', () => {
    const markdown = '- Item 1\n- Item 2\n- Item 3';
    render(<MarkdownRenderer content={markdown} />);

    const list = screen.getByText('Item 1').closest('ul');
    expect(list).toBeInTheDocument();
    expect(screen.getByText('Item 2')).toBeInTheDocument();
    expect(screen.getByText('Item 3')).toBeInTheDocument();
  });

  it('should render markdown code blocks', () => {
    const markdown = '```javascript\nconst x = 1;\n```';
    render(<MarkdownRenderer content={markdown} />);

    const codeBlock = screen.getByText('const x = 1;');
    expect(codeBlock.closest('pre')).toBeInTheDocument();
  });

  it('should render markdown links', () => {
    const markdown = '[Link Text](https://example.com)';
    render(<MarkdownRenderer content={markdown} />);

    const link = screen.getByText('Link Text');
    expect(link.tagName).toBe('A');
    expect(link).toHaveAttribute('href', 'https://example.com');
  });

  it('should render markdown bold and italic text', () => {
    const markdown = '**bold** and *italic*';
    render(<MarkdownRenderer content={markdown} />);

    const bold = screen.getByText('bold');
    expect(bold.tagName).toBe('STRONG');

    const italic = screen.getByText('italic');
    expect(italic.tagName).toBe('EM');
  });

  it('should handle empty content', () => {
    const { container } = render(<MarkdownRenderer content="" />);
    const markdownContent = container.querySelector('.markdown-content');
    expect(markdownContent).toBeInTheDocument();
  });

  it('should apply custom className', () => {
    render(<MarkdownRenderer content="Test" className="custom-class" />);
    const container = screen.getByText('Test').closest('.markdown-content');
    expect(container).toHaveClass('custom-class');
  });
});
</file>

<file path="__tests__/models.test.ts">
import {
  getModelsForProvider,
  getDefaultModel,
  getModelInfo,
} from '@/lib/models';

describe('Models', () => {
  describe('getModelsForProvider', () => {
    it('should return models for a valid provider', () => {
      const models = getModelsForProvider('openai');
      expect(models).toBeDefined();
      expect(Array.isArray(models)).toBe(true);
    });

    it('should return empty array for invalid provider', () => {
      const models = getModelsForProvider('invalid-provider');
      expect(models).toEqual([]);
    });
  });

  describe('getDefaultModel', () => {
    it('should return default model for a valid provider', () => {
      const model = getDefaultModel('openai');
      expect(model).toBeDefined();
      expect(typeof model).toBe('string');
    });

    it('should return empty string for invalid provider', () => {
      const model = getDefaultModel('invalid-provider');
      expect(model).toBe('');
    });
  });

  describe('getModelInfo', () => {
    it('should return model info for valid provider and model', () => {
      const defaultModel = getDefaultModel('openai');
      if (defaultModel) {
        const modelInfo = getModelInfo('openai', defaultModel);
        expect(modelInfo).toBeDefined();
        if (modelInfo) {
          expect(modelInfo).toHaveProperty('id');
          expect(modelInfo).toHaveProperty('name');
          expect(modelInfo).toHaveProperty('description');
        }
      }
    });

    it('should return undefined for invalid model', () => {
      const modelInfo = getModelInfo('openai', 'invalid-model-id');
      expect(modelInfo).toBeUndefined();
    });
  });
});
</file>

<file path="__tests__/ThemeSwitcher.test.tsx">
import '@testing-library/jest-dom';
import { render, screen } from '@testing-library/react';
import ThemeSwitcher from '@/components/ThemeSwitcher';

// Mock next-themes since it uses client-side hooks
jest.mock('next-themes', () => ({
  useTheme: () => ({
    theme: 'light',
    setTheme: jest.fn(),
    themes: ['light', 'dark', 'system'],
  }),
  ThemeProvider: ({ children }: { children: React.ReactNode }) => children,
}));

describe('ThemeSwitcher', () => {
  it('should render the theme switcher', () => {
    render(<ThemeSwitcher />);
    // The component should render without errors
    expect(document.body).toBeInTheDocument();
  });
});
</file>

<file path=".github/workflows/README.md">
# GitHub Actions Workflows

## AI Code Review Workflow

The `ai-code-review.yml` workflow automatically reviews pull requests using your AI code review API.

### Setup

1. **Deploy your AI Code Review app** to a publicly accessible URL (e.g., Vercel, Railway, etc.)

2. **Configure GitHub Secrets** in your repository settings (Settings ‚Üí Secrets and variables ‚Üí Actions):
   - `AI_REVIEW_API_ENDPOINT`: The full URL to your deployed API endpoint
     - Example: `https://ai-agent-86bjedhz7-xiongemis-projects.vercel.app/api/codereview`
   - `AI_API_KEY`: Your AI provider API key (OpenAI, Anthropic, etc.)
   - `AI_PROVIDER`: (Optional) AI provider to use (default: `openai`). Options: `openai`, `gemini`, `anthropic`, `deepseek`, `qwen`, `vercel-ai-gateway`
   - `AI_MODEL`: (Optional) Specific model to use. If not provided, uses the default model for the selected provider

   **Example Configuration:**

   ```
   AI_REVIEW_API_ENDPOINT: https://ai-agent-86bjedhz7-xiongemis-projects.vercel.app/api/codereview
   AI_API_KEY: sk-... (your OpenAI API key)
   AI_PROVIDER: openai
   ```

### How it Works

1. Triggers automatically on:
   - Pull request opened
   - Pull request updated (new commits pushed)

2. The workflow:
   - Checks out the PR code
   - Calls your AI code review API with the PR URL
   - Posts the review as a comment on the PR

### Example Configuration

```yaml
# In your workflow file, you can customize:
env:
  AI_PROVIDER: 'openai' # or 'anthropic', 'gemini', etc.
  AI_MODEL: 'gpt-4' # optional, uses default if not specified
```

### Customization

You can customize the workflow by:

- Changing the trigger events
- Modifying the system prompt
- Adding additional review criteria
- Customizing the comment format

### Troubleshooting

- **API endpoint not found**: Make sure `AI_REVIEW_API_ENDPOINT` secret is set correctly
- **No review posted**: Check the workflow logs for API errors
- **Authentication errors**: Verify your `AI_API_KEY` secret is correct
</file>

<file path="components/MarkdownRenderer.tsx">
'use client';

import { marked } from 'marked';
import { useEffect, useMemo } from 'react';

interface MarkdownRendererProps {
  content: string;
  className?: string;
}

export function MarkdownRenderer({
  content,
  className = '',
}: MarkdownRendererProps) {
  // Configure marked options
  useEffect(() => {
    marked.setOptions({
      breaks: true, // Convert line breaks to <br>
      gfm: true, // GitHub Flavored Markdown
    });
  }, []);

  // Convert markdown to HTML
  const htmlContent = useMemo(() => {
    try {
      return marked.parse(content) as string;
    } catch (error) {
      console.error('Error parsing markdown:', error);
      return content; // Fallback to raw content
    }
  }, [content]);

  return (
    <div
      className={`markdown-content ${className}`}
      dangerouslySetInnerHTML={{ __html: htmlContent }}
    />
  );
}
</file>

<file path="components/VercelGatewayFallbackModels.tsx">
'use client';

import { useState, useEffect } from 'react';
import { X, ChevronDown } from 'lucide-react';
import { getModelsForProvider, type ModelInfo } from '@/lib/models';

interface VercelGatewayFallbackModelsProps {
  fallbackModels: string[];
  onFallbackModelsChange: (models: string[]) => void;
}

const STORAGE_KEY = 'vercel-gateway-fallback-models';

export function VercelGatewayFallbackModels({
  fallbackModels,
  onFallbackModelsChange,
}: VercelGatewayFallbackModelsProps) {
  const [isOpen, setIsOpen] = useState(false);
  const [availableModels] = useState<ModelInfo[]>(() =>
    getModelsForProvider('vercel-ai-gateway'),
  );

  const addModel = (modelId: string) => {
    if (!fallbackModels.includes(modelId)) {
      onFallbackModelsChange([...fallbackModels, modelId]);
    }
  };

  const removeModel = (modelId: string) => {
    onFallbackModelsChange(fallbackModels.filter((id) => id !== modelId));
  };

  const availableToAdd = availableModels.filter(
    (model) => !fallbackModels.includes(model.id),
  );

  return (
    <div className="flex flex-col gap-2">
      <button
        type="button"
        onClick={() => setIsOpen(!isOpen)}
        className="flex items-center justify-between w-full max-w-md px-3 py-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 hover:bg-zinc-50 dark:hover:bg-zinc-700 transition-colors"
      >
        <span className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
          Fallback Models ({fallbackModels.length})
        </span>
        <ChevronDown
          size={16}
          className={`text-zinc-500 transition-transform ${
            isOpen ? 'rotate-180' : ''
          }`}
        />
      </button>

      {isOpen && (
        <div className="flex flex-col gap-3 p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-zinc-50 dark:bg-zinc-900 max-w-md">
          {/* Current Fallback Models */}
          {fallbackModels.length > 0 ? (
            <div className="flex flex-col gap-2">
              <label className="text-xs font-medium text-zinc-700 dark:text-zinc-300">
                Current Fallback Models (in order):
              </label>
              <div className="flex flex-col gap-1.5">
                {fallbackModels.map((modelId, index) => {
                  const model = availableModels.find((m) => m.id === modelId);
                  return (
                    <div
                      key={modelId}
                      className="flex items-center justify-between p-2 rounded border border-zinc-200 dark:border-zinc-700 bg-white dark:bg-zinc-800"
                    >
                      <div className="flex items-center gap-2">
                        <span className="text-xs text-zinc-500 dark:text-zinc-400">
                          {index + 1}.
                        </span>
                        <span className="text-sm text-zinc-900 dark:text-zinc-100">
                          {model?.name || modelId}
                        </span>
                      </div>
                      <button
                        type="button"
                        onClick={() => removeModel(modelId)}
                        className="p-1 rounded hover:bg-zinc-100 dark:hover:bg-zinc-700 text-zinc-500 hover:text-red-600 dark:hover:text-red-400 transition-colors"
                        title="Remove"
                      >
                        <X size={14} />
                      </button>
                    </div>
                  );
                })}
              </div>
            </div>
          ) : (
            <p className="text-sm text-zinc-500 dark:text-zinc-400">
              No fallback models configured. Add models below.
            </p>
          )}

          {/* Add Model Dropdown */}
          {availableToAdd.length > 0 && (
            <div className="flex flex-col gap-2">
              <label className="text-xs font-medium text-zinc-700 dark:text-zinc-300">
                Add Fallback Model:
              </label>
              <div className="relative">
                <select
                  onChange={(e) => {
                    if (e.target.value) {
                      addModel(e.target.value);
                      e.target.value = ''; // Reset selection
                    }
                  }}
                  className="w-full appearance-none px-3 py-2 pr-10 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 text-sm focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent cursor-pointer"
                  defaultValue=""
                >
                  <option value="">Select a model to add...</option>
                  {availableToAdd.map((model) => (
                    <option key={model.id} value={model.id}>
                      {model.name}{' '}
                      {model.description && `- ${model.description}`}
                    </option>
                  ))}
                </select>
                <ChevronDown
                  size={16}
                  className="absolute right-3 top-1/2 -translate-y-1/2 text-zinc-500 pointer-events-none"
                />
              </div>
            </div>
          )}

          {availableToAdd.length === 0 && fallbackModels.length > 0 && (
            <p className="text-xs text-zinc-500 dark:text-zinc-400">
              All available models have been added.
            </p>
          )}

          {/* Info */}
          <p className="text-xs text-zinc-500 dark:text-zinc-400 mt-1">
            Fallback models are used in order if the primary model is
            unavailable.
          </p>
        </div>
      )}
    </div>
  );
}

// Hook to load fallback models from localStorage
export function useVercelGatewayFallbackModels(): [
  string[],
  (models: string[]) => void,
] {
  const [fallbackModels, setFallbackModels] = useState<string[]>([]);

  useEffect(() => {
    const saved = localStorage.getItem(STORAGE_KEY);
    if (saved) {
      try {
        const parsed = JSON.parse(saved);
        if (Array.isArray(parsed)) {
          setFallbackModels(parsed);
        }
      } catch (e) {
        console.error('Failed to parse saved fallback models', e);
      }
    }
  }, []);

  const updateFallbackModels = (models: string[]) => {
    setFallbackModels(models);
    if (models.length > 0) {
      localStorage.setItem(STORAGE_KEY, JSON.stringify(models));
    } else {
      localStorage.removeItem(STORAGE_KEY);
    }
  };

  return [fallbackModels, updateFallbackModels];
}
</file>

<file path="lib/models.ts">
import modelsConfig from './models.json';

export interface ModelInfo {
  id: string;
  name: string;
  description: string;
}

export interface ProviderModels {
  defaultModel: string;
  models: ModelInfo[];
}

export type ModelsConfig = Record<string, ProviderModels>;

export const models = modelsConfig as ModelsConfig;

export function getModelsForProvider(providerId: string): ModelInfo[] {
  return models[providerId]?.models || [];
}

export function getDefaultModel(providerId: string): string {
  return models[providerId]?.defaultModel || '';
}

export function getModelInfo(
  providerId: string,
  modelId: string,
): ModelInfo | undefined {
  return models[providerId]?.models.find((m) => m.id === modelId);
}
</file>

<file path=".prettierrc.json">
{
  "singleQuote": true
}
</file>

<file path="eslint.config.mjs">
import { defineConfig, globalIgnores } from 'eslint/config';
import nextVitals from 'eslint-config-next/core-web-vitals';
import nextTs from 'eslint-config-next/typescript';

const eslintConfig = defineConfig([
  ...nextVitals,
  ...nextTs,
  // Override default ignores of eslint-config-next.
  globalIgnores([
    // Default ignores of eslint-config-next:
    '.next/**',
    'out/**',
    'build/**',
    'next-env.d.ts',
  ]),
]);

export default eslintConfig;
</file>

<file path="jest.setup.ts">
import '@testing-library/jest-dom';

// Polyfill for TransformStream which is not available in jsdom environment
if (typeof globalThis.TransformStream === 'undefined') {
  // @ts-ignore - TransformStream polyfill
  globalThis.TransformStream = class TransformStream {
    constructor() {
      // Minimal polyfill - actual implementation not needed for tests
    }
  };
}
</file>

<file path="next.config.ts">
import type { NextConfig } from 'next';

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;
</file>

<file path="postcss.config.mjs">
const config = {
  plugins: {
    '@tailwindcss/postcss': {},
  },
};

export default config;
</file>

<file path="__tests__/providers.test.ts">
import {
  providerConfigs,
  getEnvApiKey,
  type ProviderId,
} from '@/lib/providers';

describe('Providers', () => {
  describe('providerConfigs', () => {
    it('should have all expected providers', () => {
      const expectedProviders: ProviderId[] = [
        'openai',
        'gemini',
        'anthropic',
        'deepseek',
        'qwen',
        'cohere',
        'vercel-ai-gateway',
      ];

      expectedProviders.forEach((provider) => {
        expect(providerConfigs[provider]).toBeDefined();
        expect(providerConfigs[provider]).toHaveProperty('createProvider');
        expect(providerConfigs[provider]).toHaveProperty('defaultModel');
      });
    });

    it('should create provider with API key', () => {
      const testApiKey = 'test-api-key-123';
      const provider = providerConfigs.openai.createProvider(testApiKey);
      expect(provider).toBeDefined();
    });
  });

  describe('getEnvApiKey', () => {
    it('should return undefined when env var is not set', () => {
      // Clear the env var for testing
      const originalEnv = process.env.OPENAI_API_KEY;
      delete process.env.OPENAI_API_KEY;

      const apiKey = getEnvApiKey('openai');
      expect(apiKey).toBeUndefined();

      // Restore original env
      if (originalEnv) {
        process.env.OPENAI_API_KEY = originalEnv;
      }
    });

    it('should map provider IDs to correct env var names', () => {
      const testCases: Array<{ provider: ProviderId; expectedEnv: string }> = [
        { provider: 'openai', expectedEnv: 'OPENAI_API_KEY' },
        { provider: 'gemini', expectedEnv: 'GOOGLE_GENERATIVE_AI_API_KEY' },
        { provider: 'anthropic', expectedEnv: 'ANTHROPIC_API_KEY' },
        { provider: 'deepseek', expectedEnv: 'DEEPSEEK_API_KEY' },
        { provider: 'qwen', expectedEnv: 'QWEN_API_KEY' },
        { provider: 'cohere', expectedEnv: 'COHERE_API_KEY' },
        {
          provider: 'vercel-ai-gateway',
          expectedEnv: 'VERCEL_AI_GATEWAY_API_KEY',
        },
      ];

      testCases.forEach(({ provider, expectedEnv }) => {
        // This test verifies the mapping exists, not that the env var is set
        const envKey = expectedEnv;
        expect(envKey).toBeDefined();
      });
    });
  });
});
</file>

<file path="jest.config.ts">
import type { Config } from 'jest';
import nextJest from 'next/jest.js';

const createJestConfig = nextJest({
  // Provide the path to your Next.js app to load next.config.js and .env files in your test environment
  dir: './',
});

// Add any custom config to be passed to Jest
const config: Config = {
  coverageProvider: 'v8',
  testEnvironment: 'jsdom',
  // Add more setup options before each test is run
  setupFilesAfterEnv: ['<rootDir>/jest.setup.ts'],
  // Handle module path aliases (matching tsconfig.json paths)
  moduleNameMapper: {
    '^@/(.*)$': '<rootDir>/$1',
  },
  // Mock web streams API for tests
  testEnvironmentOptions: {
    customExportConditions: [''],
  },
  // Transform ES modules from node_modules (marked uses ESM)
  transformIgnorePatterns: ['node_modules/(?!(marked)/)'],
  // Extensions to treat as ESM
  extensionsToTreatAsEsm: ['.ts', '.tsx'],
};

// createJestConfig is exported this way to ensure that next/jest can load the Next.js config which is async
export default createJestConfig(config);
</file>

<file path="README.md">
# AI Code Reviewer

A Next.js application that uses AI to review code files and GitHub pull requests. Supports multiple AI providers with streaming responses, markdown rendering, and a credit-based billing system.

## Features

- ü§ñ **Multi-Provider AI Support**: Choose from OpenAI, Google Gemini, Anthropic Claude, DeepSeek, Qwen, and Vercel AI Gateway
- üìù **Code Review**: Review local files or GitHub pull requests
- üí¨ **Chat Interface**: General-purpose AI chat with optional file reading tools
- üìä **Usage Tracking**: Credit-based billing system with cost tracking
- üåì **Dark Mode**: Built-in theme switcher
- üì± **Responsive Design**: Works on desktop and mobile devices
- ‚ö° **Streaming Responses**: Real-time streaming for better UX
- üé® **Markdown Rendering**: Beautiful markdown rendering for AI responses
- üß™ **Testing**: Jest test suite included

## Tech Stack

- **Framework**: Next.js 16 (App Router)
- **Language**: TypeScript
- **Styling**: Tailwind CSS 4
- **AI SDK**: Vercel AI SDK v5
- **Markdown**: Marked
- **Testing**: Jest + React Testing Library
- **Package Manager**: pnpm

## Getting Started

### Prerequisites

- Node.js 20+
- pnpm (recommended) or npm/yarn

### Installation

1. Clone the repository:

```bash
git clone <repository-url>
cd my-ai-app
```

2. Install dependencies:

```bash
pnpm install
```

3. Set up environment variables (optional - you can also add API keys in Settings):

```bash
cp .env.example .env
```

Add your API keys:

```env
OPENAI_API_KEY=your_key_here
GOOGLE_GENERATIVE_AI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here
DEEPSEEK_API_KEY=your_key_here
QWEN_API_KEY=your_key_here
VERCEL_AI_GATEWAY_API_KEY=your_key_here
```

4. Run the development server:

```bash
pnpm dev
```

5. Open [http://localhost:3000](http://localhost:3000) in your browser.

## Usage

### Code Review

1. Navigate to the home page (`/`)
2. Select your preferred AI provider and model
3. Choose input mode:
   - **File Path**: Enter a local file path to review
   - **PR Link**: Enter a GitHub PR URL (e.g., `https://github.com/owner/repo/pull/123`)
4. Optionally customize the system prompt
5. Click "Review Code" to start the review

### Chat

1. Navigate to `/chat`
2. Select your AI provider and model
3. Optionally enable file reading tools
4. Start chatting with the AI

### Settings

1. Navigate to `/settings`
2. Add or update API keys for different providers
3. Keys are stored in localStorage

## Project Structure

```
my-ai-app/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ billing/        # Billing API endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/          # Chat API endpoint
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ codereview/    # Code review API endpoint
‚îÇ   ‚îú‚îÄ‚îÄ chat/              # Chat page
‚îÇ   ‚îú‚îÄ‚îÄ settings/          # Settings page
‚îÇ   ‚îî‚îÄ‚îÄ page.tsx           # Code review page (home)
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ AISettingsPanel.tsx    # AI provider/model selector
‚îÇ   ‚îú‚îÄ‚îÄ MarkdownRenderer.tsx   # Markdown to HTML renderer
‚îÇ   ‚îî‚îÄ‚îÄ ThemeSwitcher.tsx      # Dark mode toggle
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îî‚îÄ‚îÄ useAIChat.ts           # Custom hook for AI chat
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ ai-handler.ts         # Shared AI request handler
‚îÇ   ‚îú‚îÄ‚îÄ billing.ts            # Credit/billing logic
‚îÇ   ‚îú‚îÄ‚îÄ models.ts              # Model configuration
‚îÇ   ‚îî‚îÄ‚îÄ providers.ts           # AI provider configurations
‚îî‚îÄ‚îÄ __tests__/                 # Jest test files
```

## API Routes

### `/api/codereview`

Code review endpoint with file reading and PR reading tools.

**Request Body:**

```json
{
  "messages": [...],
  "provider": "openai",
  "model": "gpt-4",
  "apiKey": "optional-api-key",
  "stream": true,
  "systemPrompt": "Custom system prompt"
}
```

### `/api/chat`

General chat endpoint with optional file reading tools.

**Request Body:**

```json
{
  "messages": [...],
  "provider": "openai",
  "model": "gpt-4",
  "apiKey": "optional-api-key",
  "stream": true,
  "systemPrompt": "Custom system prompt",
  "enableTools": false
}
```

### `/api/billing`

Get current credit balance and usage history.

## Available Scripts

- `pnpm dev` - Start development server
- `pnpm build` - Build for production
- `pnpm start` - Start production server
- `pnpm lint` - Run ESLint
- `pnpm format` - Format code with Prettier
- `pnpm test` - Run Jest tests
- `pnpm test:watch` - Run tests in watch mode
- `pnpm test:coverage` - Run tests with coverage report

## Testing

The project includes Jest tests for:

- Model utilities (`lib/models.ts`)
- Provider configurations (`lib/providers.ts`)
- Markdown renderer component
- Theme switcher component

Run tests:

```bash
pnpm test
```

## Supported AI Providers

See [`lib/models.json`](./lib/models.json) for the complete list of supported models for each provider.

| Provider          | Notes                    |
| ----------------- | ------------------------ |
| OpenAI            | Requires API key         |
| Google Gemini     | Requires API key         |
| Anthropic         | Requires API key         |
| DeepSeek          | Requires API key         |
| Qwen              | Requires API key         |
| Vercel AI Gateway | Requires gateway API key |

## Features in Detail

### Code Review Tools

- **readFile**: Reads local file contents
- **readPullRequest**: Fetches and parses GitHub PR files (public repos only)

### Billing System

- Credit-based system
- Tracks usage per model
- Cost calculation based on input/output tokens
- Usage history tracking

### Markdown Rendering

- Full markdown support (headings, lists, code blocks, tables, etc.)
- Syntax highlighting ready
- Dark mode compatible
- Compact spacing for readability

## Environment Variables

All API keys can be set via environment variables or through the Settings page:

- `OPENAI_API_KEY`
- `GOOGLE_GENERATIVE_AI_API_KEY`
- `ANTHROPIC_API_KEY`
- `DEEPSEEK_API_KEY`
- `QWEN_API_KEY`
- `VERCEL_AI_GATEWAY_API_KEY`

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

[Add your license here]

## Acknowledgments

- Built with [Next.js](https://nextjs.org)
- AI powered by [Vercel AI SDK](https://sdk.vercel.ai)
- Styled with [Tailwind CSS](https://tailwindcss.com)
</file>

<file path="tailwind.config.ts">
import type { Config } from 'tailwindcss';

const config: Config = {
  darkMode: 'class',
  content: [
    './pages/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    './app/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {},
  },
  plugins: [],
};
export default config;
</file>

<file path="components/ThemeSwitcher.tsx">
'use client';

import { useTheme } from 'next-themes';
import { useState, useEffect } from 'react';
import { Sun, Moon } from 'lucide-react';

const ThemeSwitcher = () => {
  const [mounted, setMounted] = useState(false);
  const { theme, setTheme } = useTheme();

  useEffect(() => {
    setMounted(true);
  }, []);

  if (!mounted) {
    return null;
  }

  return (
    <button
      onClick={() => setTheme(theme === 'dark' ? 'light' : 'dark')}
      className="flex items-center gap-2 px-3 py-2 text-sm font-medium text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white"
    >
      {theme === 'dark' ? <Sun size={16} /> : <Moon size={16} />}
      {theme === 'dark' ? 'Light' : 'Dark'} Mode
    </button>
  );
};

export default ThemeSwitcher;
</file>

<file path="lib/providers.ts">
import { createOpenAI } from '@ai-sdk/openai';
import { createGoogleGenerativeAI } from '@ai-sdk/google';
import { createAnthropic } from '@ai-sdk/anthropic';
import { createDeepSeek } from '@ai-sdk/deepseek';
import { createQwen } from 'qwen-ai-provider';
import { createCohere } from '@ai-sdk/cohere';
import { getDefaultModel } from '@/lib/models';

// Provider configurations - exported for reuse
export const providerConfigs = {
  openai: {
    createProvider: (apiKey: string) => createOpenAI({ apiKey }),
    defaultModel: getDefaultModel('openai'),
  },
  gemini: {
    createProvider: (apiKey: string) => createGoogleGenerativeAI({ apiKey }),
    defaultModel: getDefaultModel('gemini'),
  },
  anthropic: {
    createProvider: (apiKey: string) => createAnthropic({ apiKey }),
    defaultModel: getDefaultModel('anthropic'),
  },
  deepseek: {
    createProvider: (apiKey: string) =>
      createDeepSeek({
        apiKey,
        baseURL: 'https://api.deepseek.com',
      }),
    defaultModel: getDefaultModel('deepseek'),
  },
  qwen: {
    createProvider: (apiKey: string) => createQwen({ apiKey }),
    defaultModel: getDefaultModel('qwen'),
  },
  cohere: {
    createProvider: (apiKey: string) => createCohere({ apiKey }),
    defaultModel: getDefaultModel('cohere'),
  },
  'vercel-ai-gateway': {
    createProvider: (apiKey: string) =>
      createOpenAI({
        apiKey,
        baseURL: 'https://ai-gateway.vercel.sh/v1',
      }),
    defaultModel: getDefaultModel('vercel-ai-gateway'),
  },
};

export type ProviderId = keyof typeof providerConfigs;

export function getEnvApiKey(providerId: ProviderId): string | undefined {
  const envKeys: Record<ProviderId, string> = {
    openai: 'OPENAI_API_KEY',
    gemini: 'GOOGLE_GENERATIVE_AI_API_KEY',
    anthropic: 'ANTHROPIC_API_KEY',
    deepseek: 'DEEPSEEK_API_KEY',
    qwen: 'QWEN_API_KEY',
    cohere: 'COHERE_API_KEY',
    'vercel-ai-gateway': 'VERCEL_AI_GATEWAY_API_KEY',
  };
  return process.env[envKeys[providerId]];
}
</file>

<file path=".github/workflows/ai-code-review.yml">
name: AI Code Review

on:
  pull_request:
    types: [opened, synchronize]

jobs:
  review:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: AI Code Review
        env:
          # Required: API endpoint for the code review service
          API_ENDPOINT: ${{ secrets.AI_REVIEW_API_ENDPOINT }}
          # Optional: API key for the AI provider (if not provided, server-side keys will be used)
          # Recommended to set this if your API endpoint doesn't have server-side keys configured
          AI_API_KEY: ${{ secrets.AI_API_KEY }}
          # Optional: AI provider (defaults to 'openai' if not set)
          # Use 'vercel-ai-gateway' to use Vercel AI Gateway
          AI_PROVIDER: ${{ secrets.AI_PROVIDER }}
          # Optional: Specific model to use (uses provider default if not set)
          AI_MODEL: ${{ secrets.AI_MODEL }}
          # Optional: Fallback models for Vercel AI Gateway (comma-separated list)
          # Example: "openai/gpt-4,anthropic/claude-3-opus"
          AI_FALLBACK_MODELS: ${{ secrets.AI_FALLBACK_MODELS }}
          # GITHUB_TOKEN is automatically provided by GitHub Actions - no need to set it as a secret
          # It's available as ${{ secrets.GITHUB_TOKEN }} or via the GITHUB_TOKEN environment variable
        run: |
          # Get PR information
          PR_URL="${{ github.event.pull_request.html_url }}"
          PR_NUMBER="${{ github.event.pull_request.number }}"
          REPO_OWNER="${{ github.repository_owner }}"
          REPO_NAME="${{ github.event.repository.name }}"

          # Set defaults if not provided
          AI_PROVIDER="${AI_PROVIDER:-openai}"

          echo "Reviewing PR: $PR_URL"
          echo "Using provider: $AI_PROVIDER"

          # Prepare request payload
          # Note: apiKey is optional - if not provided, the API will use server-side stored keys
          # For GitHub Actions, it's recommended to provide the API key via secrets
          # Build base payload
          BASE_PAYLOAD=$(cat <<EOF
          {
            "messages": [
              {
                "role": "user",
                "content": "Please review this pull request: $PR_URL"
              }
            ],
            "provider": "$AI_PROVIDER",
            "stream": false
          }
          EOF
          )
          
          # Add apiKey if provided
          if [ -n "$AI_API_KEY" ]; then
            BASE_PAYLOAD=$(echo "$BASE_PAYLOAD" | jq --arg key "$AI_API_KEY" '. + {"apiKey": $key}')
          fi
          
          # Add model if provided
          if [ -n "$AI_MODEL" ]; then
            BASE_PAYLOAD=$(echo "$BASE_PAYLOAD" | jq --arg model "$AI_MODEL" '. + {"model": $model}')
          fi
          
          # Add fallbackModels if provided (for Vercel AI Gateway)
          if [ -n "$AI_FALLBACK_MODELS" ]; then
            # Convert comma-separated string to JSON array
            FALLBACK_ARRAY=$(echo "$AI_FALLBACK_MODELS" | jq -R -s -c 'split(",") | map(select(length > 0) | gsub("^\\s+|\\s+$"; ""))')
            BASE_PAYLOAD=$(echo "$BASE_PAYLOAD" | jq --argjson fallbacks "$FALLBACK_ARRAY" '. + {"fallbackModels": $fallbacks}')
          fi
          
          # Add GitHub token and PR URL for automatic comment posting
          # GITHUB_TOKEN is automatically provided by GitHub Actions
          GITHUB_TOKEN="${{ secrets.GITHUB_TOKEN }}"
          if [ -n "$GITHUB_TOKEN" ]; then
            BASE_PAYLOAD=$(echo "$BASE_PAYLOAD" | jq --arg token "$GITHUB_TOKEN" '. + {"githubToken": $token}')
            BASE_PAYLOAD=$(echo "$BASE_PAYLOAD" | jq --arg url "$PR_URL" '. + {"prUrl": $url}')
          fi
          
          REQUEST_PAYLOAD="$BASE_PAYLOAD"

          # Call the AI code review API (non-streaming)
          echo "Calling API endpoint: $API_ENDPOINT"
          echo "Request payload: $(echo "$REQUEST_PAYLOAD" | jq -c .)"
          
          # Use curl with better error handling
          HTTP_CODE=$(curl -s -o /tmp/response.json -w "%{http_code}" -X POST "$API_ENDPOINT" \
            -H "Content-Type: application/json" \
            -d "$REQUEST_PAYLOAD")
          
          REVIEW_RESPONSE=$(cat /tmp/response.json)
          
          echo "HTTP Status Code: $HTTP_CODE"
          echo "Response (first 500 chars): ${REVIEW_RESPONSE:0:500}"
          
          # Check HTTP status code
          if [ "$HTTP_CODE" -lt 200 ] || [ "$HTTP_CODE" -ge 300 ]; then
            echo "Error: API returned HTTP $HTTP_CODE"
            echo "Full response: $REVIEW_RESPONSE"
            exit 1
          fi
          
          # Check if response is empty
          if [ -z "$REVIEW_RESPONSE" ]; then
            echo "Error: Empty response from API"
            exit 1
          fi
          
          # Check if response is valid JSON
          if ! echo "$REVIEW_RESPONSE" | jq . > /dev/null 2>&1; then
            echo "Error: Invalid JSON response from API"
            echo "Response: $REVIEW_RESPONSE"
            exit 1
          fi
          
          # Check if the API call returned an error
          if echo "$REVIEW_RESPONSE" | jq -e '.error' > /dev/null 2>&1; then
            ERROR_MSG=$(echo "$REVIEW_RESPONSE" | jq -r '.error')
            echo "Error from API: $ERROR_MSG"
            exit 1
          fi
          
          # Extract the review text from the response
          # The API returns { text: "...", usage: {...} } for non-streaming
          REVIEW_TEXT=$(echo "$REVIEW_RESPONSE" | jq -r '.text // empty')
          
          if [ -z "$REVIEW_TEXT" ] || [ "$REVIEW_TEXT" = "null" ]; then
            echo "Error: No review content received"
            echo "Full response: $REVIEW_RESPONSE"
            echo "Response structure: $(echo "$REVIEW_RESPONSE" | jq 'keys')"
            exit 1
          fi

          # If githubToken was provided, the API automatically posted the comment
          # Otherwise, we would need to post it manually here (but that's not needed anymore)
          if [ -n "$GITHUB_TOKEN" ]; then
            echo "Review completed! Comment should be automatically posted to PR #$PR_NUMBER"
          else
            echo "Review completed! (No GitHub token provided, comment not posted)"
          fi
</file>

<file path="app/api/billing/route.ts">
import { getCredits, getUsageHistory } from '@/lib/billing';
import { NextResponse } from 'next/server';

export function GET() {
  const credits = getCredits();
  const usageHistory = getUsageHistory();
  
  // Calculate total cost from usage history
  const totalCost = usageHistory.reduce((sum, entry) => sum + entry.cost, 0);
  
  // Calculate total tokens consumed
  const totalTokens = usageHistory.reduce((sum, entry) => sum + entry.totalTokens, 0);
  
  return NextResponse.json({
    credits,
    totalCost,
    totalTokens,
    usageHistory: usageHistory.map((entry) => ({
      timestamp: entry.timestamp.toISOString(),
      model: entry.model,
      promptTokens: entry.promptTokens,
      completionTokens: entry.completionTokens,
      totalTokens: entry.totalTokens,
      cost: entry.cost,
    })),
  });
}
</file>

<file path="lib/ai-handler.ts">
import {
  streamText,
  generateText,
  convertToModelMessages,
  UIMessage,
  stepCountIs,
  type LanguageModel,
  type ToolSet,
  type StopCondition,
} from 'ai';
import { NextResponse } from 'next/server';
import { providerConfigs, ProviderId, getEnvApiKey } from '@/lib/providers';
import { deductCredits, getCredits } from '@/lib/billing';

export interface AIHandlerOptions {
  messages: UIMessage[] | Array<{ role: string; content: string }>;
  provider?: ProviderId;
  apiKey?: string;
  model?: string;
  stream?: boolean;
  systemPrompt?: string;
  tools?: ToolSet;
  stopWhen?: StopCondition<any> | Array<StopCondition<any>>;
  enableUsageMetadata?: boolean; // Whether to include usage in streaming response metadata
  logPrefix?: string; // Prefix for log messages
  enableStepLogging?: boolean; // Whether to log tool calls and results
  contextFileHash?: string; // Hash of context file for cache key generation
  fallbackModels?: string[]; // Fallback models for Vercel AI Gateway (providerOptions.gateway.models)
}

export async function handleAIRequest(options: AIHandlerOptions) {
  const {
    messages: rawMessages,
    provider: providerId = 'openai',
    apiKey,
    model: requestedModel,
    stream = true,
    systemPrompt,
    tools,
    stopWhen,
    enableUsageMetadata = false,
    logPrefix = 'AI',
    enableStepLogging = false,
    contextFileHash,
    fallbackModels,
  } = options;

  // Check credits
  if (getCredits() <= 0) {
    return NextResponse.json(
      { error: 'Insufficient credits' },
      { status: 402 },
    );
  }

  // Validate messages
  if (!rawMessages || !Array.isArray(rawMessages)) {
    return NextResponse.json(
      { error: 'Messages are required and must be an array' },
      { status: 400 },
    );
  }

  // Convert messages to ModelMessage[] format
  // For streaming: rawMessages is UIMessage[] format
  // For non-streaming: rawMessages is Array<{ role: string; content: string }> format
  // Normalize to UIMessage[] format first, then convert to ModelMessage[]
  let normalizedMessages: UIMessage[];
  
  if (stream) {
    // Streaming mode already sends UIMessage[] format
    normalizedMessages = rawMessages as UIMessage[];
  } else {
    // Non-streaming mode sends simple format, convert to UIMessage[] format
    // UIMessage can have either 'content' (string) or 'parts' (array)
    normalizedMessages = (rawMessages as Array<{ role: string; content: string }>).map(
      (m) => ({
        id: `msg-${Date.now()}-${Math.random()}`,
        role: m.role as 'user' | 'assistant' | 'system',
        parts: [{ type: 'text' as const, text: m.content }],
      }),
    ) as unknown as UIMessage[];
  }

  let messages = convertToModelMessages(normalizedMessages);

  // Cohere requires messages to have either content or tool calls
  // Filter out messages that have neither (which shouldn't happen, but be safe)
  if (providerId === 'cohere') {
    messages = messages.filter((m, i) => {
      // Check if content exists and is non-empty (handle string or array types)
      const contentStr =
        typeof m.content === 'string'
          ? m.content
          : Array.isArray(m.content)
            ? m.content
                .map((p: any) => (p.type === 'text' ? p.text : ''))
                .join('')
            : '';
      const hasContent = !!contentStr && contentStr.trim().length > 0;
      const hasToolCalls = !!(m as any).toolCalls?.length;
      const hasToolResults = !!(m as any).toolResults?.length;
      const isValid = hasContent || hasToolCalls || hasToolResults;

      if (!isValid) {
        console.warn(
          `[${logPrefix}] Filtering out invalid message at index ${i}: no content, tool calls, or tool results`,
        );
      }

      return isValid;
    });

    // Log messages for debugging
    console.log(
      `[${logPrefix}] Converted messages (${messages.length} valid):`,
      JSON.stringify(
        messages.map((m, i) => {
          const contentStr =
            typeof m.content === 'string'
              ? m.content
              : Array.isArray(m.content)
                ? m.content
                    .map((p: any) => (p.type === 'text' ? p.text : ''))
                    .join('')
                : '';
          return {
            index: i,
            role: m.role,
            hasContent: !!contentStr && contentStr.length > 0,
            contentLength: contentStr.length,
            hasToolCalls: !!(m as any).toolCalls?.length,
            hasToolResults: !!(m as any).toolResults?.length,
          };
        }),
        null,
        2,
      ),
    );
  }

  // Validate provider
  if (!providerConfigs[providerId]) {
    return NextResponse.json({ error: 'Invalid provider' }, { status: 400 });
  }

  const config = providerConfigs[providerId];

  // Log for debugging
  console.log(
    `[${logPrefix}] Provider: ${providerId}, Model: ${requestedModel || config.defaultModel}, Stream: ${stream}, Has API Key: ${!!apiKey}`,
  );

  // Get API key from request or environment
  const resolvedApiKey = apiKey || getEnvApiKey(providerId);

  if (!resolvedApiKey) {
    return NextResponse.json(
      {
        error: `No API key provided for ${providerId}. Please add it in Settings.`,
      },
      { status: 400 },
    );
  }

  // Log for debugging
  console.log(
    `[${logPrefix}] Requested model: ${requestedModel || config.defaultModel}, API key source: ${apiKey ? 'request body' : 'environment'}`,
  );

  const provider = config.createProvider(resolvedApiKey);
  const modelName = requestedModel || config.defaultModel;
  // Type assertion: providers return LanguageModelV1 | LanguageModelV2, but streamText expects LanguageModel
  const model = provider(modelName) as any as LanguageModel;

  // Provider-level caching explanation:
  // - OpenAI automatically caches responses when the exact same prompt is sent
  // - If contextFileHash is unchanged AND user query is identical, the full prompt (system + messages) will be identical
  // - OpenAI will then return cached responses automatically (faster + cheaper)
  // - The contextFileHash helps track when context changes, ensuring cache invalidation when needed
  // Note: Some providers may require explicit cache headers/options, but OpenAI's caching is automatic for identical prompts
  if (contextFileHash) {
    console.log(
      `[${logPrefix}] Context file hash: ${contextFileHash.substring(0, 8)}... (caching enabled when prompt is identical)`,
    );
  }

  if (stream) {
    // Streaming mode
    // Store usage from onFinish to use in messageMetadata
    let streamUsage: { inputTokens: number; outputTokens: number } | null = null;

    const result = streamText({
      model,
      system: systemPrompt,
      ...(tools && { tools }),
      messages,
      ...(stopWhen && { stopWhen }),
      // Add providerOptions for Vercel AI Gateway fallback models
      ...(providerId === 'vercel-ai-gateway' &&
        fallbackModels &&
        fallbackModels.length > 0 && {
          providerOptions: {
            gateway: {
              models: fallbackModels,
            },
          },
        }),
      onFinish: ({ usage, finishReason }) => {
        console.log(
          `[${logPrefix}] Stream finished. Reason: ${finishReason}, Tokens: ${usage.inputTokens}/${usage.outputTokens}`,
        );
        // Store usage for messageMetadata
        streamUsage = {
          inputTokens: usage.inputTokens ?? 0,
          outputTokens: usage.outputTokens ?? 0,
        };
        deductCredits(
          modelName,
          usage.inputTokens ?? 0,
          usage.outputTokens ?? 0,
        );
      },
      ...(enableStepLogging && {
        onStepFinish: ({ text, toolCalls, toolResults, finishReason }) => {
          if (toolCalls && toolCalls.length > 0) {
            console.log(
              `[${logPrefix}] Tool calls made: ${toolCalls.map((tc) => tc.toolName).join(', ')}`,
            );
          }
          if (toolResults && toolResults.length > 0) {
            console.log(
              `[${logPrefix}] Tool results received: ${toolResults.length} results, Step finish reason: ${finishReason}`,
            );
          }
        },
      }),
    });

    return result.toUIMessageStreamResponse({
      ...(enableUsageMetadata && {
        messageMetadata: ({ part }) => {
          // Include usage information when available
          if (part.type === 'finish') {
            // Log what's available in part
            console.log(
              `[${logPrefix}] messageMetadata called for finish part:`,
              JSON.stringify({
                hasTotalUsage: !!part.totalUsage,
                totalUsage: part.totalUsage,
                hasStreamUsage: !!streamUsage,
                streamUsage,
              }),
            );

            // Try to get usage from part.totalUsage first, then fall back to stored streamUsage
            const usage = part.totalUsage
              ? {
                  promptTokens: part.totalUsage.inputTokens ?? 0,
                  completionTokens: part.totalUsage.outputTokens ?? 0,
                  totalTokens:
                    part.totalUsage.totalTokens ??
                    (part.totalUsage.inputTokens ?? 0) +
                      (part.totalUsage.outputTokens ?? 0),
                }
              : streamUsage
                ? {
                    promptTokens: streamUsage.inputTokens,
                    completionTokens: streamUsage.outputTokens,
                    totalTokens:
                      streamUsage.inputTokens + streamUsage.outputTokens,
                  }
                : {
                    promptTokens: 0,
                    completionTokens: 0,
                    totalTokens: 0,
                  };

            console.log(
              `[${logPrefix}] Attaching usage metadata to message:`,
              JSON.stringify(usage),
            );

            return { usage };
          }
          return undefined;
        },
      }),
      onFinish: ({ messages, responseMessage }) => {
        console.log(
          `[${logPrefix}] Stream completed. Response message ID: ${responseMessage.id}`,
        );
        // Log usage if available in the response message
        if (enableUsageMetadata && (responseMessage as any).usage) {
          console.log(
            `[${logPrefix}] Usage attached to response message:`,
            (responseMessage as any).usage,
          );
        } else if (enableUsageMetadata && streamUsage) {
          console.log(
            `[${logPrefix}] Usage was available but not attached. Stream usage:`,
            streamUsage,
          );
        }
      },
    });
  } else {
    // Non-streaming mode - better for usage tracking
    try {
      const result = await generateText({
        model,
        system: systemPrompt,
        ...(tools && { tools }),
        messages,
        ...(stopWhen && { stopWhen }), // Include stopWhen for non-streaming too
        // Add providerOptions for Vercel AI Gateway fallback models
        ...(providerId === 'vercel-ai-gateway' &&
          fallbackModels &&
          fallbackModels.length > 0 && {
            providerOptions: {
              gateway: {
                models: fallbackModels,
              },
            },
          }),
      });

      // Log result for debugging
      console.log(
        `[${logPrefix}] Non-streaming result:`,
        JSON.stringify({
          hasText: !!result.text,
          textLength: result.text?.length ?? 0,
          textPreview: result.text?.substring(0, 100) ?? 'N/A',
          finishReason: result.finishReason,
          stepsCount: (result as any).steps?.length ?? 0,
          usage: result.usage,
        }),
      );

      // Extract text from result - check both result.text and steps
      let finalText = result.text || '';
      
      // If no text but we have steps, try to extract text from the last step
      if (!finalText && (result as any).steps) {
        const steps = (result as any).steps;
        // Look for text in the last step's content
        const lastStep = steps[steps.length - 1];
        if (lastStep?.content) {
          // Extract text parts from content array
          const textParts = lastStep.content
            .filter((part: any) => part.type === 'text')
            .map((part: any) => part.text)
            .join('');
          if (textParts) {
            finalText = textParts;
            console.log(
              `[${logPrefix}] Extracted text from steps: ${textParts.length} chars`,
            );
          }
        }
      }

      // If still no text and finishReason is tool-calls, the model made tool calls but hasn't generated final response
      if (!finalText && result.finishReason === 'tool-calls') {
        console.warn(
          `[${logPrefix}] Model made tool calls but no final text generated. This might indicate the model needs more steps.`,
        );
        finalText =
          'The model processed your request and made tool calls, but the final response is not yet available. Please try using streaming mode for better tool call handling.';
      }

      // Accurate usage tracking - AI SDK v5 uses inputTokens/outputTokens
      const promptTokens = result.usage.inputTokens ?? 0;
      const completionTokens = result.usage.outputTokens ?? 0;
      const billingResult = deductCredits(
        modelName,
        promptTokens,
        completionTokens,
      );

      return NextResponse.json({
        text: finalText,
        usage: {
          promptTokens,
          completionTokens,
          totalTokens: promptTokens + completionTokens,
        },
        billing: billingResult,
      });
    } catch (error: any) {
      // Handle citation parsing errors where the API returns valid text but citations format doesn't match SDK expectations
      // Some providers (like Cohere) return citations with tool_output instead of document field
      // The error.value contains the raw API response with the actual text
      // Error structure: AI_APICallError -> cause: [Error[AI_TypeValidationError]] -> value: {message, usage}
      
      // Log error structure for debugging
      console.log(`[${logPrefix}] Caught error:`, {
        errorName: error?.name,
        errorMessage: error?.message?.substring(0, 100),
        hasValue: !!error?.value,
        hasMessage: !!error?.value?.message,
        hasContent: !!error?.value?.message?.content,
        contentIsArray: Array.isArray(error?.value?.message?.content),
        hasCitations: !!error?.value?.message?.citations,
        hasResponseBody: !!error?.responseBody,
      });

      // Try to get the response data from error.value or parse from responseBody
      let responseData = error?.value;
      if (!responseData?.message?.content && error?.responseBody) {
        try {
          responseData = JSON.parse(error.responseBody);
          console.log(`[${logPrefix}] Parsed responseBody to get response data`);
        } catch (e) {
          console.warn(`[${logPrefix}] Failed to parse responseBody:`, e);
        }
      }

      const hasValueWithMessage =
        responseData?.message?.content &&
        Array.isArray(responseData.message.content);

      // If we have valid response data (error.value.message.content), extract the text
      // This handles citation parsing errors where the API returns valid text but citations format doesn't match SDK expectations
      if (hasValueWithMessage) {
        console.log(
          `[${logPrefix}] Error handler check:`,
          JSON.stringify({
            hasValueWithMessage,
            hasInvalidJson: error?.message?.includes('Invalid JSON response'),
            hasCitations: !!responseData?.message?.citations,
            willExtract: true,
          }),
        );
        console.log(
          `[${logPrefix}] Detected citation error with valid response data - extracting text`,
        );
        console.warn(
          `[${logPrefix}] Citation format mismatch detected, extracting text manually from response data`,
        );

        // Extract text from the API response format
        const apiMessage = responseData.message;
        let extractedText = '';

        if (Array.isArray(apiMessage.content)) {
          extractedText = apiMessage.content
            .filter((part: any) => part.type === 'text')
            .map((part: any) => part.text)
            .join('');
        }

        if (extractedText) {
          // Extract usage from response data if available
          // Cohere uses: usage.tokens.input_tokens / usage.tokens.output_tokens
          // Some providers use input_tokens/output_tokens format, others use inputTokens/outputTokens
          const usage = responseData?.usage?.tokens || responseData?.usage;
          const promptTokens =
            usage?.input_tokens ??
            usage?.inputTokens ??
            responseData?.usage?.billed_units?.input_tokens ??
            0;
          const completionTokens =
            usage?.output_tokens ??
            usage?.outputTokens ??
            responseData?.usage?.billed_units?.output_tokens ??
            0;

          console.log(
            `[${logPrefix}] Extracted text from API response: ${extractedText.length} chars, tokens: ${promptTokens}/${completionTokens}`,
          );

          const billingResult = deductCredits(
            modelName,
            promptTokens,
            completionTokens,
          );

          return NextResponse.json({
            text: extractedText,
            usage: {
              promptTokens,
              completionTokens,
              totalTokens: promptTokens + completionTokens,
            },
            billing: billingResult,
          });
        } else {
          console.warn(
            `[${logPrefix}] Failed to extract text from error.value.message.content`,
            {
              contentLength: apiMessage.content?.length ?? 0,
              contentTypes: apiMessage.content?.map((p: any) => p?.type),
            },
          );
        }
      }

      console.error(`[${logPrefix}] Error in generateText:`, error);
      // Log the error structure for debugging
      if (error?.value) {
        console.error(
          `[${logPrefix}] Error value structure:`,
          JSON.stringify(
            {
              hasMessage: !!error.value.message,
              hasContent: !!error.value.message?.content,
              hasUsage: !!error.value.usage,
              errorName: error.name,
              errorMessage: error.message,
              causeCount: error.cause?.length ?? 0,
              causeTypes: error.cause?.map((c: any) => c?.name || c?.code),
            },
            null,
            2,
          ),
        );
      }
      throw error;
    }
  }
}
</file>

<file path="lib/billing.ts">
// In a real-world application, this data should be stored in a database.
// This is a simulation for demonstration purposes.
// Using in-memory storage - will reset on server restart.

export const modelCosts: Record<string, { input: number; output: number }> = {
  'gpt-4o': {
    input: 0.005 / 1000, // Cost per 1000 tokens
    output: 0.015 / 1000, // Cost per 1000 tokens
  },
  'gemini-1.5-pro': {
    input: 0.00125 / 1000,
    output: 0.005 / 1000,
  },
  'claude-sonnet-4-20250514': {
    input: 0.003 / 1000,
    output: 0.015 / 1000,
  },
  'deepseek-chat': {
    input: 0.00014 / 1000,
    output: 0.00028 / 1000,
  },
  'qwen-plus': {
    input: 0.0008 / 1000,
    output: 0.002 / 1000,
  },
  'command-a-03-2025': {
    input: 0.003 / 1000,
    output: 0.015 / 1000,
  },
  'command-r-plus-08-2024': {
    input: 0.003 / 1000,
    output: 0.015 / 1000,
  },
  'command-r-08-2024': {
    input: 0.0005 / 1000,
    output: 0.0015 / 1000,
  },
  'command-r7b-12-2024': {
    input: 0.0003 / 1000,
    output: 0.0012 / 1000,
  },
  'command-a-reasoning-08-2025': {
    input: 0.003 / 1000,
    output: 0.015 / 1000,
  },
  'command-a-vision-07-2025': {
    input: 0.003 / 1000,
    output: 0.015 / 1000,
  },
  'command-a-translate-08-2025': {
    input: 0.002 / 1000,
    output: 0.008 / 1000,
  },
};

export let userCredits = 1.0; // Initial credits in USD

// Usage history for tracking (in-memory)
export const usageHistory: Array<{
  timestamp: Date;
  model: string;
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
  cost: number;
}> = [];

export function deductCredits(
  model: string,
  promptTokens: number,
  completionTokens: number,
): { cost: number; remaining: number } {
  const costs = modelCosts[model] || modelCosts['gpt-4o'];
  const cost = promptTokens * costs.input + completionTokens * costs.output;
  userCredits -= cost;

  // Track usage
  usageHistory.push({
    timestamp: new Date(),
    model,
    promptTokens,
    completionTokens,
    totalTokens: promptTokens + completionTokens,
    cost,
  });

  return { cost, remaining: userCredits };
}

export function getCredits() {
  return userCredits;
}

export function getUsageHistory() {
  return usageHistory;
}
</file>

<file path="app/globals.css">
@import 'tailwindcss';

/* Enable class-based dark mode for next-themes */
@custom-variant dark (&:where(.dark, .dark *));

:root {
  --background: #ffffff;
  --foreground: #171717;
}

.dark {
  --background: #0a0a0a;
  --foreground: #ededed;
}

body {
  background: var(--background);
  color: var(--foreground);
  font-family: Arial, Helvetica, sans-serif;
}

/* Markdown content styles */
.markdown-content * {
  line-height: 1;
}

.markdown-content h1 {
  @apply text-2xl font-bold text-zinc-900 dark:text-zinc-100;
  border-bottom: 1px solid rgb(228 228 231 / 0.5);
}

.markdown-content h2 {
  @apply text-xl font-semibold text-zinc-900 dark:text-zinc-100;
  border-bottom: 1px solid rgb(228 228 231 / 0.3);
}

.markdown-content h3 {
  @apply text-lg font-semibold text-zinc-900 dark:text-zinc-100;
}

.markdown-content h4 {
  @apply text-base font-semibold text-zinc-900 dark:text-zinc-100;
}

.markdown-content h5,
.markdown-content h6 {
  @apply text-sm font-semibold text-zinc-900 dark:text-zinc-100;
}

.markdown-content ul,
.markdown-content ol {
  @apply ml-6;
}

.markdown-content ul {
  @apply list-disc;
}

.markdown-content ol {
  @apply list-decimal;
}

.markdown-content blockquote {
  @apply border-l-4 border-zinc-300 dark:border-zinc-700 pl-4 italic text-zinc-600 dark:text-zinc-400;
}

.markdown-content code {
  @apply bg-zinc-100 dark:bg-zinc-800 px-1.5 py-0.5 rounded text-sm font-mono text-zinc-900 dark:text-zinc-100;
}

.markdown-content pre {
  @apply bg-zinc-100 dark:bg-zinc-800 p-3 rounded-lg overflow-x-auto;
  margin: 0;
}

.markdown-content pre code {
  @apply bg-transparent p-0;
}

.markdown-content a {
  @apply text-blue-600 dark:text-blue-400 hover:underline;
}

.markdown-content strong {
  @apply font-semibold text-zinc-900 dark:text-zinc-100;
}

.markdown-content em {
  @apply italic;
}

.markdown-content hr {
  @apply border-zinc-300 dark:border-zinc-700;
}

.markdown-content table {
  @apply w-full border-collapse;
}

.markdown-content th,
.markdown-content td {
  @apply border border-zinc-300 dark:border-zinc-700 px-4 py-2;
}

.markdown-content th {
  @apply bg-zinc-100 dark:bg-zinc-800 font-semibold text-zinc-900 dark:text-zinc-100;
}

.markdown-content img {
  @apply max-w-full h-auto rounded-lg;
}
</file>

<file path="lib/models.json">
{
  "openai": {
    "defaultModel": "gpt-5-mini",
    "models": [
      {
        "id": "gpt-5.2",
        "name": "GPT-5.2",
        "description": "The best model for coding and agentic tasks across industries"
      },
      {
        "id": "gpt-5.2-pro",
        "name": "GPT-5.2 Pro",
        "description": "The smartest and most precise model"
      },
      {
        "id": "gpt-5-mini",
        "name": "GPT-5 Mini",
        "description": "A faster, cheaper version of GPT-5 for well-defined tasks"
      },
      {
        "id": "gpt-4.1",
        "name": "GPT-4.1",
        "description": "Enhanced GPT-4 model"
      },
      {
        "id": "gpt-4.1-mini",
        "name": "GPT-4.1 Mini",
        "description": "Faster GPT-4.1 variant"
      },
      {
        "id": "gpt-4.1-nano",
        "name": "GPT-4.1 Nano",
        "description": "Lightweight GPT-4.1 variant"
      }
    ]
  },
  "gemini": {
    "defaultModel": "gemini-1.5-pro",
    "models": [
      {
        "id": "gemini-1.5-pro",
        "name": "Gemini 1.5 Pro",
        "description": "Most capable model"
      },
      {
        "id": "gemini-1.5-flash",
        "name": "Gemini 1.5 Flash",
        "description": "Faster and cheaper"
      },
      {
        "id": "gemini-pro",
        "name": "Gemini Pro",
        "description": "Previous generation"
      }
    ]
  },
  "anthropic": {
    "defaultModel": "claude-sonnet-4-20250514",
    "models": [
      {
        "id": "claude-sonnet-4-20250514",
        "name": "Claude Sonnet 4",
        "description": "Latest Claude model"
      },
      {
        "id": "claude-3-5-sonnet-20241022",
        "name": "Claude 3.5 Sonnet",
        "description": "Previous generation"
      },
      {
        "id": "claude-3-opus-20240229",
        "name": "Claude 3 Opus",
        "description": "Most capable"
      },
      {
        "id": "claude-3-sonnet-20240229",
        "name": "Claude 3 Sonnet",
        "description": "Balanced performance"
      },
      {
        "id": "claude-3-haiku-20240307",
        "name": "Claude 3 Haiku",
        "description": "Fast and affordable"
      }
    ]
  },
  "deepseek": {
    "defaultModel": "deepseek-coder",
    "models": [
      {
        "id": "deepseek-coder",
        "name": "DeepSeek Coder",
        "description": "Optimized for code"
      },
      {
        "id": "deepseek-chat",
        "name": "DeepSeek Chat",
        "description": "Standard model"
      }
    ]
  },
  "qwen": {
    "defaultModel": "qwen-plus",
    "models": [
      {
        "id": "qwen-plus",
        "name": "Qwen Plus",
        "description": "Enhanced model"
      },
      {
        "id": "qwen-turbo",
        "name": "Qwen Turbo",
        "description": "Faster model"
      },
      { "id": "qwen-max", "name": "Qwen Max", "description": "Most capable" }
    ]
  },
  "cohere": {
    "defaultModel": "command-r7b-12-2024",
    "models": [
      {
        "id": "command-a-03-2025",
        "name": "Command A",
        "description": "Most performant model, excels at tool use, agents, RAG, and multilingual (256K context)"
      },
      {
        "id": "command-r-plus-08-2024",
        "name": "Command R+",
        "description": "Best suited for complex RAG workflows and multi-step tool use (128K context)"
      },
      {
        "id": "command-r-08-2024",
        "name": "Command R",
        "description": "Instruction-following conversational model for complex workflows (128K context)"
      },
      {
        "id": "command-r7b-12-2024",
        "name": "Command R7B",
        "description": "Small, fast model for RAG, tool use, and agents (128K context)"
      },
      {
        "id": "command-a-reasoning-08-2025",
        "name": "Command A Reasoning",
        "description": "Reasoning model for nuanced problem-solving and agent-based tasks (256K context)"
      },
      {
        "id": "command-a-vision-07-2025",
        "name": "Command A Vision",
        "description": "Multimodal model for analyzing charts, graphs, diagrams, OCR, and document Q&A (128K context)"
      },
      {
        "id": "command-a-translate-08-2025",
        "name": "Command A Translate",
        "description": "State-of-the-art machine translation model for 23 languages (8K context)"
      }
    ]
  },
  "vercel-ai-gateway": {
    "defaultModel": "openai/gpt-5-mini",
    "models": [
      {
        "id": "openai/gpt-5.2",
        "name": "OpenAI GPT-5.2",
        "description": "The best model for coding and agentic tasks across industries"
      },
      {
        "id": "openai/gpt-5.2-pro",
        "name": "OpenAI GPT-5.2 Pro",
        "description": "The smartest and most precise model"
      },
      {
        "id": "openai/gpt-5-mini",
        "name": "OpenAI GPT-5 Mini",
        "description": "A faster, cheaper version of GPT-5 for well-defined tasks"
      },
      {
        "id": "openai/gpt-4.1",
        "name": "OpenAI GPT-4.1",
        "description": "Enhanced GPT-4 model"
      },
      {
        "id": "openai/gpt-4.1-mini",
        "name": "OpenAI GPT-4.1 Mini",
        "description": "Faster GPT-4.1 variant"
      },
      {
        "id": "openai/gpt-4.1-nano",
        "name": "OpenAI GPT-4.1 Nano",
        "description": "Lightweight GPT-4.1 variant"
      },
      {
        "id": "google/gemini-1.5-pro",
        "name": "Google Gemini 1.5 Pro",
        "description": "Most capable model"
      },
      {
        "id": "google/gemini-1.5-flash",
        "name": "Google Gemini 1.5 Flash",
        "description": "Faster and cheaper"
      },
      {
        "id": "google/gemini-pro",
        "name": "Google Gemini Pro",
        "description": "Previous generation"
      },
      {
        "id": "anthropic/claude-sonnet-4-20250514",
        "name": "Anthropic Claude Sonnet 4",
        "description": "Latest Claude model"
      },
      {
        "id": "anthropic/claude-3-5-sonnet-20241022",
        "name": "Anthropic Claude 3.5 Sonnet",
        "description": "Previous generation"
      },
      {
        "id": "anthropic/claude-3-opus-20240229",
        "name": "Anthropic Claude 3 Opus",
        "description": "Most capable"
      },
      {
        "id": "anthropic/claude-3-sonnet-20240229",
        "name": "Anthropic Claude 3 Sonnet",
        "description": "Balanced performance"
      },
      {
        "id": "anthropic/claude-3-haiku-20240307",
        "name": "Anthropic Claude 3 Haiku",
        "description": "Fast and affordable"
      },
      {
        "id": "deepseek/deepseek-coder",
        "name": "DeepSeek Coder",
        "description": "Optimized for code"
      },
      {
        "id": "deepseek/deepseek-chat",
        "name": "DeepSeek Chat",
        "description": "Standard model"
      },
      {
        "id": "qwen/qwen-plus",
        "name": "Qwen Plus",
        "description": "Enhanced model"
      },
      {
        "id": "qwen/qwen-turbo",
        "name": "Qwen Turbo",
        "description": "Faster model"
      },
      {
        "id": "qwen/qwen-max",
        "name": "Qwen Max",
        "description": "Most capable"
      }
    ]
  }
}
</file>

<file path="app/settings/page.tsx">
'use client';

import { useState, useEffect } from 'react';
import Link from 'next/link';
import { Eye, EyeOff, Save, ArrowLeft, Sparkles } from 'lucide-react';

interface ApiKeyConfig {
  id: string;
  name: string;
  placeholder: string;
  key: string;
}

const defaultProviders: ApiKeyConfig[] = [
  { id: 'openai', name: 'OpenAI', placeholder: 'sk-...', key: '' },
  { id: 'gemini', name: 'Google Gemini', placeholder: 'AIza...', key: '' },
  { id: 'deepseek', name: 'DeepSeek', placeholder: 'sk-...', key: '' },
  { id: 'anthropic', name: 'Anthropic', placeholder: 'sk-ant-...', key: '' },
  { id: 'qwen', name: 'Qwen (Alibaba)', placeholder: 'sk-...', key: '' },
  { id: 'cohere', name: 'Cohere', placeholder: 'co_...', key: '' },
  {
    id: 'vercel-ai-gateway',
    name: 'Vercel AI Gateway',
    placeholder: 'vag_...',
    key: '',
  },
];

export default function SettingsPage() {
  const [providers, setProviders] = useState<ApiKeyConfig[]>(defaultProviders);
  const [visibleKeys, setVisibleKeys] = useState<Record<string, boolean>>({});
  const [saved, setSaved] = useState(false);
  const [githubToken, setGithubToken] = useState('');

  useEffect(() => {
    // Load saved keys from localStorage
    const savedKeys = localStorage.getItem('ai-api-keys');
    if (savedKeys) {
      try {
        const parsed = JSON.parse(savedKeys);
        setProviders((prev) =>
          prev.map((p) => ({
            ...p,
            key: parsed[p.id] || '',
          })),
        );
        // Load GitHub token if present
        if (parsed.githubToken) {
          setGithubToken(parsed.githubToken);
        }
      } catch (e) {
        console.error('Failed to parse saved keys', e);
      }
    }
  }, []);

  const toggleVisibility = (id: string) => {
    setVisibleKeys((prev) => ({ ...prev, [id]: !prev[id] }));
  };

  const updateKey = (id: string, value: string) => {
    setProviders((prev) =>
      prev.map((p) => (p.id === id ? { ...p, key: value } : p)),
    );
    setSaved(false);
  };

  const handleSave = () => {
    const keysToSave: Record<string, string> = providers.reduce(
      (acc, p) => ({ ...acc, [p.id]: p.key }),
      {} as Record<string, string>,
    );
    // Include GitHub token in saved keys
    if (githubToken) {
      keysToSave.githubToken = githubToken;
    }
    localStorage.setItem('ai-api-keys', JSON.stringify(keysToSave));
    setSaved(true);
    setTimeout(() => setSaved(false), 2000);
  };

  return (
    <div className="w-full max-w-3xl mx-auto py-16 px-4 sm:px-6 lg:px-8">
      <div className="flex flex-col gap-8">
        <div className="flex flex-col gap-2">
          <Link
            href="/"
            className="flex items-center gap-2 text-sm text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white w-fit"
          >
            <ArrowLeft size={16} />
            Back to Home
          </Link>
          <h1 className="text-3xl font-semibold leading-10 tracking-tight text-black dark:text-zinc-50">
            Settings
          </h1>
          <p className="text-lg leading-8 text-zinc-600 dark:text-zinc-400">
            Configure your AI provider API keys below.
          </p>
        </div>

        <div className="flex flex-col gap-6">
          <div className="flex items-center gap-2 text-zinc-800 dark:text-zinc-200">
            <Sparkles size={20} />
            <h2 className="text-xl font-medium">AI Provider API Keys</h2>
          </div>

          <div className="flex flex-col gap-4">
            {providers.map((provider) => (
              <div
                key={provider.id}
                className="flex flex-col gap-2 p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-white dark:bg-zinc-900"
              >
                <label
                  htmlFor={provider.id}
                  className="text-sm font-medium text-zinc-700 dark:text-zinc-300"
                >
                  {provider.name}
                </label>
                <div className="relative flex items-center">
                  <input
                    id={provider.id}
                    type={visibleKeys[provider.id] ? 'text' : 'password'}
                    value={provider.key}
                    onChange={(e) => updateKey(provider.id, e.target.value)}
                    placeholder={provider.placeholder}
                    className="w-full px-3 py-2 pr-10 text-sm border border-zinc-300 dark:border-zinc-700 rounded-md bg-zinc-50 dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 placeholder-zinc-400 dark:placeholder-zinc-500 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
                  />
                  <button
                    type="button"
                    onClick={() => toggleVisibility(provider.id)}
                    className="absolute right-2 p-1 text-zinc-500 hover:text-zinc-700 dark:hover:text-zinc-300"
                  >
                    {visibleKeys[provider.id] ? (
                      <EyeOff size={18} />
                    ) : (
                      <Eye size={18} />
                    )}
                  </button>
                </div>
              </div>
            ))}
          </div>

          <p className="text-xs text-zinc-500 dark:text-zinc-500">
            API keys are stored locally in your browser. They are never sent to
            our servers.
          </p>
        </div>

        <div className="flex flex-col gap-6">
          <div className="flex items-center gap-2 text-zinc-800 dark:text-zinc-200">
            <Sparkles size={20} />
            <h2 className="text-xl font-medium">GitHub Integration</h2>
          </div>

          <div className="flex flex-col gap-2 p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-white dark:bg-zinc-900">
            <label
              htmlFor="github-token"
              className="text-sm font-medium text-zinc-700 dark:text-zinc-300"
            >
              GitHub Token
            </label>
            <p className="text-xs text-zinc-500 dark:text-zinc-500 mb-2">
              Required for GitHub Actions workflow to post PR comments. Create a
              personal access token with{' '}
              <code className="px-1 py-0.5 bg-zinc-100 dark:bg-zinc-800 rounded">
                repo
              </code>{' '}
              scope.
            </p>
            <div className="relative flex items-center">
              <input
                id="github-token"
                type={visibleKeys['github-token'] ? 'text' : 'password'}
                value={githubToken}
                onChange={(e) => {
                  setGithubToken(e.target.value);
                  setSaved(false);
                }}
                placeholder="ghp_..."
                className="w-full px-3 py-2 pr-10 text-sm border border-zinc-300 dark:border-zinc-700 rounded-md bg-zinc-50 dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 placeholder-zinc-400 dark:placeholder-zinc-500 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
              />
              <button
                type="button"
                onClick={() => toggleVisibility('github-token')}
                className="absolute right-2 p-1 text-zinc-500 hover:text-zinc-700 dark:hover:text-zinc-300"
              >
                {visibleKeys['github-token'] ? (
                  <EyeOff size={18} />
                ) : (
                  <Eye size={18} />
                )}
              </button>
            </div>
          </div>

          <button
            onClick={handleSave}
            className="flex items-center justify-center gap-2 w-full sm:w-fit px-6 py-3 text-sm font-medium text-white bg-blue-600 hover:bg-blue-700 rounded-lg transition-colors"
          >
            <Save size={16} />
            {saved ? 'Saved!' : 'Save API Keys'}
          </button>
        </div>
      </div>
    </div>
  );
}
</file>

<file path="components/AISettingsPanel.tsx">
'use client';

import { useState, useEffect, useMemo } from 'react';
import { ChevronDown, Zap, Clock } from 'lucide-react';
import {
  getModelsForProvider,
  getDefaultModel,
  type ModelInfo,
} from '@/lib/models';
import {
  VercelGatewayFallbackModels,
  useVercelGatewayFallbackModels,
} from './VercelGatewayFallbackModels';
import { useBilling } from './Billing';

export const providers = [
  { id: 'openai', name: 'OpenAI' },
  { id: 'gemini', name: 'Google Gemini' },
  { id: 'anthropic', name: 'Anthropic (Claude)' },
  { id: 'deepseek', name: 'DeepSeek' },
  { id: 'qwen', name: 'Qwen (Alibaba)' },
  { id: 'cohere', name: 'Cohere' },
  { id: 'vercel-ai-gateway', name: 'Vercel AI Gateway' },
];

export interface UsageInfo {
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
}

export interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  usage?: UsageInfo;
}

interface AISettingsPanelProps {
  selectedProvider: string;
  onProviderChange: (provider: string) => void;
  selectedModel?: string;
  onModelChange?: (model: string) => void;
  useStreaming: boolean;
  onStreamingChange: (streaming: boolean) => void;
  showCredits?: boolean;
  fallbackModels?: string[];
  onFallbackModelsChange?: (models: string[]) => void;
}

export function AISettingsPanel({
  selectedProvider,
  onProviderChange,
  selectedModel,
  onModelChange,
  useStreaming,
  onStreamingChange,
  showCredits = true,
  fallbackModels,
  onFallbackModelsChange,
}: AISettingsPanelProps) {
  // Use the shared useBilling hook instead of local state
  const { billingData, refetch: refetchBilling } = useBilling();
  const [apiKeys, setApiKeys] = useState<Record<string, string>>({});
  const [internalFallbackModels, setInternalFallbackModels] =
    useVercelGatewayFallbackModels();

  // Use prop fallback models if provided, otherwise use internal state
  const currentFallbackModels =
    fallbackModels !== undefined ? fallbackModels : internalFallbackModels;
  const handleFallbackModelsChange =
    onFallbackModelsChange || setInternalFallbackModels;

  // Get available models for the selected provider
  const availableModels = useMemo(
    () => getModelsForProvider(selectedProvider),
    [selectedProvider],
  );

  // Determine the current selected model (use prop or default)
  const currentModel = selectedModel || getDefaultModel(selectedProvider) || '';

  // Handle provider change - reset model to default
  const handleProviderChange = (provider: string) => {
    onProviderChange(provider);
    if (onModelChange) {
      onModelChange(getDefaultModel(provider));
    }
  };

  useEffect(() => {
    // Load API keys from localStorage
    const savedKeys = localStorage.getItem('ai-api-keys');
    if (savedKeys) {
      try {
        setApiKeys(JSON.parse(savedKeys));
      } catch (e) {
        console.error('Failed to parse saved keys', e);
      }
    }
  }, []);

  const currentProvider = providers.find((p) => p.id === selectedProvider);

  return (
    <div className="flex flex-col gap-4">
      {/* Credits/Cost Display */}
      {showCredits && billingData && (
        <div className="flex flex-col gap-1">
          <p className="text-lg leading-8 text-zinc-600 dark:text-zinc-400">
            Total Cost:{' '}
            <span className="font-semibold text-black dark:text-white">
              ${billingData?.totalCost?.toFixed(4) ?? 0}
            </span>
          </p>
          {billingData.totalTokens !== undefined && (
            <p className="text-sm text-zinc-500 dark:text-zinc-500">
              Total Tokens:{' '}
              <span className="font-semibold text-zinc-700 dark:text-zinc-300">
                {billingData.totalTokens.toLocaleString()}
              </span>
            </p>
          )}
        </div>
      )}

      {/* Provider Selector */}
      <div className="flex flex-col gap-2">
        <label className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
          AI Provider
        </label>
        <div className="relative w-full max-w-md">
          <select
            value={selectedProvider}
            onChange={(e) => handleProviderChange(e.target.value)}
            className="w-full appearance-none px-3 py-2 pr-10 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent cursor-pointer"
          >
            {providers.map((provider) => (
              <option key={provider.id} value={provider.id}>
                {provider.name}
              </option>
            ))}
          </select>
          <ChevronDown
            size={16}
            className="absolute right-3 top-1/2 -translate-y-1/2 text-zinc-500 pointer-events-none"
          />
        </div>
        {!apiKeys[selectedProvider] && (
          <p className="text-xs text-amber-600 dark:text-amber-400">
            No API key set for {currentProvider?.name}. Add it in Settings.
          </p>
        )}
      </div>

      {/* Model Selector */}
      {availableModels.length > 0 && onModelChange && (
        <div className="flex flex-col gap-2">
          <label className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
            Model
          </label>
          <div className="relative w-full max-w-md">
            <select
              value={currentModel}
              onChange={(e) => onModelChange(e.target.value)}
              className="w-full appearance-none px-3 py-2 pr-10 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent cursor-pointer"
            >
              {availableModels.map((model) => (
                <option key={model.id} value={model.id}>
                  {model.name} {model.description && `- ${model.description}`}
                </option>
              ))}
            </select>
            <ChevronDown
              size={16}
              className="absolute right-3 top-1/2 -translate-y-1/2 text-zinc-500 pointer-events-none"
            />
          </div>
        </div>
      )}

      {/* Vercel AI Gateway Fallback Models */}
      {selectedProvider === 'vercel-ai-gateway' && (
        <VercelGatewayFallbackModels
          fallbackModels={currentFallbackModels}
          onFallbackModelsChange={handleFallbackModelsChange}
        />
      )}

      {/* Streaming Toggle */}
      <div className="flex items-center gap-3 max-w-md">
        <button
          type="button"
          onClick={() => onStreamingChange(true)}
          className={`flex items-center gap-2 px-3 py-2 rounded-md text-sm font-medium transition-colors ${
            useStreaming
              ? 'bg-blue-600 text-white'
              : 'bg-zinc-100 dark:bg-zinc-800 text-zinc-700 dark:text-zinc-300 hover:bg-zinc-200 dark:hover:bg-zinc-700'
          }`}
        >
          <Zap size={14} />
          Streaming
        </button>
        <button
          type="button"
          onClick={() => onStreamingChange(false)}
          className={`flex items-center gap-2 px-3 py-2 rounded-md text-sm font-medium transition-colors ${
            !useStreaming
              ? 'bg-blue-600 text-white'
              : 'bg-zinc-100 dark:bg-zinc-800 text-zinc-700 dark:text-zinc-300 hover:bg-zinc-200 dark:hover:bg-zinc-700'
          }`}
        >
          <Clock size={14} />
          Non-Streaming
        </button>
        <span className="text-xs text-zinc-500 dark:text-zinc-500">
          {useStreaming ? 'Real-time output' : 'Better usage tracking'}
        </span>
      </div>
    </div>
  );
}

// Hook to load API keys from localStorage
export function useApiKeys() {
  const [apiKeys, setApiKeys] = useState<Record<string, string>>({});

  useEffect(() => {
    const savedKeys = localStorage.getItem('ai-api-keys');
    if (savedKeys) {
      try {
        setApiKeys(JSON.parse(savedKeys));
      } catch (e) {
        console.error('Failed to parse saved keys', e);
      }
    }
  }, []);

  return apiKeys;
}
</file>

<file path="hooks/useAIChat.ts">
'use client';

import { useChat } from '@ai-sdk/react';
import { DefaultChatTransport } from 'ai';
import { useState, useMemo, useCallback, useRef, useEffect } from 'react';
import { useApiKeys, Message } from '@/components/AISettingsPanel';
import { useBilling } from '@/components/Billing';
import { getDefaultModel } from '@/lib/models';

interface UseAIChatOptions {
  /** API endpoint to call */
  endpoint: string;
  /** Extra body parameters to include in requests (can be a function for dynamic values) */
  extraBody?: () => Record<string, unknown>;
}

export function useAIChat({ endpoint, extraBody }: UseAIChatOptions) {
  const [selectedProvider, setSelectedProvider] = useState('openai');
  const [selectedModel, setSelectedModel] = useState<string>(() =>
    getDefaultModel('openai'),
  );

  // Reset model when provider changes
  const handleProviderChange = useCallback((provider: string) => {
    const defaultModel = getDefaultModel(provider);
    setSelectedProvider(provider);
    setSelectedModel(defaultModel);
  }, []);
  const [inputValue, setInputValue] = useState('');
  const [useStreaming, setUseStreaming] = useState(true);
  const [nonStreamingMessages, setNonStreamingMessages] = useState<Message[]>(
    [],
  );
  const [isNonStreamingLoading, setIsNonStreamingLoading] = useState(false);
  const [nonStreamingError, setNonStreamingError] = useState<Error | null>(
    null,
  );
  const [streamingErrorState, setStreamingErrorState] = useState<Error | null>(
    null,
  );
  // Track usage for streaming messages - map message ID to usage
  const [messageUsageMap, setMessageUsageMap] = useState<
    Map<string, { promptTokens: number; completionTokens: number; totalTokens: number }>
  >(new Map());

  const apiKeys = useApiKeys();
  const { billingData, refetch: refetchBilling } = useBilling();

  // Use refs to store latest values so the transport callback always reads current values
  const providerRef = useRef(selectedProvider);
  const modelRef = useRef(selectedModel);
  const apiKeysRef = useRef(apiKeys);
  const extraBodyRef = useRef(extraBody);

  // Update refs when values change
  useEffect(() => {
    providerRef.current = selectedProvider;
    modelRef.current = selectedModel;
    apiKeysRef.current = apiKeys;
    extraBodyRef.current = extraBody;
  }, [selectedProvider, selectedModel, apiKeys, extraBody]);

  // Use DefaultChatTransport with prepareSendMessagesRequest to ensure correct format
  // DefaultChatTransport works with toUIMessageStreamResponse() which properly handles errors
  const transport = useMemo(() => {
    console.log(
      `[Transport Created] Provider: ${selectedProvider}, Model: ${selectedModel}`,
    );

    return new DefaultChatTransport({
      api: endpoint,
      // Use prepareSendMessagesRequest to explicitly include messages in the request body
      prepareSendMessagesRequest: ({ messages, body: requestBody }) => {
        // Read current values from refs to ensure we always use the latest
        const currentProvider = providerRef.current;
        const currentModel = modelRef.current;
        const currentApiKeys = apiKeysRef.current;
        const providerApiKey = currentApiKeys[currentProvider];

        // Debug logging for all providers
        console.log(
          `[Frontend Streaming] Provider: ${currentProvider}, Model: ${currentModel}, API Key: ${providerApiKey ? providerApiKey.substring(0, 10) + '...' : 'NOT SET'}`,
        );

        return {
          body: {
            messages,
            provider: currentProvider,
            model: currentModel,
            apiKey: providerApiKey, // Only send if it exists
            stream: true,
            ...(extraBodyRef.current?.() ?? {}),
            ...requestBody,
          },
        };
      },
    });
  }, [endpoint, selectedProvider, selectedModel, apiKeys, extraBody]);

  const {
    messages: streamingMessages,
    status,
    sendMessage,
    error: useChatError,
    clearError: clearUseChatError,
  } = useChat({
    transport,
    onError: (error) => {
      console.error('Streaming error:', error);
      setStreamingErrorState(error);
    },
    onFinish: ({ message }) => {
      // Try to extract usage from message metadata if available
      // The messageMetadata callback should attach usage, but if it's not preserved,
      // we'll need to get it from the stream response headers or track it separately
      console.log('[Frontend Streaming] Stream finished, message:', {
        id: message.id,
        hasUsage: 'usage' in message,
        usage: (message as any).usage,
        messageKeys: Object.keys(message),
        // Check for metadata
        hasMetadata: 'metadata' in message,
        metadata: (message as any).metadata,
      });
      refetchBilling();
    },
  });

  // Combine useChat error with our state error, and check status for error state
  const streamingError =
    useChatError ||
    streamingErrorState ||
    (status === 'error'
      ? new Error('An error occurred during streaming')
      : null);

  const isStreamingLoading = status === 'streaming' || status === 'submitted';
  const isLoading = useStreaming ? isStreamingLoading : isNonStreamingLoading;

  // Debug: Check if usage metadata is attached to streaming messages
  useEffect(() => {
    if (useStreaming && streamingMessages.length > 0) {
      const lastMessage = streamingMessages[streamingMessages.length - 1];
      if (lastMessage.role === 'assistant') {
        const msgAny = lastMessage as any;
        console.log('[Frontend Streaming] Message check:', {
          messageId: lastMessage.id,
          hasUsage: 'usage' in lastMessage,
          usage: msgAny.usage,
          hasMetadata: 'metadata' in lastMessage,
          metadata: msgAny.metadata,
          messageKeys: Object.keys(lastMessage),
          // Check all properties
          allProps: Object.keys(msgAny),
        });
      }
    }
  }, [streamingMessages, useStreaming]);

  // Non-streaming submit handler
  const handleNonStreamingSubmit = useCallback(async () => {
    if (!inputValue.trim() || isLoading) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      role: 'user',
      content: inputValue,
    };

    setNonStreamingMessages((prev) => [...prev, userMessage]);
    setInputValue('');
    setIsNonStreamingLoading(true);
    setNonStreamingError(null);

    try {
      const requestBody = {
        messages: [...nonStreamingMessages, userMessage].map((m) => ({
          role: m.role,
          content: m.content,
        })),
        provider: selectedProvider,
        model: selectedModel,
        apiKey: apiKeys[selectedProvider],
        stream: false,
        ...(extraBody?.() ?? {}),
      };

      // Debug logging for non-streaming requests
      console.log(
        `[Frontend Non-Streaming] Provider: ${selectedProvider}, Model: ${selectedModel}, API Key: ${apiKeys[selectedProvider] ? apiKeys[selectedProvider].substring(0, 10) + '...' : 'NOT SET'}`,
      );

      const response = await fetch(endpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(requestBody),
      });

      let data;
      try {
        const text = await response.text();
        data = text ? JSON.parse(text) : {};
      } catch (parseError) {
        throw new Error(
          `Failed to parse response: ${response.status} ${response.statusText}`,
        );
      }

      if (!response.ok) {
        throw new Error(
          data.error ||
            data.message ||
            `HTTP error! status: ${response.status}`,
        );
      }

      // Log response data for debugging
      console.log('[Frontend Non-Streaming] Response data:', {
        hasText: !!data.text,
        textLength: data.text?.length ?? 0,
        textPreview: data.text?.substring(0, 100) ?? 'N/A',
        usage: data.usage,
      });

      if (!data.text || data.text.trim().length === 0) {
        console.warn(
          '[Frontend Non-Streaming] Warning: Empty or missing text in response',
        );
      }

      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: 'assistant',
        content: data.text || 'No response received from the model.',
        usage: data.usage,
      };

      setNonStreamingMessages((prev) => [...prev, assistantMessage]);
      refetchBilling();
    } catch (error) {
      console.error('Error:', error);
      setNonStreamingError(
        error instanceof Error ? error : new Error(String(error)),
      );
    } finally {
      setIsNonStreamingLoading(false);
    }
  }, [
    inputValue,
    isLoading,
    nonStreamingMessages,
    selectedProvider,
    selectedModel,
    apiKeys,
    endpoint,
    extraBody,
    refetchBilling,
  ]);

  // Unified submit handler
  const handleSubmit = useCallback(
    async (e: React.FormEvent) => {
      e.preventDefault();
      if (!inputValue.trim() || isLoading) return;

      if (useStreaming) {
        // Clear previous errors before sending new message
        setStreamingErrorState(null);
        // Use sendMessage with text format (AI SDK 5 style)
        await sendMessage({ text: inputValue });
        setInputValue('');
      } else {
        await handleNonStreamingSubmit();
      }
    },
    [
      inputValue,
      isLoading,
      useStreaming,
      sendMessage,
      handleNonStreamingSubmit,
    ],
  );

  // Enhance streaming messages with usage if available
  // Since messageMetadata might not be preserved by useChat, we'll manually add usage
  const enhancedStreamingMessages = useMemo(() => {
    if (!useStreaming) return streamingMessages;
    
    return streamingMessages.map((msg) => {
      // If message already has usage, return as-is
      if ('usage' in msg && msg.usage) {
        return msg;
      }
      
      // Check if we have stored usage for this message
      const storedUsage = messageUsageMap.get(msg.id);
      if (storedUsage) {
        return { ...msg, usage: storedUsage };
      }
      
      // Check if usage is in message metadata
      const msgAny = msg as any;
      if (msgAny.metadata?.usage) {
        return { ...msg, usage: msgAny.metadata.usage };
      }
      
      return msg;
    });
  }, [streamingMessages, useStreaming, messageUsageMap]);

  // Use appropriate messages based on mode
  const messages = useStreaming ? enhancedStreamingMessages : nonStreamingMessages;

  // Use appropriate error based on mode
  const error = useStreaming ? streamingError : nonStreamingError;

  // Clear error function
  const clearError = useCallback(() => {
    if (useStreaming) {
      clearUseChatError();
      setStreamingErrorState(null);
    } else {
      setNonStreamingError(null);
    }
  }, [useStreaming, clearUseChatError]);

  return {
    // State
    selectedProvider,
    setSelectedProvider: handleProviderChange,
    selectedModel,
    setSelectedModel,
    inputValue,
    setInputValue,
    useStreaming,
    setUseStreaming,
    isLoading,
    messages,
    billingData,
    error,

    // Handlers
    handleSubmit,
    clearError,
  };
}
</file>

<file path="app/chat/page.tsx">
'use client';

import { useState, useCallback } from 'react';
import { MessageCircle, Send, Sparkles, AlertCircle, X } from 'lucide-react';
import { AISettingsPanel, providers } from '@/components/AISettingsPanel';
import { useAIChat } from '@/hooks/useAIChat';
import { MarkdownRenderer } from '@/components/MarkdownRenderer';
import { useVercelGatewayFallbackModels } from '@/components/VercelGatewayFallbackModels';

export default function ChatPage() {
  const [systemPrompt, setSystemPrompt] = useState(
    'You are a helpful AI assistant.',
  );
  const [showSettings, setShowSettings] = useState(false);
  const [enableTools, setEnableTools] = useState(false);
  const [fallbackModels, setFallbackModels] = useVercelGatewayFallbackModels();

  // Use useCallback so the function reference is stable
  const getExtraBody = useCallback(
    () => ({
      systemPrompt,
      enableTools,
      ...(fallbackModels.length > 0 && { fallbackModels }),
    }),
    [systemPrompt, enableTools, fallbackModels],
  );

  const {
    selectedProvider,
    setSelectedProvider,
    selectedModel,
    setSelectedModel,
    inputValue,
    setInputValue,
    useStreaming,
    setUseStreaming,
    isLoading,
    messages,
    handleSubmit,
    error,
    clearError,
  } = useAIChat({
    endpoint: '/api/chat',
    extraBody: getExtraBody,
  });

  const currentProvider = providers.find((p) => p.id === selectedProvider);

  return (
    <div className="w-full max-w-3xl mx-auto py-16 px-4 sm:px-6 lg:px-8">
      <div className="flex flex-col items-center gap-6 text-center sm:items-start sm:text-left">
        <h1 className="flex items-center gap-3 text-3xl font-semibold leading-10 tracking-tight text-black dark:text-zinc-50">
          <MessageCircle size={32} />
          AI Chat
        </h1>
        <p className="max-w-md text-lg leading-8 text-zinc-600 dark:text-zinc-400">
          Chat with AI using a custom system prompt.
        </p>
      </div>

      <form onSubmit={handleSubmit} className="w-full mt-8">
        <div className="flex flex-col gap-4">
          {/* Shared AI Settings Panel */}
          <AISettingsPanel
            selectedProvider={selectedProvider}
            onProviderChange={setSelectedProvider}
            selectedModel={selectedModel}
            onModelChange={setSelectedModel}
            useStreaming={useStreaming}
            onStreamingChange={setUseStreaming}
            fallbackModels={fallbackModels}
            onFallbackModelsChange={setFallbackModels}
          />

          {/* Settings Toggle */}
          <button
            type="button"
            onClick={() => setShowSettings(!showSettings)}
            className="flex items-center gap-2 text-sm text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white w-fit"
          >
            <Sparkles size={14} />
            {showSettings ? 'Hide' : 'Show'} Chat Settings
          </button>

          {/* Collapsible Settings */}
          {showSettings && (
            <div className="flex flex-col gap-4 p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-zinc-50 dark:bg-zinc-900">
              {/* System Prompt */}
              <div className="flex flex-col gap-2">
                <label className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
                  System Prompt
                </label>
                <textarea
                  value={systemPrompt}
                  onChange={(e) => setSystemPrompt(e.target.value)}
                  placeholder="Enter a custom system prompt..."
                  rows={3}
                  className="w-full p-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-white placeholder-zinc-400 resize-none"
                />
              </div>

              {/* Enable Tools Toggle */}
              <label className="flex items-center gap-2 cursor-pointer">
                <input
                  type="checkbox"
                  checked={enableTools}
                  onChange={(e) => setEnableTools(e.target.checked)}
                  className="w-4 h-4 rounded border-zinc-300 dark:border-zinc-600"
                />
                <span className="text-sm text-zinc-700 dark:text-zinc-300">
                  Enable file reading tools
                </span>
              </label>
            </div>
          )}

          {/* Message Input */}
          <div className="flex gap-2 max-w-md">
            <input
              className="flex-1 p-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-white placeholder-zinc-400"
              value={inputValue}
              placeholder="Type your message..."
              onChange={(e) => setInputValue(e.target.value)}
            />
            <button
              type="submit"
              disabled={isLoading || !inputValue.trim()}
              className="flex h-10 items-center justify-center gap-2 rounded-md bg-blue-600 px-4 text-white transition-colors hover:bg-blue-700 disabled:opacity-50 disabled:cursor-not-allowed"
            >
              <Send size={16} />
              {isLoading ? '...' : 'Send'}
            </button>
          </div>
        </div>
      </form>

      {/* Error Banner */}
      {error && (
        <div className="mt-4 p-4 rounded-lg border border-red-300 dark:border-red-800 bg-red-50 dark:bg-red-950 flex items-start gap-3">
          <AlertCircle
            size={20}
            className="text-red-600 dark:text-red-400 flex-shrink-0 mt-0.5"
          />
          <div className="flex-1">
            <p className="font-medium text-red-800 dark:text-red-200">Error</p>
            <p className="text-sm text-red-700 dark:text-red-300 mt-1">
              {error.message}
            </p>
          </div>
          <button
            onClick={clearError}
            className="text-red-600 dark:text-red-400 hover:text-red-800 dark:hover:text-red-200"
          >
            <X size={18} />
          </button>
        </div>
      )}

      <div className="flex flex-col-reverse w-full mt-8 gap-4">
        {messages.map((m) => (
          <div
            key={m.id}
            className={`whitespace-pre-wrap p-4 rounded-lg border ${
              m.role === 'user'
                ? 'border-blue-200 dark:border-blue-800 bg-blue-50 dark:bg-blue-950'
                : 'border-zinc-200 dark:border-zinc-800 bg-white dark:bg-zinc-900'
            }`}
          >
            <div className="flex items-center justify-between mb-2">
              <strong className="text-zinc-900 dark:text-zinc-100">
                {m.role === 'user' ? 'You' : currentProvider?.name || 'AI'}
              </strong>
              {/* Show usage for both streaming and non-streaming messages */}
              {'usage' in m && m.usage && (
                <span className="text-xs text-zinc-500 dark:text-zinc-500">
                  {m.usage.promptTokens} prompt + {m.usage.completionTokens}{' '}
                  completion = {m.usage.totalTokens} tokens
                </span>
              )}
            </div>
            <div className="text-zinc-700 dark:text-zinc-300">
              {/* Handle both streaming (parts) and non-streaming (content) formats */}
              {'parts' in m && m.parts ? (
                m.parts.map((part, i) =>
                  part.type === 'text' ? (
                    <MarkdownRenderer key={i} content={part.text} />
                  ) : null,
                )
              ) : 'content' in m && m.content ? (
                <MarkdownRenderer content={m.content} />
              ) : null}
            </div>
          </div>
        ))}
      </div>
    </div>
  );
}
</file>

<file path="package.json">
{
  "name": "my-ai-app",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "eslint",
    "format": "prettier --write .",
    "test": "jest",
    "test:watch": "jest --watch",
    "test:coverage": "jest --coverage"
  },
  "dependencies": {
    "@ai-sdk/anthropic": "^2.0.56",
    "@ai-sdk/cohere": "^2.0.21",
    "@ai-sdk/deepseek": "^1.0.32",
    "@ai-sdk/google": "^2.0.47",
    "@ai-sdk/openai": "^2.0.87",
    "@ai-sdk/react": "^2.0.115",
    "@types/marked": "^6.0.0",
    "ai": "^5.0.113",
    "lucide-react": "^0.561.0",
    "marked": "^17.0.1",
    "next": "16.0.10",
    "next-themes": "^0.4.6",
    "qwen-ai-provider": "^0.1.1",
    "react": "19.2.1",
    "react-dom": "19.2.1",
    "zod": "^4.2.1"
  },
  "devDependencies": {
    "@tailwindcss/postcss": "^4",
    "@testing-library/dom": "^10.4.1",
    "@testing-library/jest-dom": "^6.9.1",
    "@testing-library/react": "^16.3.1",
    "@types/jest": "^30.0.0",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "16.0.10",
    "jest": "^30.2.0",
    "jest-environment-jsdom": "^30.2.0",
    "prettier": "^3.7.4",
    "tailwindcss": "^4",
    "ts-node": "^10.9.2",
    "typescript": "^5"
  }
}
</file>

<file path="app/api/chat/route.ts">
import { UIMessage } from 'ai';
import { stepCountIs } from 'ai';
import { NextResponse } from 'next/server';
import { handleAIRequest } from '@/lib/ai-handler';
import { codeTools } from '@/app/api/codereview/route';
import { ProviderId } from '@/lib/providers';

export async function POST(req: Request) {
  try {
    const {
      messages: rawMessages,
      provider: providerId = 'openai',
      apiKey,
      model: requestedModel,
      stream = true,
      systemPrompt,
      enableTools = false, // Whether to enable file reading tools
    }: {
      messages: UIMessage[];
      provider?: ProviderId;
      apiKey?: string;
      model?: string;
      stream?: boolean;
      systemPrompt?: string;
      enableTools?: boolean;
    } = await req.json();

    return handleAIRequest({
      messages: rawMessages,
      provider: providerId,
      apiKey,
      model: requestedModel,
      stream,
      systemPrompt: systemPrompt || 'You are a helpful AI assistant.',
      tools: enableTools ? codeTools : undefined,
      // Allow up to 10 steps for chat (less than codereview since it's simpler)
      stopWhen: enableTools ? stepCountIs(10) : undefined,
      enableUsageMetadata: true, // Include usage in streaming response for UI display
      logPrefix: 'Chat',
    });
  } catch (error) {
    console.error('Chat error:', error);
    const message =
      error instanceof Error ? error.message : 'An unexpected error occurred';
    return NextResponse.json({ error: message }, { status: 500 });
  }
}
</file>

<file path="app/layout.tsx">
import type { Metadata } from 'next';
import { Geist, Geist_Mono } from 'next/font/google';
import { ThemeProvider } from 'next-themes';
import ThemeSwitcher from '../components/ThemeSwitcher';
import { BillingProvider } from '../components/Billing';
import Link from 'next/link';
import { Code, MessageCircle, Settings } from 'lucide-react';
import './globals.css';

const geistSans = Geist({
  variable: '--font-geist-sans',
  subsets: ['latin'],
});

const geistMono = Geist_Mono({
  variable: '--font-geist-mono',
  subsets: ['latin'],
});

export const metadata: Metadata = {
  title: 'Create Next App',
  description: 'Generated by create next app',
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" suppressHydrationWarning>
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased flex flex-col min-h-screen bg-zinc-50 dark:bg-black`}
      >
        <ThemeProvider attribute="class">
          <BillingProvider>
            <header className="w-full border-b border-zinc-200 dark:border-zinc-800">
              <div className="flex items-center justify-between h-16 px-4 mx-auto max-w-7xl sm:px-6 lg:px-8">
                <div className="flex items-center gap-6">
                  <h1 className="text-2xl font-bold text-black dark:text-white">
                    <Link href="/">AI Agent</Link>
                  </h1>
                  <Link
                    href="/"
                    className="flex items-center gap-2 text-sm font-medium text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white"
                  >
                    <Code size={16} />
                    Code Review
                  </Link>
                  <Link
                    href="/chat"
                    className="flex items-center gap-2 text-sm font-medium text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white"
                  >
                    <MessageCircle size={16} />
                    Chat
                  </Link>
                </div>
                <div className="flex items-center gap-4">
                  <ThemeSwitcher />
                  <Link
                    href="/settings"
                    className="flex items-center gap-2 text-sm font-medium text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white"
                  >
                    <Settings size={16} />
                    Settings
                  </Link>
                </div>
              </div>
            </header>
            <main className="flex flex-col flex-1">{children}</main>
          </BillingProvider>
        </ThemeProvider>
      </body>
    </html>
  );
}
</file>

<file path="app/page.tsx">
'use client';

import { useState, useCallback, useRef, useEffect } from 'react';
import {
  Code,
  GitPullRequest,
  AlertCircle,
  X,
  Sparkles,
  FileText,
  Upload,
} from 'lucide-react';
import { AISettingsPanel, providers } from '@/components/AISettingsPanel';
import { useAIChat } from '@/hooks/useAIChat';
import { MarkdownRenderer } from '@/components/MarkdownRenderer';
import { useVercelGatewayFallbackModels } from '@/components/VercelGatewayFallbackModels';

type InputMode = 'file' | 'pr';

// Simple hash function for file content (for caching purposes)
async function hashFileContent(content: string): Promise<string> {
  const encoder = new TextEncoder();
  const data = encoder.encode(content);
  const hashBuffer = await crypto.subtle.digest('SHA-256', data);
  const hashArray = Array.from(new Uint8Array(hashBuffer));
  return hashArray.map((b) => b.toString(16).padStart(2, '0')).join('');
}

// Generic system prompt that works for both modes
const GENERIC_SYSTEM_PROMPT =
  'You are a code reviewer. You will be given either a file path or a GitHub pull request URL, and you will review the code accordingly. For file paths, review the single file. For pull requests, review all changed files in the PR.';

export default function Home() {
  const [inputMode, setInputMode] = useState<InputMode>('file');
  const [systemPrompt, setSystemPrompt] = useState(GENERIC_SYSTEM_PROMPT);
  const [hasCustomPrompt, setHasCustomPrompt] = useState(false);
  const [showSettings, setShowSettings] = useState(false);
  const [fallbackModels, setFallbackModels] = useVercelGatewayFallbackModels();
  const [contextFile, setContextFile] = useState<{
    name: string;
    content: string;
    hash: string;
  } | null>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);

  // Update system prompt when input mode changes (only if user hasn't customized it)
  useEffect(() => {
    if (!hasCustomPrompt) {
      setSystemPrompt(GENERIC_SYSTEM_PROMPT);
    }
  }, [inputMode, hasCustomPrompt]);

  // Handle file upload
  const handleFileUpload = useCallback(
    async (event: React.ChangeEvent<HTMLInputElement>) => {
      const file = event.target.files?.[0];
      if (!file) return;

      try {
        const content = await file.text();
        const hash = await hashFileContent(content);
        setContextFile({
          name: file.name,
          content,
          hash,
        });
      } catch (error) {
        console.error('Error reading file:', error);
        alert('Failed to read file. Please try again.');
      }
    },
    [],
  );

  // Remove context file
  const handleRemoveContextFile = useCallback(() => {
    setContextFile(null);
    if (fileInputRef.current) {
      fileInputRef.current.value = '';
    }
  }, []);

  // Use useCallback so the function reference is stable
  const getExtraBody = useCallback(
    () => ({
      systemPrompt,
      ...(fallbackModels.length > 0 && { fallbackModels }),
      ...(contextFile && {
        contextFile: {
          name: contextFile.name,
          content: contextFile.content,
          hash: contextFile.hash, // Include hash for future caching
        },
      }),
    }),
    [systemPrompt, fallbackModels, contextFile],
  );

  const {
    selectedProvider,
    setSelectedProvider,
    selectedModel,
    setSelectedModel,
    inputValue,
    setInputValue,
    useStreaming,
    setUseStreaming,
    isLoading,
    messages,
    handleSubmit,
    error,
    clearError,
  } = useAIChat({
    endpoint: '/api/codereview',
    extraBody: getExtraBody,
  });

  const currentProvider = providers.find((p) => p.id === selectedProvider);

  return (
    <div className="w-full max-w-3xl mx-auto py-16 px-4 sm:px-6 lg:px-8">
      <div className="flex flex-col items-center gap-6 text-center sm:items-start sm:text-left">
        <h1 className="max-w-xs text-3xl font-semibold leading-10 tracking-tight text-black dark:text-zinc-50">
          AI Code Reviewer
        </h1>
        <p className="max-w-md text-lg leading-8 text-zinc-600 dark:text-zinc-400">
          Enter a file path or GitHub PR URL to have it reviewed by an AI agent.
        </p>
      </div>

      <form onSubmit={handleSubmit} className="w-full mt-8">
        <div className="flex flex-col gap-4">
          {/* Shared AI Settings Panel */}
          <AISettingsPanel
            selectedProvider={selectedProvider}
            onProviderChange={setSelectedProvider}
            selectedModel={selectedModel}
            onModelChange={setSelectedModel}
            useStreaming={useStreaming}
            onStreamingChange={setUseStreaming}
            fallbackModels={fallbackModels}
            onFallbackModelsChange={setFallbackModels}
          />

          {/* Settings Toggle */}
          <button
            type="button"
            onClick={() => setShowSettings(!showSettings)}
            className="flex items-center gap-2 text-sm text-zinc-600 dark:text-zinc-400 hover:text-black dark:hover:text-white w-fit"
          >
            <Sparkles size={14} />
            {showSettings ? 'Hide' : 'Show'} Review Settings
          </button>

          {/* Collapsible Settings */}
          {showSettings && (
            <div className="flex flex-col gap-4 p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-zinc-50 dark:bg-zinc-900">
              {/* System Prompt */}
              <div className="flex flex-col gap-2">
                <label className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
                  System Prompt
                </label>
                <textarea
                  value={systemPrompt}
                  onChange={(e) => {
                    setSystemPrompt(e.target.value);
                    setHasCustomPrompt(true);
                  }}
                  placeholder="Enter a custom system prompt for code review..."
                  rows={3}
                  className="w-full p-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-white placeholder-zinc-400 resize-none"
                />
                <button
                  type="button"
                  onClick={() => {
                    setSystemPrompt(GENERIC_SYSTEM_PROMPT);
                    setHasCustomPrompt(false);
                  }}
                  className="text-xs text-blue-600 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-300 w-fit"
                >
                  Reset to default
                </button>
              </div>

              {/* Context File Upload */}
              <div className="flex flex-col gap-2">
                <label className="text-sm font-medium text-zinc-700 dark:text-zinc-300">
                  Repository Context File (Optional)
                </label>
                <p className="text-xs text-zinc-500 dark:text-zinc-400">
                  Upload a file containing repository context (e.g.,
                  documentation, architecture overview) to help the AI
                  understand your codebase better.
                </p>
                {contextFile ? (
                  <div className="flex items-center gap-2 p-2 rounded-md border border-zinc-300 dark:border-zinc-700 bg-white dark:bg-zinc-800">
                    <FileText
                      size={16}
                      className="text-zinc-600 dark:text-zinc-400"
                    />
                    <span className="flex-1 text-sm text-zinc-900 dark:text-zinc-100 truncate">
                      {contextFile.name}
                    </span>
                    <button
                      type="button"
                      onClick={handleRemoveContextFile}
                      className="text-zinc-500 hover:text-zinc-700 dark:text-zinc-400 dark:hover:text-zinc-200"
                    >
                      <X size={16} />
                    </button>
                  </div>
                ) : (
                  <label className="flex items-center gap-2 px-3 py-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-sm text-zinc-700 dark:text-zinc-300 cursor-pointer hover:bg-zinc-50 dark:hover:bg-zinc-700 transition-colors">
                    <Upload size={16} />
                    <span>Choose file...</span>
                    <input
                      ref={fileInputRef}
                      type="file"
                      onChange={handleFileUpload}
                      className="hidden"
                      accept=".txt,.md,.json,.yaml,.yml"
                    />
                  </label>
                )}
              </div>
            </div>
          )}

          {/* Input Mode Toggle */}
          <div className="flex gap-2">
            <button
              type="button"
              onClick={() => setInputMode('file')}
              className={`flex items-center gap-2 px-3 py-1.5 rounded-md text-sm font-medium transition-colors ${
                inputMode === 'file'
                  ? 'bg-blue-600 text-white'
                  : 'bg-zinc-100 dark:bg-zinc-800 text-zinc-600 dark:text-zinc-400 hover:bg-zinc-200 dark:hover:bg-zinc-700'
              }`}
            >
              <Code size={14} />
              File Path
            </button>
            <button
              type="button"
              onClick={() => setInputMode('pr')}
              className={`flex items-center gap-2 px-3 py-1.5 rounded-md text-sm font-medium transition-colors ${
                inputMode === 'pr'
                  ? 'bg-blue-600 text-white'
                  : 'bg-zinc-100 dark:bg-zinc-800 text-zinc-600 dark:text-zinc-400 hover:bg-zinc-200 dark:hover:bg-zinc-700'
              }`}
            >
              <GitPullRequest size={14} />
              PR Link
            </button>
          </div>

          <input
            className="w-full max-w-md p-2 border border-zinc-300 dark:border-zinc-700 rounded-md bg-white dark:bg-zinc-800 text-zinc-900 dark:text-white placeholder-zinc-400"
            value={inputValue}
            placeholder={
              inputMode === 'file'
                ? 'Enter file path...'
                : 'Enter GitHub PR URL (e.g., https://github.com/owner/repo/pull/123)'
            }
            onChange={(e) => setInputValue(e.target.value)}
          />
          <button
            type="submit"
            disabled={isLoading || !inputValue.trim()}
            className="flex h-12 w-fit items-center justify-center gap-2 rounded-md bg-blue-600 px-6 text-white transition-colors hover:bg-blue-700 disabled:opacity-50 disabled:cursor-not-allowed"
          >
            {inputMode === 'file' ? (
              <Code size={16} />
            ) : (
              <GitPullRequest size={16} />
            )}
            {isLoading ? 'Reviewing...' : 'Review Code'}
          </button>
        </div>
      </form>

      {/* Error Banner */}
      {error && (
        <div className="mt-4 p-4 rounded-lg border border-red-300 dark:border-red-800 bg-red-50 dark:bg-red-950 flex items-start gap-3">
          <AlertCircle
            size={20}
            className="text-red-600 dark:text-red-400 flex-shrink-0 mt-0.5"
          />
          <div className="flex-1">
            <p className="font-medium text-red-800 dark:text-red-200">Error</p>
            <p className="text-sm text-red-700 dark:text-red-300 mt-1">
              {error.message}
            </p>
          </div>
          <button
            onClick={clearError}
            className="text-red-600 dark:text-red-400 hover:text-red-800 dark:hover:text-red-200"
          >
            <X size={18} />
          </button>
        </div>
      )}

      <div className="flex flex-col-reverse w-full mt-8 gap-4">
        {messages.map((m) => (
          <div
            key={m.id}
            className="whitespace-pre-wrap p-4 rounded-lg border border-zinc-200 dark:border-zinc-800 bg-white dark:bg-zinc-900"
          >
            <div className="flex items-center justify-between mb-2">
              <strong className="text-zinc-900 dark:text-zinc-100">
                {m.role === 'user' ? 'You' : currentProvider?.name || 'AI'}
              </strong>
              {/* Show usage for both streaming and non-streaming messages */}
              {'usage' in m && m.usage && (
                <span className="text-xs text-zinc-500 dark:text-zinc-500">
                  {m.usage.promptTokens} prompt + {m.usage.completionTokens}{' '}
                  completion = {m.usage.totalTokens} tokens
                </span>
              )}
            </div>
            <div className="text-zinc-700 dark:text-zinc-300">
              {/* Handle both streaming (parts) and non-streaming (content) formats */}
              {'parts' in m && m.parts ? (
                m.parts.map((part, i) =>
                  part.type === 'text' ? (
                    <MarkdownRenderer key={i} content={part.text} />
                  ) : null,
                )
              ) : 'content' in m && m.content ? (
                <MarkdownRenderer content={m.content} />
              ) : null}
            </div>
          </div>
        ))}
      </div>
    </div>
  );
}
</file>

<file path="app/api/codereview/route.ts">
import { UIMessage, stepCountIs } from 'ai';
import { tool } from 'ai';
import { z } from 'zod';
import { readFile } from 'fs/promises';
import { NextResponse } from 'next/server';
import { handleAIRequest } from '@/lib/ai-handler';
import { ProviderId } from '@/lib/providers';

// Shared tools
export const codeTools = {
  readFile: tool({
    description: 'Read the content of a file.',
    inputSchema: z.object({
      path: z.string().describe('The path to the file to read.'),
    }),
    execute: async ({ path }) => {
      try {
        const content = await readFile(path, 'utf-8');
        return content;
      } catch (error) {
        return `Error reading file: ${error instanceof Error ? error.message : 'Unknown error'}`;
      }
    },
  }),
  readPullRequest: tool({
    description:
      'Read files from a public GitHub pull request. Provide the PR URL to fetch all changed files and their contents.',
    inputSchema: z.object({
      prUrl: z
        .string()
        .describe(
          'The GitHub PR URL (e.g., https://github.com/owner/repo/pull/123).',
        ),
    }),
    execute: async ({ prUrl }) => {
      try {
        // Parse PR URL: https://github.com/owner/repo/pull/123
        const prUrlMatch = prUrl.match(
          /github\.com\/([^\/]+)\/([^\/]+)\/pull\/(\d+)/i,
        );
        if (!prUrlMatch) {
          return 'Error: Invalid GitHub PR URL format. Expected: https://github.com/owner/repo/pull/123';
        }

        const [, owner, repo, prNumber] = prUrlMatch;

        // Fetch PR files using GitHub REST API (no auth needed for public repos)
        const filesResponse = await fetch(
          `https://api.github.com/repos/${owner}/${repo}/pulls/${prNumber}/files`,
          {
            headers: {
              Accept: 'application/vnd.github.v3+json',
            },
          },
        );

        if (!filesResponse.ok) {
          if (filesResponse.status === 404) {
            return `Error: PR not found. Make sure the PR is public and the URL is correct.`;
          }
          return `Error fetching PR files: ${filesResponse.status} ${filesResponse.statusText}`;
        }

        const files = await filesResponse.json();

        if (!Array.isArray(files) || files.length === 0) {
          return 'No files found in this pull request.';
        }

        // Format the response with file information
        const fileContents = files.map((file: any) => {
          const content = file.patch || file.contents || '';
          return {
            filename: file.filename,
            status: file.status, // added, modified, removed, renamed
            additions: file.additions,
            deletions: file.deletions,
            changes: file.changes,
            patch: content.substring(0, 50000), // Limit patch size
            raw_url: file.contents_url || file.blob_url,
          };
        });

        return JSON.stringify(
          {
            pr_url: prUrl,
            owner,
            repo,
            pr_number: prNumber,
            total_files: files.length,
            files: fileContents,
          },
          null,
          2,
        );
      } catch (error) {
        return `Error reading pull request: ${error instanceof Error ? error.message : 'Unknown error'}`;
      }
    },
  }),
};

// Helper function to extract PR info from URL or messages
function extractPRInfo(
  prUrl?: string,
  messages?: UIMessage[] | Array<{ role: string; content: string }>,
): { owner: string; repo: string; prNumber: string } | null {
  // First try explicit prUrl parameter
  if (prUrl) {
    const match = prUrl.match(/github\.com\/([^\/]+)\/([^\/]+)\/pull\/(\d+)/i);
    if (match) {
      return { owner: match[1], repo: match[2], prNumber: match[3] };
    }
  }

  // Otherwise, try to extract from messages
  if (messages) {
    for (const message of messages) {
      let content = '';
      
      // Handle UIMessage type (has parts array)
      if ('parts' in message && Array.isArray(message.parts)) {
        content = message.parts
          .map((p: any) => (p.type === 'text' ? p.text : ''))
          .join('');
      }
      // Handle simple { role, content } type
      else if ('content' in message && typeof message.content === 'string') {
        content = message.content;
      }
      // Handle array content type
      else if ('content' in message && Array.isArray(message.content)) {
        content = (message.content as any[])
          .map((p: any) => (p.type === 'text' ? p.text : ''))
          .join('');
      }

      const match = content.match(/github\.com\/([^\/]+)\/([^\/]+)\/pull\/(\d+)/i);
      if (match) {
        return { owner: match[1], repo: match[2], prNumber: match[3] };
      }
    }
  }

  return null;
}

// Helper function to post comment to GitHub PR
async function postPRComment(
  githubToken: string,
  owner: string,
  repo: string,
  prNumber: string,
  reviewText: string,
  usage?: { promptTokens: number; completionTokens: number; totalTokens: number },
): Promise<void> {
  let commentBody = `## ü§ñ AI Code Review\n\n${reviewText}`;

  if (usage && usage.totalTokens > 0) {
    commentBody += `\n\n---\n**Usage:** ${usage.promptTokens} prompt + ${usage.completionTokens} completion = ${usage.totalTokens} tokens`;
  }

  const apiUrl = `https://api.github.com/repos/${owner}/${repo}/issues/${prNumber}/comments`;
  
  console.log('[CodeReview] Posting PR comment:', {
    owner,
    repo,
    prNumber,
    commentLength: commentBody.length,
    tokenPrefix: githubToken.substring(0, 10) + '...',
  });

  try {
    const response = await fetch(apiUrl, {
      method: 'POST',
      headers: {
        Authorization: `token ${githubToken}`,
        Accept: 'application/vnd.github.v3+json',
        'Content-Type': 'application/json',
        'User-Agent': 'AI-Code-Reviewer',
      },
      body: JSON.stringify({ body: commentBody }),
    });

    console.log('[CodeReview] GitHub API response:', {
      status: response.status,
      statusText: response.statusText,
      ok: response.ok,
      headers: Object.fromEntries(response.headers.entries()),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('[CodeReview] GitHub API error response:', {
        status: response.status,
        statusText: response.statusText,
        errorText,
        url: apiUrl,
      });
      throw new Error(
        `Failed to post PR comment: ${response.status} ${response.statusText}. ${errorText}`,
      );
    }

    const responseData = await response.json().catch(() => null);
    console.log('[CodeReview] Successfully posted PR comment:', {
      commentId: responseData?.id,
      commentUrl: responseData?.html_url,
    });
  } catch (error) {
    console.error('[CodeReview] Error posting PR comment:', {
      error: error instanceof Error ? error.message : String(error),
      stack: error instanceof Error ? error.stack : undefined,
      owner,
      repo,
      prNumber,
      apiUrl,
    });
    throw error;
  }
}

export async function POST(req: Request) {
  try {
    let requestBody;
    try {
      requestBody = await req.json();
    } catch (jsonError) {
      console.error('[CodeReview] Failed to parse request JSON:', jsonError);
      return NextResponse.json(
        { error: 'Invalid JSON in request body' },
        { status: 400 },
      );
    }

    const {
      messages: rawMessages,
      provider: providerId = 'openai',
      apiKey,
      model: requestedModel,
      stream = true, // Default to streaming
      systemPrompt, // Custom system prompt
      contextFile, // Optional context file with name, content, and hash
      fallbackModels, // Optional fallback models for Vercel AI Gateway
      githubToken, // Optional GitHub token for posting PR comments
      prUrl, // Optional explicit PR URL (otherwise extracted from messages)
      // Note: The hash field enables future caching. If the context file hash hasn't changed,
      // you could cache embeddings or processed context to avoid reprocessing the same content.
      // Cache key could be: `context-${contextFile.hash}` or similar.
    }: {
      messages: UIMessage[] | Array<{ role: string; content: string }>;
      provider?: ProviderId;
      apiKey?: string;
      model?: string;
      stream?: boolean;
      systemPrompt?: string;
      contextFile?: {
        name: string;
        content: string;
        hash: string; // SHA-256 hash of file content for cache invalidation
      };
      fallbackModels?: string[]; // Array of fallback model IDs for Vercel AI Gateway
      githubToken?: string; // GitHub token for posting PR comments
      prUrl?: string; // Explicit PR URL (optional, will be extracted from messages if not provided)
    } = requestBody;

    // Build enhanced system prompt with context file if provided
    let enhancedSystemPrompt =
      systemPrompt ||
      `You are a code reviewer.
You will be given a file path and you will review the code in that file.`;

    if (contextFile) {
      enhancedSystemPrompt += `\n\n## Repository Context\n\nThe following context file (${contextFile.name}) provides additional information about this repository:\n\n${contextFile.content}\n\nUse this context to better understand the codebase when reviewing files.`;
    }

    // Get GitHub token from request or environment
    const resolvedGithubToken =
      githubToken || process.env.GITHUB_TOKEN || undefined;

    // Extract PR info if githubToken is provided
    const prInfo = resolvedGithubToken
      ? extractPRInfo(prUrl, rawMessages)
      : null;

    // If githubToken is provided but no PR info found, warn but continue
    if (resolvedGithubToken && !prInfo) {
      console.warn(
        '[CodeReview] GitHub token provided but no PR URL found in request. Comment will not be posted.',
      );
    }

    // For non-streaming with githubToken, we need to intercept the response to post comment
    if (!stream && resolvedGithubToken && prInfo) {
      try {
        // Call handleAIRequest and get the response
        const response = await handleAIRequest({
          messages: rawMessages,
          provider: providerId,
          apiKey,
          model: requestedModel,
          stream: false,
          systemPrompt: enhancedSystemPrompt,
          tools: codeTools,
          stopWhen: stepCountIs(20),
          enableUsageMetadata: true,
          enableStepLogging: true,
          logPrefix: 'CodeReview',
          contextFileHash: contextFile?.hash,
          fallbackModels,
        });

        // Check if response is an error response
        if (!response.ok) {
          const errorData = await response.json().catch(() => ({ error: 'Unknown error' }));
          console.error('[CodeReview] handleAIRequest returned error:', errorData);
          return NextResponse.json(
            { error: errorData.error || 'AI request failed' },
            { status: response.status },
          );
        }

        // Extract text and usage from response
        let responseData;
        try {
          responseData = await response.json();
        } catch (jsonError) {
          console.error('[CodeReview] Failed to parse response JSON:', jsonError);
          const responseText = await response.text().catch(() => 'Unable to read response');
          console.error('[CodeReview] Response text:', responseText.substring(0, 500));
          return NextResponse.json(
            { error: 'Failed to parse API response', details: responseText.substring(0, 200) },
            { status: 500 },
          );
        }
        
        // Check if response has error field
        if (responseData.error) {
          console.error('[CodeReview] Response contains error:', responseData.error);
          return NextResponse.json(
            { error: responseData.error },
            { status: 500 },
          );
        }

        const reviewText = responseData.text || '';
        const usage = responseData.usage;

        if (!reviewText) {
          console.error('[CodeReview] No review text in response:', JSON.stringify(responseData, null, 2));
          return NextResponse.json(
            { error: 'No review text generated', responseKeys: Object.keys(responseData) },
            { status: 500 },
          );
        }

        // Post comment to GitHub PR
        if (reviewText && prInfo && resolvedGithubToken) {
          console.log('[CodeReview] Attempting to post PR comment:', {
            hasReviewText: !!reviewText,
            reviewTextLength: reviewText.length,
            prInfo,
            hasToken: !!resolvedGithubToken,
            tokenLength: resolvedGithubToken.length,
            tokenPrefix: resolvedGithubToken.substring(0, 10) + '...',
          });
          
          try {
            await postPRComment(
              resolvedGithubToken,
              prInfo.owner,
              prInfo.repo,
              prInfo.prNumber,
              reviewText,
              usage,
            );
            console.log(
              `[CodeReview] Successfully posted PR comment to ${prInfo.owner}/${prInfo.repo}#${prInfo.prNumber}`,
            );
          } catch (error) {
            console.error('[CodeReview] Failed to post PR comment:', {
              error: error instanceof Error ? error.message : String(error),
              stack: error instanceof Error ? error.stack : undefined,
              prInfo,
              reviewTextLength: reviewText.length,
              usage,
              tokenPrefix: resolvedGithubToken.substring(0, 10) + '...',
            });
            // Don't fail the request if comment posting fails, but log the error
          }
        } else {
          console.warn('[CodeReview] Skipping PR comment post:', {
            hasReviewText: !!reviewText,
            hasPrInfo: !!prInfo,
            hasToken: !!resolvedGithubToken,
            prInfo,
          });
        }

        return NextResponse.json(responseData);
      } catch (error) {
        console.error('[CodeReview] Error in non-streaming with githubToken:', error);
        const errorMessage =
          error instanceof Error ? error.message : 'Unknown error occurred';
        return NextResponse.json(
          { error: errorMessage, details: error instanceof Error ? error.stack : undefined },
          { status: 500 },
        );
      }
    }

    // For streaming, we can't easily intercept and post comment, so we'll skip it
    // Users can use non-streaming mode when they want automatic PR comments
    if (stream && resolvedGithubToken && prInfo) {
      console.warn(
        '[CodeReview] GitHub token provided but streaming is enabled. PR comments are only posted for non-streaming requests. Set stream=false to enable automatic PR comments.',
      );
    }

    // Use handleAIRequest from the shared library, with codereview-specific options
    return handleAIRequest({
      messages: rawMessages,
      provider: providerId,
      apiKey,
      model: requestedModel,
      stream,
      systemPrompt: enhancedSystemPrompt,
      tools: codeTools, // Always enable tools for code review
      // Allow up to 20 steps to handle complex reviews with multiple tool calls
      // This covers: reading PR, reading multiple files, and generating the review
      stopWhen: stepCountIs(20),
      enableUsageMetadata: true, // Include usage in streaming response for UI display
      enableStepLogging: true, // Log tool calls for debugging
      logPrefix: 'CodeReview',
      contextFileHash: contextFile?.hash, // Pass hash for provider-level caching
      fallbackModels, // Pass fallback models for Vercel AI Gateway
      // Note: When contextFile.hash is unchanged and user query is identical,
      // OpenAI will automatically return cached responses (faster + cheaper).
      // The hash helps track when context changes and cache should be invalidated.
    });
  } catch (error) {
    console.error('Code review error:', error);
    const message =
      error instanceof Error ? error.message : 'An unexpected error occurred';
    const stack = error instanceof Error ? error.stack : undefined;
    
    // Log full error details for debugging
    console.error('Error details:', {
      message,
      stack,
      error: error instanceof Error ? error.toString() : String(error),
    });
    
    return NextResponse.json(
      {
        error: message,
        ...(process.env.NODE_ENV === 'development' && { stack }),
      },
      { status: 500 },
    );
  }
}
</file>

</files>
